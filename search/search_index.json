{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Saira AI Companion Documentation","text":"<p>Welcome to the official documentation for Saira AI Companion, the emotion-aware life companion AI.</p> <ul> <li>Architecture</li> <li>API Reference</li> <li>Development Guide</li> <li>Deployment</li> <li>Contributing</li> <li>Changelog</li> </ul> <p>For questions or support, open an issue or email support@saira-ai.com.</p>"},{"location":"architecture/","title":"Saira AI Companion \u2013 Architecture Documentation","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>Saira is an empathetic, intelligent AI companion that understands user emotions, remembers conversations, and adapts to different personalities across devices. Saira focuses on privacy-first processing, with all emotion detection and conversation data stored and processed locally.</p>"},{"location":"architecture/#high-level-system-diagram","title":"High-Level System Diagram","text":"<p>Include or update the following diagram as your architecture evolves:</p> <pre><code>saira-ai-companion/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 core/           # Core AI and business logic\n\u2502   \u251c\u2500\u2500 voice/          # Speech processing\n\u2502   \u251c\u2500\u2500 emotion/        # Emotion analysis\n\u2502   \u251c\u2500\u2500 memory/         # Memory management\n\u2502   \u251c\u2500\u2500 ui/             # User interface\n\u2502   \u251c\u2500\u2500 mobile/         # iOS companion app\n\u2502   \u2514\u2500\u2500 speaker/        # Smart speaker interface\n\u251c\u2500\u2500 docs/               # Documentation\n\u251c\u2500\u2500 tests/              # Comprehensive test suites\n\u251c\u2500\u2500 scripts/            # Development and deployment scripts\n\u2514\u2500\u2500 config/            # Environment configurations\n</code></pre> <p>Consider replacing or extending this with a graphical diagram (e.g., system-diagram.png) if available.</p>"},{"location":"architecture/#main-components","title":"Main Components","text":"Component Description Interfaces/APIs Core AI Handles business logic, orchestrates emotion detection, memory, etc Internal classes/services Voice Speech-to-text, text-to-speech, wake-word detection Microphone/Audio APIs Emotion Detects, analyzes, and provides feedback on user emotions Internal, UI feedback Memory Vector embeddings, context management, long-term memory Vector DB APIs UI User interface for all platforms React (Web), iOS (Swift) Mobile iOS companion app Apple APIs, REST, BLE Speaker Smart speaker interface Device SDK, Audio APIs"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li>User Input: Voice or text input is captured via the UI or device microphone.</li> <li>Speech Processing: Audio processed into text (STT), analyzed for emotional cues.</li> <li>Emotion Analysis: Emotion detection modules extract emotional state.</li> <li>Core AI: Business logic chooses responses, adapts personality mode, consults memory.</li> <li>Memory: Vector DB and embeddings provide context from past interactions.</li> <li>Response Generation: Appropriate, emotionally-aware response is generated.</li> <li>Output: Response is delivered through UI, voice (TTS), or other device outputs.</li> <li>Local Storage: All sensitive data remains local, encrypted, and never leaves the device.</li> </ol>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":"<ul> <li>Backend/Core: Node.js 18+, Python 3.9+, custom AI modules</li> <li>Frontend/UI: React (Web), Swift (iOS)</li> <li>AI/ML: HuggingFace Transformers, custom emotion models</li> <li>Database: Local vector DB (e.g., Chroma, Faiss, or custom)</li> <li>Voice Processing: Web Audio API, CoreML, or platform-native solutions</li> <li>DevOps: GitHub Actions, scripts for setup and deployment</li> <li>Security/Privacy: Local encryption, no cloud processing</li> </ul>"},{"location":"architecture/#deployment-environments","title":"Deployment Environments","text":"Environment Purpose Notes Development Local dev, feature testing macOS, iOS, emulators Staging Pre-release, integration testing (Planned) Production End-user deployment Mac app, iOS app, Speakers"},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Privacy-first: All processing and storage are local; encrypted sync for multi-device.</li> <li>Emotion-aware: Core feature is real-time, on-device emotion detection.</li> <li>Extensible personalities: Modular design allows for new personality modes.</li> <li>Multi-platform: Designed for seamless context and memory sharing across Mac, iPhone, and smart speakers.</li> </ul>"},{"location":"architecture/#security-and-privacy-considerations","title":"Security and Privacy Considerations","text":"<ul> <li>End-to-end encryption for device sync.</li> <li>No personal data or audio leaves the device.</li> <li>Local storage of all conversation data and emotion metrics.</li> <li>Compliance with privacy best practices and user transparency.</li> </ul>"},{"location":"architecture/#extensibility-and-modularity","title":"Extensibility and Modularity","text":"<ul> <li>Plugin-ready architecture for new emotion models, platforms, or personality modes.</li> <li>Separation of concerns: core, UI, memory, voice, emotion, platform-specific code.</li> </ul>"},{"location":"architecture/#references","title":"References","text":"<ul> <li>Project README</li> <li>UI Implementation</li> <li>Low-fidelity Prototype</li> <li>High-level Component Diagram</li> <li>Contributing Guide</li> </ul> <ul> <li>System Diagrams (add/update as needed)</li> </ul> <p>Last updated: 2025-06-09</p>"},{"location":"brainstorming/","title":"Application Architecture Brainstorm: A Comprehensive Structural Report for a Voice-Enabled macOS Application","text":"<p>This report outlines the proposed architectural structure for a voice-enabled macOS application, focusing on key features for the Minimum Viable Product (MVP) and considerations for future expansion. The design prioritizes local, on-device processing for privacy and performance, leveraging a hybrid technology stack that combines cross-platform development with native macOS capabilities.</p>"},{"location":"brainstorming/#launch-features-mvp","title":"Launch Features (MVP)","text":""},{"location":"brainstorming/#core-application-framework-native-macos-ui-with-swift-swiftui","title":"Core Application Framework: Native macOS UI with Swift (SwiftUI)","text":"<p>The user interface of the application will be developed using Swift and SwiftUI, providing a fully native macOS experience. This approach is chosen to ensure optimal performance, seamless integration with the Apple ecosystem, and to address the identified stability concerns with React Native for macOS. Swift is a modern, high-performance language that compiles directly to native code, offering predictable memory management and robust safety features. SwiftUI, Apple's declarative UI framework, allows for efficient development of responsive and interactive user interfaces that feel natural on macOS.\u00a0\u00a0</p> <p>A key advantage of Swift is its unmatched interoperability with C and C++. This is crucial for Saira, as the core AI inference engines (LLM, ASR, TTS, SER) are implemented in C/C++. Direct C++ interoperability eliminates the need for complex bridging layers, ensuring high performance and low overhead when calling C++ APIs from Swift and vice versa. This native integration is vital for real-time voice interaction and AI processing on target devices like the MacBook M1, which can leverage Apple Silicon optimizations. Furthermore, Swift and SwiftUI provide a strong foundation for future expansion to iOS, allowing for significant code reuse and a consistent development experience across Apple platforms.\u00a0\u00a0</p> <p>The initial focus for the MVP will be exclusively on voice interaction with the LLM, ensuring a streamlined and highly polished core experience before expanding to other modalities.</p>"},{"location":"brainstorming/#local-ai-inference-engine","title":"Local AI Inference Engine","text":"<p>The application's core intelligence will be powered by local AI models, encompassing functionalities such as Speech-to-Text (ASR), Text-to-Speech (TTS), Wake Word Detection, and Speech Emotion Recognition (SER). Running these models directly on the user's device is a fundamental design choice that ensures user privacy, enables full offline capability, and provides faster response times by eliminating network latency. This is particularly optimized for the MacBook M1 target environment.\u00a0\u00a0</p> <p>Key technologies for this layer include:</p> <ul> <li> <p><code>llama.cpp</code>: This is a pure C/C++ port of Meta's LLaMA model and other large language models (LLMs), optimized for efficient and fast local inference on both CPUs and GPUs. It supports quantized models, such as Mistral 7B Q4_0, which are crucial for running powerful LLMs on consumer hardware.\u00a0\u00a0</p> </li> <li> <p><code>whisper.cpp</code>: A C/C++ implementation of OpenAI\u2019s Whisper ASR model, providing high-performance, offline audio transcription across various platforms, including macOS. A significant advantage is its ability to leverage Apple\u2019s Metal framework for GPU acceleration on Apple Silicon devices, ensuring rapid transcription.\u00a0\u00a0</p> </li> <li> <p>Piper TTS: A fast, local neural text-to-speech system developed with C++ as its primary language. It is optimized for on-device use and generates natural-sounding speech, utilizing ONNX models for efficient inference.\u00a0\u00a0</p> </li> <li> <p>Wake Word Detection: For always-on listening with minimal resource consumption, Porcupine is the recommended choice. It is known for its high accuracy, lightweight footprint, and cross-platform compatibility, including macOS (x86_64, arm64). It is production-ready and supports pre-defined wake words, which suffice for Saira's requirements.\u00a0\u00a0</p> </li> <li> <p>Speech Emotion Recognition (SER): To achieve a more granular, continuous emotional state detection (e.g., arousal, valence), a combination of approaches is recommended:</p> </li> <li> <p><code>openSMILE</code>: A robust C++ toolkit widely used in affective computing for real-time audio feature extraction and classification. It can extract comprehensive feature sets like eGeMAPS, which are suitable for detailed emotion analysis.\u00a0\u00a0</p> </li> <li> <p>SenseVoice: This model offers high-accuracy multilingual SER with exceptionally low inference latency (e.g., 70ms to process 10 seconds of audio, 15 times faster than Whisper-Large) and supports ONNX export for C++ inference, making it highly suitable for on-device deployment.\u00a0\u00a0</p> </li> <li> <p>Lightweight MLP models: Research into optimized and computationally efficient Multilayer Perceptron (MLP)-based classifiers with adaptive quantization schemes shows promise for deployment on resource-constrained edge devices, balancing model size reduction with performance retention. This approach can be explored for a custom SER model.\u00a0\u00a0</p> </li> </ul> <p>The application's heavy reliance on local AI models necessitates robust C++ implementations for components like ASR, TTS, LLM, SER, and Wake Word detection. The integration of these C++ libraries into the Swift/SwiftUI frontend will leverage Swift's direct C++ interoperability, providing a seamless and high-performance bridge between the UI and the AI inference engines.\u00a0\u00a0</p> <p>The selection of open-source AI libraries, while advantageous for development cost and transparency, requires careful consideration of their licensing terms for commercial viability. For instance, Picovoice's Porcupine, an excellent wake word detection engine, is free for non-commercial use but mandates a paid plan for commercial applications. Similarly,\u00a0\u00a0</p> <p><code>openSMILE</code> is provided under a source-available, proprietary license for research and personal use. These licensing conditions must be thoroughly reviewed and factored into the overall business model and budget planning to ensure legal compliance and predictable long-term costs. This architectural decision extends beyond pure technical feasibility to encompass business and legal implications.\u00a0\u00a0</p>"},{"location":"brainstorming/#audio-processing-layer","title":"Audio Processing Layer","text":"<p>This layer is fundamental to the application's voice interaction capabilities, responsible for real-time audio input from the microphone, necessary pre-processing (such as noise reduction and format conversion), and precise audio output for text-to-speech synthesis. Given the stringent requirements for low-latency, on-device AI processing, direct access to the macOS Core Audio API is paramount.</p> <p>The core technologies for this layer will be:</p> <ul> <li> <p>macOS Core Audio API (C++/Objective-C/Swift): This is Apple's native, low-level audio framework, providing direct access to real-time audio input/output, sophisticated audio processing graphs, and hardware interactions. It offers the highest level of performance and granular control over audio streams, which is essential for a real-time voice application.\u00a0\u00a0</p> </li> <li> <p>Direct C++ Integration: Given the shift to a Swift/SwiftUI frontend, direct C++ integration with Core Audio is the most suitable solution for audio capture and playback. This avoids the stability issues associated with Node.js wrappers like <code>naudiodon</code> and <code>node-core-audio</code>, which have shown critical errors and lack of active maintenance. Low-level audio callbacks, crucial for real-time performance, are best written in C or C++ for reliability and consistent speed.\u00a0\u00a0</p> </li> </ul> <p>A subtle yet critical requirement for robust audio processing is maintaining consistency in audio formats across the entire pipeline. Inconsistent sampling rates or channel counts between audio capture, processing by AI models, and final playback can lead to unexpected bugs, performance degradation, or even application crashes. A resilient audio layer must establish and enforce a consistent internal audio format (e.g., 32-bit floating point, 44.1kHz, stereo) and manage all necessary conversions at the system's periphery. This includes converting incoming microphone audio to the internal format and converting AI-generated speech audio to the speaker's required format. This seemingly minor detail is a frequent source of difficult-to-diagnose audio issues and performance bottlenecks in real-time systems.\u00a0\u00a0</p>"},{"location":"brainstorming/#data-persistence-synchronization","title":"Data Persistence &amp; Synchronization","text":"<p>The application requires a robust local data storage solution for user preferences, AI model configurations, and potentially long-term conversational memory. Given the application's offline-first nature, an embedded local database with efficient synchronization capabilities will be essential.</p> <p>The primary technologies for this layer include:</p> <ul> <li> <p>SQLite: Chosen as the embedded, serverless, and cross-platform database solution. It is highly suitable for local data management in desktop applications due to its efficiency, reliability, and excellent performance, particularly for local read operations.\u00a0\u00a0</p> </li> <li> <p><code>sqlite-vec</code>: An SQLite extension that enables vector search directly within the SQLite database. This eliminates the need for an external vector database for local Retrieval-Augmented Generation (RAG), which is crucial for on-device LLM context augmentation.\u00a0\u00a0</p> </li> <li> <p>SQLite Libraries for Swift: Swift can directly interact with SQLite, and libraries exist to facilitate this. <code>op-sqlite</code> supports <code>sqlite-vec</code> and can be integrated into React Native projects, which implies its underlying C++ components could be used with Swift via interoperability.\u00a0\u00a0</p> </li> </ul> <p>Beyond its traditional role for relational data, SQLite's capability to store Binary Large Objects (BLOBs) and its demonstrated performance advantage for reading numerous small files (up to 35% faster than direct disk access in certain scenarios ) make it an excellent candidate for packaging and managing AI model assets. AI models often comprise multiple files, including the model binary (e.g., ONNX models for Piper or SenseVoice), configuration files, and vocabulary data. Consolidating these into a single SQLite database file could significantly simplify model loading, ensure atomicity of model updates, and potentially improve application load times by reducing file I/O overhead. This represents a non-obvious yet powerful application of SQLite beyond conventional data storage.\u00a0\u00a0</p> <p>While managing offline data modifications for user-specific information is relatively straightforward, handling shared data with offline changes introduces significant complexities related to conflict resolution. If the application were to evolve to include collaborative features or shared knowledge bases, relying solely on simple strategies like \"first update wins\" or manual synchronization could degrade the user experience and potentially lead to data inconsistencies or loss. This highlights that while E2EE solutions effectively secure data in transit and at rest in the cloud , the application-level logic for merging conflicting offline changes is a complex problem that requires careful design, potentially involving advanced techniques like Conflict-Free Replicated Data Types (CRDTs) if true multi-user collaboration is a future requirement. This is a critical future architectural consideration that impacts the data model design and overall user experience.\u00a0\u00a0</p>"},{"location":"brainstorming/#future-features-post-mvp","title":"Future Features (Post-MVP)","text":""},{"location":"brainstorming/#advanced-ai-capabilities-long-term-memory-emotional-intelligence","title":"Advanced AI Capabilities: Long-Term Memory &amp; Emotional Intelligence","text":"<p>Future iterations of the application could significantly enhance the user experience by incorporating advanced AI capabilities, particularly long-term conversational memory and emotional intelligence. This would involve:</p> <ul> <li> <p>Persistent, context-aware conversational memory for LLMs: Allowing the LLM to remember past interactions and user preferences across sessions.</p> </li> <li> <p>Retrieval-Augmented Generation (RAG): Grounding LLM responses in user-specific data to provide more accurate and personalized information.</p> </li> <li> <p>Real-time detection and interpretation of user emotions from speech: Enabling the application to understand and respond to the user's emotional state.</p> </li> <li> <p>Ability for LLM to adapt responses based on detected emotion: Creating more empathetic and natural interactions.</p> </li> </ul> <p>Key technologies for these future capabilities include:</p> <ul> <li> <p>LLM Long-Term Memory: Frameworks like <code>LangMem</code> provide abstractions for semantic, episodic, and procedural memory, allowing LLMs to retain facts, past experiences, and system instructions across conversations.\u00a0\u00a0</p> </li> <li> <p><code>LlamaEdge</code> implements \"server-side RAG\" by coupling long-term knowledge in a vector database with the LLM. For a fully local application, the combination of\u00a0\u00a0</p> </li> <li> <p><code>llama.cpp</code> with the <code>sqlite-vec</code> extension is the local alternative for RAG.\u00a0\u00a0</p> </li> <li> <p>Speech Emotion Recognition (SER) Models: <code>openSMILE</code> (C++) is a robust toolkit for extracting audio features for real-time SER. Transformer-based models like\u00a0\u00a0</p> </li> <li> <p><code>SenseVoice</code> and\u00a0\u00a0</p> </li> <li> <p><code>GigaAM-Emo</code> offer high accuracy and support ONNX export for C++ inference, making them suitable for on-device deployment. Research also points to lightweight MLP models for edge devices.\u00a0\u00a0</p> </li> <li> <p>LLM Orchestration: Frameworks like LlamaIndex provide tools for managing documents and nodes (text chunks) and building indexes for RAG.\u00a0\u00a0</p> </li> </ul> <p>Implementing truly local RAG with <code>llama.cpp</code> necessitates a local vector database. The <code>sqlite-vec</code> extension for SQLite is a significant enabler for this, as it allows vector embeddings to be stored and searched directly within the application's existing SQLite database. This eliminates the operational complexity and overhead associated with running a separate vector database server locally, such as Qdrant. By embedding vector search capabilities directly into SQLite, the architecture is simplified, external dependencies are reduced, and the \"local-first\" principle is maintained, making RAG much more feasible and performant on-device.\u00a0\u00a0</p> <p>The implementation of real-time emotion detection on-device presents nuanced challenges. While various SER models exist, their \"real-time\" performance and \"on-device\" suitability can vary considerably. Models like SenseVoice-Small are highlighted for their exceptionally low inference latency (e.g., 70ms to process 10 seconds of audio, 15 times faster than Whisper-Large ), making them strong candidates for interactive applications. However, it is important to acknowledge the limitations of some models, such as \"limited emotion coverage\" (e.g., classifying only 8 basic emotions) and sensitivity to audio quality. Additionally, open-source models trained on smaller datasets may exhibit \"lower robustness\". This indicates that achieving high-quality, real-time emotion detection across a full spectrum of human emotions on a desktop device remains a non-trivial challenge. It may require careful model selection, potential fine-tuning for specific use cases, or a pragmatic approach to the scope of emotions detected in initial phases, recognizing that comprehensive, nuanced emotion detection is an ongoing area of research.\u00a0\u00a0</p>"},{"location":"brainstorming/#system-level-integration-background-services","title":"System-Level Integration &amp; Background Services","text":"<p>For advanced capabilities like always-on listening for wake words or continuous speech emotion recognition, the application will require robust system-level integration and the ability to run AI models and audio processing as background services. This ensures that core functionalities remain active even when the main UI is not in the foreground.</p> <ul> <li> <p>Background Services: AI models and audio processing components will operate as background services.</p> </li> <li> <p>Secure Inter-Process Communication (IPC): A secure mechanism will be established for communication between the main UI application and these background services.</p> </li> <li> <p>Seamless Updates: A robust strategy for updating AI models and application components will be implemented.</p> </li> </ul> <p>The necessity of XPC services for robust background AI services is a critical architectural consideration. Running AI models continuously within the main application thread would lead to inefficiencies, potential UI freezes, and excessive resource consumption. A dedicated background process is essential to offload these intensive computations. macOS XPC services are the native, secure, and Apple-recommended mechanism for achieving this. They provide a lightweight framework for inter-process communication, enabling the delegation of work that needs to persist beyond the client's lifecycle or requires specific privilege isolation. This design choice ensures that AI processing can continue reliably even if the user closes the main application window or if the UI encounters a temporary issue, representing a fundamental shift towards a more distributed, service-oriented architecture on the local machine.\u00a0\u00a0</p> <p>While Docker is an excellent tool for creating consistent local development environments for AI models , it is generally not suitable as a production deployment mechanism for a desktop application. Bundling Docker containers with the end-user application introduces significant overhead, including increased application size, higher resource consumption, and the requirement for the user to manage a Docker daemon. The primary objective is to leverage Docker during the\u00a0\u00a0</p> <p>development phase to simplify dependency management and ensure reproducible AI model setups. However, for production deployment, the optimized C++ models (e.g., <code>llama.cpp</code>, <code>whisper.cpp</code> compiled specifically for macOS) should be directly integrated into the native application bundle. Docker serves as a powerful toolchain and testing ground for AI components, facilitating development without becoming the final deployment mechanism for the end-user application.</p>"},{"location":"brainstorming/#system-diagram","title":"System Diagram","text":"<p>The system architecture is designed with a clean, layered approach to ensure modularity, clear component relationships, and a strong separation of concerns.</p> <ul> <li> <p>Presentation Layer (UI): This is the topmost layer, encompassing the User Interface (SwiftUI macOS App). It contains the SwiftUI Views &amp; Components responsible for all UI rendering, user input handling (voice, text, UI interactions), and output display (text, visualizations, alerts).</p> </li> <li> <p>Application Layer (Business Logic / Orchestration): Positioned beneath the UI, this layer houses the Application Core (Swift). It includes Feature Orchestrators that manage the flow of operations (e.g., ASR -&gt; LLM -&gt; TTS), Data Managers that interact with the local data persistence, and a Settings &amp; Preferences Manager. This layer orchestrates interactions between the UI, native services, and data layers, acting as the central hub for translating UI events into backend calls and processing results for display.</p> </li> <li> <p>Native Services Layer (Performance Critical / OS Interaction): This critical layer contains the Native C++ Services (via Swift C++ Interoperability). It comprises the Audio I/O Service for direct interface with macOS Core Audio for real-time microphone input and speaker output, including audio format conversion and buffering. The AI Inference Engine Service wraps <code>llama.cpp</code>, <code>whisper.cpp</code>, Piper, and SER models, managing model loading, inference execution, and resource allocation (CPU/GPU). The Wake Word Background Service is implemented as an XPC Service for lightweight, always-on processing. An IPC Bridge (XPC / Shared Memory) provides secure, high-throughput communication channels between native services and the Application Core, and between background services and the main application. This layer is predominantly implemented in C++ and Swift to achieve low-latency and efficient resource utilization.</p> </li> <li> <p>Data Layer (Persistence): Located at the base, this layer manages all local data storage within the Local Data Persistence (SQLite) container. It includes the SQLite Database for application data, user preferences, conversational history, and potentially packaged AI models (as BLOBs). The <code>sqlite-vec</code> Extension is integrated to enable vector search for RAG directly within SQLite. An Encryption Module handles the encryption and decryption of sensitive local data. SQLite is chosen for its embedded nature, performance, and reliability.</p> </li> <li> <p>External Integrations (Future / Optional): This conceptual layer represents potential future Cloud Services. It could include a Cloud Sync Service (e.g., EteSync or a custom backend), User Authentication / Account Management, and Remote Model Updates / Telemetry. This layer would communicate with the Local Data Persistence layer, implementing secure, end-to-end encrypted synchronization protocols.</p> </li> </ul> <p>Component Relationships &amp; Data Flow:</p> <ul> <li> <p>UI &lt;-&gt; Application Core: Communication is bidirectional within the Swift/SwiftUI framework.</p> </li> <li> <p>Application Core &lt;-&gt; Native Services Layer: Bidirectional communication occurs through Swift's direct C++ interoperability, enabling the invocation of C++ functions and the reception of callbacks or events.</p> </li> <li> <p>Native Services Layer (Audio I/O) &lt;-&gt; OS Hardware: This involves direct interaction with the macOS Core Audio framework for managing microphone input and speaker output.</p> </li> <li> <p>Native Services Layer (AI Inference) &lt;-&gt; AI Models (Files/SQLite BLOBs): Model binaries are loaded from local storage.</p> </li> <li> <p>Native Services Layer (Wake Word Service) &lt;-&gt; Native Services Layer (Audio I/O): Raw audio streams are continuously processed for wake word detection.</p> </li> <li> <p>Native Services Layer &lt;-&gt; Local Data Persistence: Configuration, model data, and conversational memory are read from and written to the local database.</p> </li> <li> <p>Local Data Persistence &lt;-&gt; Cloud Services (Future): Bidirectional synchronization of encrypted data would be established.</p> </li> </ul> <p>Key Data Flows:</p> <ol> <li> <p>Voice Command Flow: A user speaks, and the Audio I/O Service captures the raw audio. This audio is streamed to the Wake Word Background Service. Upon wake word detection, a signal is sent to the Application Core via IPC. The Application Core then activates the ASR, and the Audio I/O streams the audio to the AI Inference Service (<code>whisper.cpp</code>). The ASR converts speech to text, which is then sent to the Application Core. The Application Core forwards the text to the LLM (via the AI Inference Service). The LLM processes the request, potentially querying the Local Data Persistence (for RAG via <code>sqlite-vec</code>) for contextual information. The LLM generates a response text, which is sent back to the Application Core. The Application Core then sends this text to the TTS (via the AI Inference Service, Piper TTS). The TTS generates audio, which is streamed to the Audio I/O Service and played through the speakers.</p> </li> <li> <p>Data Sync Flow (Future): The Application Core initiates a synchronization process, and the Local Data Persistence prepares delta changes. This data is encrypted locally and transmitted to the Cloud Sync Service. The Cloud Sync Service resolves any conflicts and updates the remote data, subsequently pushing updates to other devices if multi-device functionality is supported. Other devices then pull these updates, decrypt them locally, and update their Local Data Persistence.</p> </li> </ol> <p>The following table provides a concise overview of the proposed technology stack, highlighting the purpose, key benefits, and critical production readiness considerations for each component.</p> Component/Feature Primary Technology Purpose Key Benefits Production Readiness / Concerns UI Framework Swift (SwiftUI) Native macOS UI development Optimal performance, deep OS integration, Apple Silicon optimization, strong C++ interop, iOS readiness Requires Swift/Objective-C expertise. Local LLM Inference <code>llama.cpp</code> Offline, efficient LLM execution High performance, CPU/GPU support, privacy, quantized models Mature, actively maintained. Speech-to-Text (ASR) <code>whisper.cpp</code> Offline, high-performance audio transcription On-device, Metal GPU acceleration on Apple Silicon, privacy Mature, actively maintained.\u00a0\u00a0 Text-to-Speech (TTS) Piper TTS Fast, local neural speech synthesis On-device, C++ optimized, natural-sounding speech Mature, actively maintained.\u00a0\u00a0 Wake Word Detection Porcupine Always-on, low-power activation Highly accurate, pre-defined wake words, on-device, privacy Commercial license required for commercial use.\u00a0\u00a0 Speech Emotion Recognition (SER) openSMILE / SenseVoice (via C++ inference) Real-time emotion detection from voice (arousal, valence) On-device, C++ optimized, feature extraction, low latency openSMILE: Source-available, proprietary license for commercial use. SenseVoice: Low latency, ONNX export for C++.\u00a0\u00a0 Local Data Storage SQLite (with <code>sqlite-vec</code>) Embedded, reliable data persistence &amp; vector search Serverless, fast local reads, ACID, RAG support, single file Highly stable, well-tested.\u00a0\u00a0 Audio I/O macOS Core Audio API (C++/Swift) Real-time microphone input &amp; speaker output Low latency, direct hardware access, fine-grained control Requires native development expertise.\u00a0\u00a0 Inter-Process Communication (IPC) macOS XPC Services (Swift/Objective-C) Secure communication between processes Privilege isolation, background task management, Apple-recommended Requires native development expertise.\u00a0\u00a0 Offline Sync Protocol EteSync (or custom E2EE) Secure, end-to-end encrypted data synchronization Zero-knowledge, open-source, change journal, privacy-focused Complexity of conflict resolution for shared data."},{"location":"brainstorming/#architecture-considerations","title":"Architecture Considerations","text":""},{"location":"brainstorming/#native-module-development-maintenance","title":"Native Module Development &amp; Maintenance","text":"<p>With the decision to use Swift/SwiftUI for the frontend, the concept of \"native modules\" shifts from a JavaScript-to-native bridge (like React Native's TurboModules) to direct interoperability between Swift and C++. This is the best-suited solution for Saira's architecture:</p> <ul> <li> <p>Direct C++ Interoperability (Swift): Swift offers robust and seamless interoperability with C and C++. This means that the core AI inference engines (\u00a0\u00a0</p> </li> <li> <p><code>llama.cpp</code>, <code>whisper.cpp</code>, Piper, <code>openSMILE</code>, SenseVoice), which are primarily C++ libraries, can be directly integrated and called from Swift code without the need for a separate bridging layer or complex wrappers. This approach minimizes overhead, maximizes performance, and simplifies the development and debugging process for the performance-critical AI components. It also leverages Apple Silicon optimizations directly.\u00a0\u00a0</p> </li> <li> <p>XPC Services for Background Tasks: For long-running or resource-intensive tasks like continuous wake word detection or background RAG processing, macOS XPC services are the recommended approach. These services run in separate processes, providing privilege isolation and ensuring that the main application UI remains responsive. Swift and Objective-C are the native languages for developing XPC services, and they can communicate securely with the main application via\u00a0\u00a0</p> </li> <li> <p><code>NSXPCConnection</code>.\u00a0\u00a0</p> </li> <li> <p>Node.js/Python for Tooling/Utilities (Optional): While the core application will be Swift/C++, Node.js or Python could still be utilized for specific tooling, data processing scripts, or even a lightweight local server for certain RAG components if a fully embedded C++ solution proves too complex for specific features. Swift can interact with these external processes via standard IPC mechanisms (ee.g., pipes, sockets, or even by launching command-line tools). However, for the core real-time AI functionalities, direct C++ integration is superior.\u00a0\u00a0</p> </li> </ul> <p>This strategy ensures a highly performant, stable, and maintainable application by leveraging the strengths of native macOS development and direct C++ integration for AI, while providing flexibility for auxiliary components.</p>"},{"location":"brainstorming/#ai-model-update-strategy-offline","title":"AI Model Update Strategy (Offline)","text":"<p>Mechanism: Local AI models (LLM \u2013 Mistral 7B, ASR \u2013 Whisper.cpp, TTS \u2013 Piper, SER \u2013 future custom model) will be updated via an in-app download mechanism, not bundled with app updates. This keeps the application lightweight and separates model delivery from platform updates.\u00a0\u00a0</p> <p>Versioning &amp; Compatibility: Models will be semantically versioned (e.g., mistral-7b-q4_0-v1.2) with metadata stored locally. The application will include a version compatibility layer to ensure older models still work if a user skips updates. Updates will be optional, with checksums and changelogs provided pre-download.</p> <p>Update Flow: When online, users will see a prompt if a newer model version is available. Downloads will be resumable, and stored in a dedicated app directory. Model hot-swapping will be supported at runtime where possible to avoid requiring app restarts.</p>"},{"location":"brainstorming/#local-rag-data-ingestion-and-management","title":"Local RAG Data Ingestion and Management","text":"<p>Ingestion Sources: The local knowledge base will be populated through two modes:</p> <ul> <li> <p>Manual Import: Users can import PDFs, text documents, notes, or web clippings.</p> </li> <li> <p>Auto Ingestion (optional): With explicit user consent, the app can scan and process local files (e.g., downloads, notes) based on user-configured folders.</p> </li> </ul> <p>Indexing &amp; Embeddings: All documents will be chunked and vectorized using a local embedding model (MiniLM or similar). For performance, chunking and embedding jobs will run asynchronously in the background using a task queue (e.g., via Swift Concurrency or C++ threads). The <code>sqlite-vec</code> extension will be used to store and search these vector embeddings directly within the SQLite database.\u00a0\u00a0</p> <p>Efficiency Strategies:</p> <ul> <li> <p>Indexing will use a lightweight on-device vector store (e.g., <code>sqlite-vec</code>).</p> </li> <li> <p>Modified files will be re-embedded incrementally based on file hashes and timestamps.</p> </li> <li> <p>A daily or user-defined refresh cycle will maintain embedding freshness.</p> </li> </ul>"},{"location":"brainstorming/#security-model-for-local-data-and-ipc","title":"Security Model for Local Data and IPC","text":"<p>Saira will adhere to GDPR and CCPA compliance from day one, including data portability, deletion rights, and clear consent flows. HIPAA is not applicable unless the application expands into clinical domains. A privacy dashboard, audit logs, and consent-based emotion/memory recording will be included. All data will be encrypted both in transit and at rest with fine-grained permission controls.</p> <p>Local Data at Rest:</p> <ul> <li> <p>The SQLite database will be encrypted using SQLCipher or equivalent with user-derived keys (e.g., secure passphrase or biometric unlock).</p> </li> <li> <p>All AI model files and embeddings will be stored in sandboxed app directories with file-level encryption and App Sandbox entitlements enabled.</p> </li> </ul> <p>IPC &amp; Local Processes (macOS):</p> <ul> <li> <p>Inter-process communication between the UI (SwiftUI) and native services (ASR, TTS, LLM) will use macOS XPC services with entitlement-based isolation.\u00a0\u00a0</p> </li> <li> <p>Shared memory or temporary files will be passed using secure, authenticated pipes.</p> </li> <li> <p>Each subprocess will run in a privilege-restricted context, with access to only the data it needs (principle of least privilege).</p> </li> </ul> <p>Hardening Measures:</p> <ul> <li> <p>Use of System Integrity Protection (SIP) and App Transport Security (ATS) settings.</p> </li> <li> <p>Local integrity checks and cryptographic signatures for AI model files.</p> </li> </ul>"},{"location":"brainstorming/#error-reporting-and-diagnostics-for-offline-ai","title":"Error Reporting and Diagnostics for Offline AI","text":"<p>Capture Mechanism: The app will maintain offline-friendly diagnostics logs, capturing crash traces, performance metrics (e.g., CPU/RAM usage), and AI-specific flags (e.g., model load time, inference latency, I/O failures). These logs will be stored locally and encrypted, with users given the option to review and share logs manually for support.</p> <p>Privacy-Aware Debugging:</p> <ul> <li> <p>No raw conversation data or user content will be logged.</p> </li> <li> <p>Logs will contain only non-identifiable metadata, such as model name, stack traces, error codes, and anonymized module states.</p> </li> <li> <p>Diagnostic bundles will be exportable as.zip files, with toggles in settings to enable/disable debug logging.</p> </li> </ul> <p>Developer Mode (optional): An opt-in \u201cdeveloper mode\u201d can expose verbose logs, including module health stats and memory profiling. Option to simulate sync/debug flows and collect repro cases locally.</p>"},{"location":"brainstorming/#conclusions","title":"Conclusions","text":"<p>The proposed architecture for the voice-enabled macOS application centers on delivering a privacy-preserving, high-performance experience through on-device AI processing. The strategic shift to Swift and SwiftUI for the frontend ensures optimal native performance and seamless integration with the Apple ecosystem, directly addressing the stability concerns of cross-platform UI frameworks. This native approach, combined with Swift's robust C++ interoperability, provides a solid foundation for integrating the core AI inference engines (<code>llama.cpp</code>, <code>whisper.cpp</code>, Piper TTS, and advanced SER models).</p> <p>The commitment to on-device inference ensures user privacy and low latency, leveraging Apple Silicon optimizations on target devices like the MacBook M1. The integration of <code>sqlite-vec</code> with SQLite addresses the challenge of local RAG by embedding vector search capabilities directly into the application's local database, simplifying the architecture and supporting the offline-first principle.</p> <p>The audio processing layer, critical for real-time voice interaction, will rely on direct interaction with macOS Core Audio via custom native modules to ensure optimal performance and stability. Future enhancements, such as advanced long-term memory and nuanced emotion detection, will build upon this foundation, leveraging specialized models and secure inter-process communication via macOS XPC services.</p> <p>Overall, the architectural strategy prioritizes a robust, privacy-centric user experience, with a clear roadmap for advanced features and cross-device continuity via optional encrypted cloud sync. Successful implementation will hinge on navigating the complexities of integrating cutting-edge AI models with a native macOS UI, while ensuring the stability and performance expected of a premium application. This requires a strong focus on native development expertise, meticulous attention to licensing implications for open-source AI components, and a proactive approach to managing offline data updates and comprehensive security measures.</p>"},{"location":"changelog/","title":"\ud83d\udcd8 Changelog","text":"<p>All notable changes to the Saira \u2013 Emotion-Aware AI Companion project will be documented in this file.</p> <p>This project adheres to Semantic Versioning and follows the Keep a Changelog principles.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initialized monorepo structure with <code>Saira</code> and <code>SairaBackendServices</code></li> <li>Set up React Native for macOS with TypeScript and ESLint</li> <li>Implemented initial folder structure for frontend (components, screens, services)</li> <li>Integrated React Navigation with macOS support</li> <li>Created reusable <code>Button</code> and <code>TextInput</code> components</li> <li>Implemented basic theme provider (light theme)</li> <li> <p>Added root-level error boundary handling for UI errors</p> </li> <li> <p>Set up Node.js backend with Express, TypeScript, and dotenv</p> </li> <li>Implemented <code>/api/v1/health</code> endpoint for server readiness check</li> <li>Added Winston logging and error-handling middleware</li> <li>Configured ts-node-dev for live reload during development</li> <li>Established service architecture: <code>ai_inference_server</code>, <code>data_manager</code>, <code>ipc_handler</code>, etc.</li> <li> <p>Stubbed Python services for model downloading and RAG ingestion</p> </li> <li> <p>Integrated MkDocs for project documentation</p> </li> <li>Added GitHub Actions workflow for automated docs build and deployment</li> </ul>"},{"location":"changelog/#010-2025-06-26","title":"[0.1.0] \u2013 2025-06-26","text":""},{"location":"changelog/#initial-milestone","title":"Initial Milestone","text":"<p>\u2705 Frontend:</p> <ul> <li>React Native macOS project bootstrapped</li> <li>TypeScript, ESLint, Prettier integrated</li> <li>Functional Hello World screen added</li> </ul> <p>\u2705 Backend:</p> <ul> <li>Node.js project initialized with TypeScript support</li> <li>Express server with health check endpoint working</li> <li>Development config and logger set up</li> </ul> <p>\u2705 Documentation:</p> <ul> <li>MkDocs configured for technical documentation</li> <li>CI/CD set up to deploy docs to GitHub Pages</li> </ul>"},{"location":"changelog/#001-2025-06-17","title":"[0.0.1] \u2013 2025-06-17","text":""},{"location":"changelog/#project-bootstrap","title":"Project Bootstrap","text":"<ul> <li>Initialized Git repository and project structure</li> <li>Added <code>README.md</code>, <code>LICENSE</code>, and <code>.gitignore</code></li> <li>Set up initial <code>docs/</code> folder and MkDocs YAML config</li> </ul> <p>\u2139\ufe0f Upcoming versions will include model integration, voice pipeline, journaling, emotional memory features, and cross-device sync.</p>"},{"location":"contribution/","title":"\ud83e\udd1d Contributing to Saira","text":"<p>Thank you for your interest in contributing to Saira! This document provides comprehensive guidelines for contributing to the project.</p>"},{"location":"contribution/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"contribution/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18+ and npm 8+</li> <li>Python 3.9+ and pip</li> <li>Git</li> <li>GitHub CLI (recommended)</li> </ul>"},{"location":"contribution/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> </ol> <p><code>bash    gh repo fork saurabhBatav/saira-ai-companion --clone</code></p> <ol> <li>Navigate to the project</li> </ol> <p><code>bash    cd saira-ai-companion</code></p> <ol> <li>Install dependencies</li> </ol> <p><code>bash    npm install    pip install -r requirements.txt</code></p> <ol> <li>Set up development environment</li> </ol> <p><code>bash    npm run setup:dev</code></p> <ol> <li>Create a new branch <code>bash    git checkout -b feature/your-feature-name</code></li> </ol>"},{"location":"contribution/#development-workflow","title":"\ud83d\udccb Development Workflow","text":""},{"location":"contribution/#branch-naming-convention","title":"Branch Naming Convention","text":"<ul> <li><code>feature/feature-name</code> - New features</li> <li><code>bugfix/bug-description</code> - Bug fixes</li> <li><code>docs/documentation-update</code> - Documentation changes</li> <li><code>refactor/component-name</code> - Code refactoring</li> <li><code>test/test-description</code> - Test additions/updates</li> </ul>"},{"location":"contribution/#commit-message-format","title":"Commit Message Format","text":"<p>Follow the Conventional Commits specification:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Types:</p> <ul> <li><code>feat</code>: A new feature</li> <li><code>fix</code>: A bug fix</li> <li><code>docs</code>: Documentation only changes</li> <li><code>style</code>: Changes that do not affect the meaning of the code</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature</li> <li><code>test</code>: Adding missing tests or correcting existing tests</li> <li><code>chore</code>: Changes to the build process or auxiliary tools</li> </ul> <p>Examples:</p> <pre><code>feat(voice): add wake word detection\nfix(emotion): resolve emotion classification accuracy\ndocs: update API documentation\ntest(memory): add vector database tests\n</code></pre>"},{"location":"contribution/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"contribution/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm run test:watch\n\n# Run E2E tests\nnpm run test:e2e\n\n# Run Python tests\npython -m pytest tests/\n</code></pre>"},{"location":"contribution/#writing-tests","title":"Writing Tests","text":"<ul> <li>Write unit tests for all new functions</li> <li>Add integration tests for new features</li> <li>Update existing tests when modifying functionality</li> <li>Ensure all tests pass before submitting PR</li> </ul>"},{"location":"contribution/#code-style","title":"\ud83c\udfa8 Code Style","text":""},{"location":"contribution/#javascripttypescript","title":"JavaScript/TypeScript","text":"<ul> <li>Use ESLint configuration provided</li> <li>Follow Prettier formatting</li> <li>Use meaningful variable names</li> <li>Add JSDoc comments for functions</li> </ul>"},{"location":"contribution/#python","title":"Python","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use Black for formatting</li> <li>Add type hints where appropriate</li> <li>Use docstrings for functions and classes</li> </ul>"},{"location":"contribution/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Keep functions small and focused</li> <li>Use descriptive commit messages</li> <li>Comment complex logic</li> <li>Remove unused code and imports</li> </ul>"},{"location":"contribution/#documentation","title":"\ud83d\udcda Documentation","text":""},{"location":"contribution/#code-documentation","title":"Code Documentation","text":"<ul> <li>Add JSDoc/docstring comments for all public functions</li> <li>Include parameter and return type information</li> <li>Provide usage examples for complex functions</li> </ul>"},{"location":"contribution/#readme-updates","title":"README Updates","text":"<ul> <li>Update README.md if your changes affect setup or usage</li> <li>Add new features to the features list</li> <li>Update installation instructions if needed</li> </ul>"},{"location":"contribution/#pull-request-process","title":"\ud83d\udd0d Pull Request Process","text":""},{"location":"contribution/#before-submitting","title":"Before Submitting","text":"<ol> <li>Test your changes</li> </ol> <p><code>bash    npm test    npm run lint    python -m pytest</code></p> <ol> <li> <p>Update documentation</p> </li> <li> <p>Update relevant documentation files</p> </li> <li>Add inline code comments</li> <li> <p>Update README if necessary</p> </li> <li> <p>Check your branch <code>bash    git checkout main    git pull origin main    git checkout your-feature-branch    git rebase main</code></p> </li> </ol>"},{"location":"contribution/#submitting-your-pr","title":"Submitting Your PR","text":"<ol> <li>Push your branch</li> </ol> <p><code>bash    git push origin feature/your-feature-name</code></p> <ol> <li> <p>Create Pull Request</p> </li> <li> <p>Use the provided PR template</p> </li> <li>Write a clear description of changes</li> <li>Reference related issues</li> <li> <p>Add screenshots if UI changes are involved</p> </li> <li> <p>Address Review Feedback</p> </li> <li>Respond to reviewer comments</li> <li>Make requested changes</li> <li>Push updates to the same branch</li> </ol>"},{"location":"contribution/#reporting-issues","title":"\ud83d\udc1b Reporting Issues","text":""},{"location":"contribution/#bug-reports","title":"Bug Reports","text":"<p>Use the bug report template and include:</p> <ul> <li>Clear description of the issue</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (OS, version, device)</li> <li>Screenshots or error logs</li> </ul>"},{"location":"contribution/#feature-requests","title":"Feature Requests","text":"<p>Use the feature request template and include:</p> <ul> <li>Clear description of the proposed feature</li> <li>Problem it solves</li> <li>Proposed solution</li> <li>Alternative approaches considered</li> </ul>"},{"location":"contribution/#issue-labels","title":"\ud83c\udff7\ufe0f Issue Labels","text":""},{"location":"contribution/#priority-labels","title":"Priority Labels","text":"<ul> <li><code>critical</code> - Urgent issues that block functionality</li> <li><code>high-priority</code> - Important issues to address soon</li> <li><code>medium-priority</code> - Standard priority issues</li> <li><code>low-priority</code> - Nice-to-have improvements</li> </ul>"},{"location":"contribution/#type-labels","title":"Type Labels","text":"<ul> <li><code>bug</code> - Something isn't working</li> <li><code>enhancement</code> - New feature or request</li> <li><code>documentation</code> - Improvements to documentation</li> <li><code>question</code> - Further information is requested</li> </ul>"},{"location":"contribution/#component-labels","title":"Component Labels","text":"<ul> <li><code>voice</code> - Voice processing related</li> <li><code>emotion</code> - Emotion detection related</li> <li><code>memory</code> - Memory system related</li> <li><code>ui-ux</code> - User interface/experience</li> <li><code>mobile</code> - iOS app related</li> <li><code>speaker</code> - Smart speaker related</li> </ul>"},{"location":"contribution/#project-phases","title":"\ud83c\udfaf Project Phases","text":"<p>We're currently working through these phases:</p> <ol> <li>Phase 1: Planning &amp; Setup (Current)</li> <li>Phase 2: Core Architecture Development</li> <li>Phase 3: Feature Integration &amp; Proactive Intelligence</li> <li>Phase 4: Multi-Modal Output &amp; Emotional UX</li> <li>Phase 5: UI/UX Polish &amp; Feedback Loops</li> <li>Phase 6: Cross-Platform Support &amp; Device Sync</li> <li>Phase 7: Launch &amp; Iteration Cycles</li> </ol> <p>See our Project Board for current status.</p>"},{"location":"contribution/#security","title":"\ud83d\udd12 Security","text":""},{"location":"contribution/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Please do not report security vulnerabilities through public GitHub issues. Instead, email security@saira-ai.com with:</p> <ul> <li>Description of the vulnerability</li> <li>Steps to reproduce</li> <li>Potential impact</li> <li>Any suggested fixes</li> </ul>"},{"location":"contribution/#security-guidelines","title":"Security Guidelines","text":"<ul> <li>Never commit sensitive data (API keys, passwords, etc.)</li> <li>Use environment variables for configuration</li> <li>Follow secure coding practices</li> <li>Validate all user inputs</li> </ul>"},{"location":"contribution/#getting-help","title":"\ud83d\udcde Getting Help","text":""},{"location":"contribution/#communication-channels","title":"Communication Channels","text":"<ul> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83d\udce7 Email: contributors@saira-ai.com</li> </ul>"},{"location":"contribution/#resources","title":"Resources","text":"<ul> <li>Project Documentation</li> <li>API Reference</li> <li>Architecture Guide</li> <li>Development Setup</li> </ul>"},{"location":"contribution/#recognition","title":"\ud83c\udfc6 Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>README.md contributors section</li> <li>Release notes for significant contributions</li> <li>Annual contributor appreciation posts</li> </ul>"},{"location":"contribution/#license","title":"\ud83d\udcdc License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p> <p>Thank you for contributing to Saira! Your efforts help make AI more human and accessible. \ud83d\ude4f</p>"},{"location":"deployment/","title":"Deployment Instructions","text":"<p>This guide explains how to deploy Saira AI Companion across supported platforms and environments.</p>"},{"location":"deployment/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Node.js: 18+</li> <li>Python: 3.9+</li> <li>macOS: 12+ (for Mac app deployment)</li> <li>iOS: 15+ (for iPhone app deployment)</li> <li>Build Tools: Xcode (for iOS), Homebrew, etc.</li> <li>Production Environment: Secure VM or server (for backend, if any)</li> <li>Environment Variables: See <code>config/</code> and <code>.env.example</code></li> </ul>"},{"location":"deployment/#2-local-deployment-development","title":"2. Local Deployment (Development)","text":"<pre><code># Start backend and frontend\nnpm install\npip install -r requirements.txt\nnpm run dev\n</code></pre> <ul> <li>For iOS: Open <code>src/mobile/</code> in Xcode and run on simulator or device.</li> <li>For Mac app: Build and run from the appropriate directory (instructions TBD).</li> </ul>"},{"location":"deployment/#3-production-deployment","title":"3. Production Deployment","text":"<p>This section will expand as the deployment target is finalized.</p> <ul> <li>Build production assets: <code>bash   npm run build</code></li> <li>Package and distribute: </li> <li>For Mac: Use Xcode or Electron builder.</li> <li> <p>For iOS: Archive and distribute via TestFlight or App Store.</p> </li> <li> <p>Environment variables: </p> </li> <li>Copy <code>.env.example</code> to <code>.env</code> and fill in production values.</li> <li>Store secrets securely (do not commit to the repo).</li> </ul>"},{"location":"deployment/#4-monitoring-logging","title":"4. Monitoring &amp; Logging","text":"<ul> <li>Ensure proper logging is enabled for debugging.</li> <li>Monitor resource usage on devices (CPU, memory, storage).</li> </ul>"},{"location":"deployment/#5-rollback-and-recovery","title":"5. Rollback and Recovery","text":"<ul> <li>Always keep backups of user data (encrypted, local).</li> <li>For app updates, ensure users can revert to a previous version if needed.</li> </ul>"},{"location":"deployment/#6-security-considerations","title":"6. Security Considerations","text":"<ul> <li>All processing and storage are local; no personal data leaves the device.</li> <li>End-to-end encryption for device sync.</li> <li>Follow principle of least privilege for any environment-specific permissions.</li> </ul>"},{"location":"deployment/#7-common-issues","title":"7. Common Issues","text":"<ul> <li>Dependency conflicts: Run <code>npm ci</code> or clean <code>node_modules</code>.</li> <li>Build errors: Check Node/Python/Xcode versions and update as needed.</li> <li>Permission issues: Ensure correct app permissions (microphone, storage, etc.).</li> </ul> <p>Last updated: 2025-06-09</p>"},{"location":"development/","title":"Development Guide","text":"<p>Welcome to the Saira AI Companion development guide! This document helps new and existing contributors set up, develop, test, and maintain code effectively.</p>"},{"location":"development/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Node.js: 18+</li> <li>Python: 3.9+</li> <li>macOS: 12+ (for Mac app)</li> <li>iOS: 15+ (for iPhone app)</li> <li>Recommended: Docker (for future containerization support)</li> </ul>"},{"location":"development/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/saurabhBatav/saira-ai-companion.git\ncd saira-ai-companion\n\n# Install dependencies\nnpm install\npip install -r requirements.txt\n\n# Set up development environment (if needed)\nnpm run setup:dev\n</code></pre> <ul> <li>Refer to <code>config/</code> and <code>.env.example</code> for environment variable setup.</li> <li>For iOS development, open <code>src/mobile/</code> in Xcode and install dependencies as needed.</li> </ul>"},{"location":"development/#3-codebase-structure","title":"3. Codebase Structure","text":"<pre><code>src/\n  core/         # Core AI and logic\n  voice/        # Speech processing\n  emotion/      # Emotion analysis\n  memory/       # Memory management\n  ui/           # User interface\n  mobile/       # iOS app\n  speaker/      # Smart speaker\ndocs/           # Documentation\ntests/          # Tests\nscripts/        # Automation/deployment\nconfig/         # Environment configs\n</code></pre> <p>See docs/architecture.md for a detailed breakdown.</p>"},{"location":"development/#4-coding-standards","title":"4. Coding Standards","text":"<ul> <li>JavaScript/TypeScript: Use Prettier and ESLint.   <code>bash   npm run lint   npm run format</code></li> <li>Python: Follow PEP8. Use Black and Flake8.   <code>bash   black .   flake8 .</code></li> <li>Add/maintain type annotations (Python) and JSDoc/TSDoc (JS/TS).</li> <li>Keep code modular, documented, and well-tested.</li> </ul>"},{"location":"development/#5-running-testing","title":"5. Running &amp; Testing","text":"<ul> <li>Start development server:</li> </ul> <p><code>bash   npm run dev</code></p> <ul> <li>Run tests:</li> </ul> <p><code>bash   npm run test     # JS/TS   pytest           # Python</code></p> <ul> <li>Coverage reports:</li> <li>JS/TS: <code>npm run test:coverage</code></li> <li>Python: <code>pytest --cov</code></li> </ul>"},{"location":"development/#6-documentation","title":"6. Documentation","text":"<ul> <li>All major changes must update docs in <code>docs/</code>.</li> <li>API changes: update <code>docs/api/</code>.</li> <li>Architecture or process changes: update <code>docs/architecture.md</code> and <code>docs/development.md</code>.</li> <li>Use <code>mkdocs serve</code> to preview docs locally.</li> </ul>"},{"location":"development/#7-submitting-changes","title":"7. Submitting Changes","text":"<ul> <li>Follow the CONTRIBUTING.md guidelines for branching, commits, and PRs.</li> <li>Ensure all tests and linting pass before submitting a PR.</li> <li>Reference related issues in your PR description.</li> </ul>"},{"location":"development/#8-troubleshooting","title":"8. Troubleshooting","text":"<ul> <li>Check GitHub Issues for common problems.</li> <li>Ask for help in Discussions or contact maintainers.</li> </ul> <p>Last updated: 2025-06-09</p>"},{"location":"feature_implementation/","title":"Granular insights and step-by-step implementation guidance for each feature","text":""},{"location":"feature_implementation/#file-system","title":"File System","text":"<p>The project will adopt a monorepo structure to manage the React Native for macOS frontend application and the Node.js/Python backend services, facilitating integration and streamlined dependency management.</p> <pre><code>saira-app/\n\u251c\u2500\u2500 .git/                                    # Git repository\n\u251c\u2500\u2500 .github/                                 # GitHub specific configurations\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 ci.yml                           # GitHub Actions CI/CD pipeline for macOS and iOS builds [1, 2]\n\u251c\u2500\u2500 README.md                                # Project README\n\u251c\u2500\u2500 LICENSE                                  # Project license\n\u251c\u2500\u2500 docs/                                    # Project documentation\n\u2502   \u251c\u2500\u2500 architecture/\n\u2502   \u2502   \u251c\u2500\u2500 system_diagram.md                # High-level system architecture diagrams\n\u2502   \u2502   \u251c\u2500\u2500 api_spec.md                      # Comprehensive API specifications\n\u2502   \u2502   \u251c\u2500\u2500 db_schema.md                     # Database schema definitions\n\u2502   \u2502   \u2514\u2500\u2500 security_model.md                # Detailed security considerations\n\u2502   \u2514\u2500\u2500 features/\n\u2502       \u251c\u2500\u2500 onboarding.md\n\u2502       \u251c\u2500\u2500 voice_interaction.md\n\u2502       \u251c\u2500\u2500 long_term_memory.md\n\u2502       \u251c\u2500\u2500 data_sync.md\n\u2502       \u251c\u2500\u2500 personality_modes.md\n\u2502       \u251c\u2500\u2500 journaling.md\n\u2502       \u2514\u2500\u2500 nudges.md\n\u2502\n\u251c\u2500\u2500 Saira/                     # Main macOS/iOS application (React Native + TypeScript)\n\u2502   \u251c\u2500\u2500 package.json                         # Node.js dependencies for React Native [3]\n\u2502   \u251c\u2500\u2500 tsconfig.json                        # TypeScript configuration [4, 5]\n\u2502   \u251c\u2500\u2500 index.js                             # Entry point for React Native app [6]\n\u2502   \u251c\u2500\u2500 App.tsx                              # Root React Native component\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 components/                      # Reusable UI components (TypeScript/TSX)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Button.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 TextInput.tsx\n\u2502   \u2502   \u2502   \u2514\u2500\u2500...\n\u2502   \u2502   \u251c\u2500\u2500 screens/                         # Main application screens (TypeScript/TSX)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 OnboardingScreen.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ConversationScreen.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 JournalingScreen.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ProfileSettingsScreen.tsx\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 DeepTalkScreen.tsx\n\u2502   \u2502   \u251c\u2500\u2500 navigation/                      # React Navigation setup [3]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 AppNavigator.tsx\n\u2502   \u2502   \u251c\u2500\u2500 services/                        # JavaScript/TypeScript services for app logic\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 AppStateManager.ts           # Manages overall app state and lifecycle\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 DataStoreService.ts          # High-level interface for SQLite operations\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 IPCService.ts                # Manages communication with Node.js/Python backend processes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 NotificationService.ts       # Handles macOS notifications\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 SyncService.ts               # Manages cloud synchronization logic\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 TaskManagerClient.ts         # NEW: Client for communicating with TaskManager\n\u2502   \u2502   \u251c\u2500\u2500 models/                          # TypeScript data models (interfaces, types)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 AppModels.ts                 # User, Message, Memory, Goal, etc.\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 DTOs.ts                      # Data Transfer Objects for IPC\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 TaskModels.ts                # NEW: Models for Task and TaskStatus\n\u2502   \u2502   \u2514\u2500\u2500 utils/                           # Utility functions and helpers\n\u2502   \u2502       \u251c\u2500\u2500 Constants.ts\n\u2502   \u2502       \u251c\u2500\u2500 Logger.ts                    # Structured logging utility\n\u2502   \u2502       \u2514\u2500\u2500 Permissions.ts               # Handles system permissions\n\u2502   \u251c\u2500\u2500 ios/                                 # iOS native project (Objective-C/Swift)\n\u2502   \u2502   \u251c\u2500\u2500 Saira.xcodeproj/\n\u2502   \u2502   \u251c\u2500\u2500 Saira/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 AppDelegate.mm               # Objective-C++ for native module bridging [7, 8, 9]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Info.plist\n\u2502   \u2502   \u2514\u2500\u2500 Podfile                          # CocoaPods for iOS dependencies [1]\n\u2502   \u251c\u2500\u2500 macos/                               # macOS native project (Objective-C/Swift)\n\u2502   \u2502   \u251c\u2500\u2500 Saira.xcodeproj/\n\u2502   \u2502   \u251c\u2500\u2500 Saira/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 AppDelegate.mm               # Objective-C++ for native module bridging [7, 8, 9]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Info.plist\n\u2502   \u2502   \u2514\u2500\u2500 Podfile                          # CocoaPods for macOS dependencies [1]\n\u2502   \u251c\u2500\u2500 __tests__/                           # Jest tests for React Native components [3]\n\u2502   \u2514\u2500\u2500 e2e/                                 # End-to-end tests (e.g., Detox) [3]\n\u2502\n\u251c\u2500\u2500 SairaBackendServices/                    # Node.js and Python backend services\n\u2502   \u251c\u2500\u2500 package.json                         # Node.js dependencies for backend services\n\u2502   \u251c\u2500\u2500 tsconfig.json                        # TypeScript configuration for Node.js services\n\u2502   \u251c\u2500\u2500 node_modules/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 node_services/                   # Node.js services (TypeScript/JavaScript)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 main.ts                      # Entry point for Node.js backend process\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai_inference_server.ts       # Local HTTP/WebSocket server for AI models\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 audio_processor.ts           # Handles audio I/O via native addons\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data_manager.ts              # Handles SQLite operations\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_manager.ts             # Manages AI model loading and updates\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ipc_handler.ts               # Handles IPC messages from frontend\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 task_manager.ts              # NEW: TaskManager service responsible for queuing and executing tasks\n\u2502   \u2502   \u251c\u2500\u2500 python_services/                 # Python scripts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 rag_ingestion.py             # For RAG document processing (chunking, embedding)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_downloader.py          # For AI model downloads\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ser_feature_extractor.py     # For openSMILE feature extraction (if Python wrapper used)\n\u2502   \u2502   \u251c\u2500\u2500 native_addons/                   # C++ Node.js native addons (N-API)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 audio_io_addon/              # Wrapper for Core Audio (PortAudio)\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 binding.gyp\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 src/audio_io.cpp\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai_inference_addon/          # Wrapper for llama.cpp, whisper.cpp, Piper, Porcupine, openSMILE/SenseVoice\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 binding.gyp\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 src/ai_inference.cpp\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 sqlite_vec_addon/            # Wrapper for sqlite-vec]\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 binding.gyp\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 src/sqlite_vec.cpp\n\u2502   \u2502   \u2514\u2500\u2500 models/                          # Pre-trained AI models (downloaded by app)\n\u2502   \u2502       \u251c\u2500\u2500 llm/\n\u2502   \u2502       \u251c\u2500\u2500 asr/\n\u2502   \u2502       \u251c\u2500\u2500 tts/\n\u2502   \u2502       \u251c\u2500\u2500 ser/\n\u2502   \u2502       \u2514\u2500\u2500 embeddings/\n\u2502   \u251c\u2500\u2500 tests/                               # Jest tests for Node.js services, Pytest for Python scripts\n\u2502   \u2514\u2500\u2500 Dockerfile                           # For local development environment setup [10, 11]\n\u2502\n\u251c\u2500\u2500 Tools/                                   # Development and utility scripts\n\u2502   \u2514\u2500\u2500 dev_scripts/                         # Miscellaneous development scripts\n\u2502\n\u251c\u2500\u2500 .gitignore                               # Git ignore file\n\u251c\u2500\u2500 .prettierrc                              # Prettier configuration [1]\n\u2514\u2500\u2500 .eslintrc.js                             # ESLint configuration [1]\n</code></pre>"},{"location":"feature_implementation/#feature-specifications","title":"Feature Specifications","text":"<p>Feature: Core Application Framework (UI &amp; App Logic)</p> <p>Feature Goal: To provide the foundational native-like macOS user interface and the core application logic that orchestrates interactions between the UI, local data storage, and the underlying Node.js/Python backend services. This feature encompasses the initial user onboarding, general navigation, and the display of core application data.</p> <p>Any API Relationships: This feature serves as the central hub, interacting with almost all other features:</p> <ul> <li>Real-time Voice Interaction: Initiates and receives data from ASR, TTS, Wake Word, and SER services via <code>IPCService</code> (communicating with <code>ai_inference_server.ts</code> and <code>audio_processor.ts</code>).</li> <li>Local Long-Term Memory (RAG): Manages CRUD operations for memories and goals through <code>DataStoreService</code>, which interacts with <code>data_manager.ts</code> (Node.js) and potentially <code>rag_ingestion.py</code> (Python).</li> <li>Offline-First Data Sync: Triggers and monitors synchronization processes via <code>SyncService</code>.</li> <li>Personality Modes &amp; Customization: Loads and applies personality profiles and user settings, influencing LLM behavior via <code>IPCService</code>.</li> <li>Daily Check-ins &amp; Journaling: Provides UI for creating, viewing, and managing journal entries and mood summaries, now primarily interacting with <code>TaskManagerClient.ts</code> to initiate background tasks.</li> <li>Proactive Nudges &amp; Notifications: Configures and displays notifications via <code>NotificationService</code>.</li> </ul> <p>Detailed Feature Requirements:</p> <ul> <li>User Onboarding Flow:<ul> <li>Initial Setup: Guide new users through personality preferences, initial goals, and desired tone style.</li> <li>Data Collection: Securely collect initial user data (e.g., name, preferred AI personality, initial goals).</li> <li>Consent Management: Clearly present privacy policy and obtain explicit consent for data collection (e.g., emotion/memory recording) and optional cloud sync, adhering to GDPR and CCPA.</li> <li>Progress Tracking: Visually indicate onboarding progress.</li> </ul> </li> <li>Main Application Navigation:<ul> <li>Tab-based Navigation: Implement a clear tab bar for primary sections: Conversation, Daily Check-ins/Journal, Deep Talk, and Profile Settings.</li> <li>Responsive Layout: Ensure the UI adapts gracefully to various macOS window sizes and future iOS screen sizes.</li> </ul> </li> <li>Conversation View (MVP - Voice Interaction Only):<ul> <li>Voice Input Indicator: Display a visual indicator (e.g., waveform, microphone icon) when the app is actively listening or processing voice input.</li> <li>Real-time Transcription Display: Show transcribed text as the user speaks.</li> <li>AI Response Display: Present AI's spoken responses as text in a chat-like interface.</li> <li>Basic Chat History: Display a scrollable history of the current conversation session.</li> </ul> </li> <li>Daily Check-ins &amp; Journaling Tab:<ul> <li>Mood Summary Display: Show a daily summary of detected moods and emotional patterns.</li> <li>Emotional Check-in Interface: Allow users to manually log their emotional state.</li> <li>Journal Entry Creation/Viewing: Provide an interface for creating, viewing, and editing journal entries. Journal entry creation will initiate a task via <code>TaskManagerClient</code> and display a \"Working on it...\" message, followed by confirmation via WebSocket.</li> </ul> </li> <li>Deep Talk Tab:<ul> <li>Dedicated Conversation Mode: Offer a specialized conversational mode for deeper exploration of life events, emotional patterns, or relationship issues. This mode will leverage the RAG system more heavily.</li> </ul> </li> <li>Profile Settings Tab:<ul> <li>Personality Preferences: Allow users to configure AI personality styles (e.g., \"Mom,\" \"Coach,\" \"Best Friend\").</li> <li>Memory Visibility: Provide controls for users to review and manage what memories the AI retains.</li> <li>Privacy Settings: Centralized dashboard for managing data consent, deletion rights, and audit logs.</li> <li>Emotional Tone Preferences: Allow users to fine-tune how the AI perceives and responds to their emotional tone.</li> <li>Offline Sync Configuration: Settings for enabling/disabling cloud sync, frequency, and conflict resolution preferences.</li> <li>Model Management: Display currently loaded AI models and provide options for in-app updates.</li> </ul> </li> <li>Proactive Nudges &amp; Notifications:<ul> <li>Notification Display: Receive and display proactive nudges (reminders, positive reinforcement, wellness suggestions) via macOS Notification Center.</li> <li>Configuration: Allow users to enable/disable and customize types/frequency of nudges in settings.</li> </ul> </li> </ul>"},{"location":"feature_implementation/#detailed-implementation-guide","title":"Detailed Implementation Guide","text":"<p>System Architecture Overview</p> <p>High-level Architecture: The application will consist of a React Native for macOS frontend (<code>Saira/</code>) and a Node.js/Python backend (<code>SairaBackendServices/</code>). The frontend handles the UI and high-level application logic. The backend handles all performance-critical AI inference, low-level audio I/O, and local data management. Communication between the frontend and backend will primarily occur via Inter-Process Communication (IPC) using local HTTP/WebSocket servers or Node.js <code>child_process</code> module.</p> <p>Technology Stack:</p> <ul> <li>Frontend UI: React Native for macOS (with TypeScript). Justification: Allows for JavaScript/React-based UI development, enabling code reuse for future iOS expansion. TypeScript enhances code quality and maintainability for large projects. React Native renders native components for a near-native experience.</li> <li>Backend Services (Orchestration): Node.js (with TypeScript). Justification: Node.js is well-suited for I/O-intensive tasks and building scalable APIs. It can manage concurrent requests efficiently due to its event-driven, non-blocking architecture. It also has a large package ecosystem (npm).</li> <li>Backend Services (AI Inference &amp; Low-Level Operations): Python and C++ native addons for Node.js. Justification: While Node.js and Python can wrap C++ libraries, the underlying AI models (<code>llama.cpp</code>, <code>whisper.cpp</code>, Piper, Porcupine, <code>openSMILE</code>, SenseVoice) are highly optimized C/C++ implementations.<ul> <li>C++ Native Addons (N-API): For direct, low-latency interaction with macOS Core Audio and for wrapping performance-critical AI libraries. This is crucial to mitigate the performance overhead of pure JavaScript/Python for real-time AI inference. N-API allows C++ code to be exposed to Node.js.[9]</li> <li>Python: For tasks like RAG document ingestion (chunking, embedding generation) where its rich ecosystem of ML libraries is beneficial. Python can also be used for specific AI model inference if a robust Node.js binding is unavailable or less performant, exposed via a local HTTP server. Python is generally slower for CPU-intensive tasks than C++.</li> </ul> </li> <li>Local Data Storage: SQLite with <code>sqlite-vec</code> extension. Justification: Embedded, serverless, and cross-platform, ideal for local data persistence and vector search. Node.js has bindings for SQLite and <code>sqlite-vec</code>]. <code>op-sqlite</code> for React Native also supports <code>sqlite-vec</code>].</li> <li>Deployment Architecture: The React Native application will be bundled as a standard macOS .app package. The Node.js backend services will run as a separate process, potentially launched as a child process by the React Native app or as a background service. C++ native addons will be compiled and bundled with the Node.js services. Python scripts will be bundled and executed by the Node.js process.</li> <li>Integration Points:<ul> <li>macOS Core Audio: Via C++ native addon (PortAudio wrapper or direct Core Audio calls) for Node.js.</li> <li>macOS Notification Center: React Native modules for native notifications.</li> <li>Local File System: For RAG document ingestion and AI model storage.</li> <li>Cloud Sync Service (Future): Custom E2EE protocol for optional data synchronization].</li> </ul> </li> </ul>"},{"location":"feature_implementation/#database-schema-design","title":"Database Schema Design","text":"<p>The local database will be SQLite, encrypted with SQLCipher. All tables will include <code>id</code> (PRIMARY KEY), <code>created_at</code> (TIMESTAMP), <code>updated_at</code> (TIMESTAMP), and <code>user_id</code> (FOREIGN KEY to Users table) where applicable.</p> <ul> <li> <p>Users Table:</p> <ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>name</code> (TEXT, NOT NULL)</li> <li><code>onboarding_completed</code> (INTEGER, BOOLEAN, DEFAULT 0)</li> <li><code>current_personality_profile_id</code> (TEXT, FOREIGN KEY to PersonalityProfiles.id)</li> <li><code>last_active_at</code> (TIMESTAMP)</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>sync_enabled</code> (INTEGER, BOOLEAN, DEFAULT 0)</li> <li><code>last_synced_at</code> (TIMESTAMP, NULLABLE)</li> <li><code>sync_frequency</code> (TEXT, ENUM: 'daily', 'weekly', 'manual', DEFAULT 'manual')</li> <li><code>encryption_key_hash</code> (TEXT, NOT NULL) - Hash of user-derived key for SQLCipher.</li> <li><code>privacy_consent_version</code> (TEXT, NOT NULL) - Tracks accepted privacy policy version.</li> <li><code>gdpr_ccpa_consent</code> (INTEGER, BOOLEAN, NOT NULL) - Explicit consent for data processing.</li> <li><code>audit_logging_enabled</code> (INTEGER, BOOLEAN, DEFAULT 1) - User preference for audit logs.</li> <li><code>data_portability_token</code> (TEXT, NULLABLE) - Token for data export.</li> <li><code>data_deletion_requested_at</code> (TIMESTAMP, NULLABLE) - Timestamp if deletion requested.</li> <li><code>profile_image_path</code> (TEXT, NULLABLE)</li> <li><code>preferred_tone_style</code> (TEXT, ENUM: 'formal', 'casual', 'empathetic', etc., DEFAULT 'empathetic')</li> <li><code>wake_word_enabled</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>proactive_nudges_enabled</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>deep_talk_mode_enabled</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>emotion_recording_consent</code> (INTEGER, BOOLEAN, NOT NULL) - Consent for emotion data recording.</li> <li><code>memory_recording_consent</code> (INTEGER, BOOLEAN, NOT NULL) - Consent for memory recording.</li> <li><code>journaling_enabled</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>last_active_status</code> (TEXT, NULLABLE) - e.g., 'online', 'offline', 'typing'</li> <li><code>last_active_device</code> (TEXT, NULLABLE) - Identifier for the last active device</li> <li><code>analytics_opt_in</code> (INTEGER, BOOLEAN, DEFAULT 0) - User consent for privacy-respecting analytics</li> </ul> </li> <li> <p>Conversations Table:</p> <ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>start_time</code> (TIMESTAMP, NOT NULL)</li> <li><code>end_time</code> (TIMESTAMP, NULLABLE)</li> <li><code>title</code> (TEXT, NULLABLE) - Auto-generated or user-edited summary.</li> <li><code>personality_profile_id</code> (TEXT, FOREIGN KEY to PersonalityProfiles.id) - Personality used during conversation.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>Messages Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>conversation_id</code> (TEXT, FOREIGN KEY to Conversations.id, NOT NULL)</li> <li><code>sender_type</code> (TEXT, ENUM: 'user', 'ai', NOT NULL)</li> <li><code>content</code> (TEXT, NOT NULL)</li> <li><code>timestamp</code> (TIMESTAMP, NOT NULL)</li> <li><code>audio_path</code> (TEXT, NULLABLE) - Local path to raw audio (if stored, with consent).</li> <li><code>emotion_arousal</code> (REAL, NULLABLE) - Detected arousal (0-1).</li> <li><code>emotion_valence</code> (REAL, NULLABLE) - Detected valence (-1 to 1).</li> <li><code>emotion_category</code> (TEXT, NULLABLE) - Categorical emotion (e.g., 'happy', 'sad').</li> <li><code>is_proactive_nudge</code> (INTEGER, BOOLEAN, DEFAULT 0) - If message is a nudge.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>Memories Table: (Semantic, Episodic, Procedural)<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>type</code> (TEXT, ENUM: 'semantic', 'episodic', 'procedural', NOT NULL)</li> <li><code>content</code> (TEXT, NOT NULL) - The actual memory text.</li> <li><code>embedding</code> (BLOB, NOT NULL) - Vector embedding of the content.</li> <li><code>source_id</code> (TEXT, NULLABLE) - ID of related conversation/journal entry/document.</li> <li><code>source_type</code> (TEXT, ENUM: 'conversation', 'journal', 'document', 'user_input', NULLABLE)</li> <li><code>timestamp</code> (TIMESTAMP, NOT NULL) - When the memory was formed/updated.</li> <li><code>relevance_score</code> (REAL, DEFAULT 0.0) - For RAG prioritization.</li> <li><code>is_active</code> (INTEGER, BOOLEAN, DEFAULT 1) - Can be toggled by user for visibility.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>Goals Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>title</code> (TEXT, NOT NULL)</li> <li><code>description</code> (TEXT, NULLABLE)</li> <li><code>status</code> (TEXT, ENUM: 'active', 'completed', 'archived', NOT NULL)</li> <li><code>due_date</code> (TIMESTAMP, NULLABLE)</li> <li><code>progress</code> (REAL, DEFAULT 0.0) - 0.0 to 1.0.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>JournalEntries Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>timestamp</code> (TIMESTAMP, NOT NULL)</li> <li><code>content</code> (TEXT, NOT NULL)</li> <li><code>detected_emotions</code> (TEXT, NULLABLE) - JSON array of detected emotions (e.g., <code>[{\"arousal\": 0.8, \"valence\": 0.7, \"category\": \"happy\"}]</code>).</li> <li><code>associated_memories</code> (TEXT, NULLABLE) - JSON array of Memory.ids.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>MoodSummaries Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>date</code> (DATE, NOT NULL, UNIQUE)</li> <li><code>summary_text</code> (TEXT, NULLABLE) - AI-generated summary of daily mood.</li> <li><code>aggregated_emotions</code> (TEXT, NULLABLE) - JSON object of aggregated emotion data for the day.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>PersonalityProfiles Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NULLABLE) - NULL for system-defined profiles.</li> <li><code>name</code> (TEXT, NOT NULL)</li> <li><code>description</code> (TEXT, NULLABLE)     <code>llm_prompt_template</code> (TEXT, NOT NULL) - Core prompt for LLM to adopt personality.</li> <li><code>tone_preferences</code> (TEXT, NULLABLE) - JSON object for specific tone adjustments.</li> <li><code>memory_visibility_rules</code> (TEXT, NULLABLE) - JSON object for rules on what memories are accessible.</li> <li><code>is_custom</code> (INTEGER, BOOLEAN, DEFAULT 0) - Indicates if user-created.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>NudgePreferences Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>type</code> (TEXT, ENUM: 'reminder', 'positive_reinforcement', 'wellness_suggestion', NOT NULL)</li> <li><code>frequency</code> (TEXT, ENUM: 'daily', 'weekly', 'monthly', 'event_based', NOT NULL)</li> <li><code>time_of_day</code> (TEXT, NULLABLE) - e.g., \"09:00\", \"18:30\".</li> <li><code>content_template</code> (TEXT, NOT NULL) - Template for nudge message.</li> <li><code>enabled</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>NudgeHistory Table:<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>nudge_preference_id</code> (TEXT, FOREIGN KEY to NudgePreferences.id, NOT NULL)</li> <li><code>timestamp</code> (TIMESTAMP, NOT NULL) - When the nudge was delivered.</li> <li><code>message_content</code> (TEXT, NOT NULL) - Actual message sent.</li> <li><code>delivered</code> (INTEGER, BOOLEAN, DEFAULT 1)</li> <li><code>interacted</code> (INTEGER, BOOLEAN, DEFAULT 0) - User interacted with notification.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>Tasks Table (NEW):<ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to Users.id, NOT NULL)</li> <li><code>task_type</code> (TEXT, NOT NULL) - e.g., 'createJournalEntry', 'setReminder'.</li> <li><code>status</code> (TEXT, ENUM: 'pending', 'in-progress', 'completed', 'failed', NOT NULL)</li> <li><code>input_data</code> (TEXT, NULLABLE) - JSON string of input data for the task.</li> <li><code>result_data</code> (TEXT, NULLABLE) - JSON string of output data/result of the task.</li> <li><code>error_message</code> (TEXT, NULLABLE) - Error details if task failed.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>completed_at</code> (TIMESTAMP, NULLABLE)</li> </ul> </li> <li>Feedback Table (NEW):<ul> <li><code>feedback_id</code> (INTEGER, PRIMARY KEY)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to <code>Users.id</code>, NOT NULL)</li> <li><code>message_id</code> (TEXT, FOREIGN KEY to <code>Messages.id</code>, NULLABLE) - For specific message feedback</li> <li><code>feedback_type</code> (TEXT, NOT NULL) - e.g., 'emotion_correction', 'response_quality', 'general'</li> <li><code>is_helpful</code> (INTEGER, BOOLEAN, NULLABLE) - true/false for thumbs up/down</li> <li><code>corrected_emotion_category</code> (TEXT, NULLABLE) - e.g., 'happy' if Saira misidentified</li> <li><code>user_comment</code> (TEXT, NULLABLE) - Detailed text feedback</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li>UsageAnalyticsEvents Table (NEW):<ul> <li><code>event_id</code> (INTEGER, PRIMARY KEY)</li> <li><code>client_id</code> (TEXT, NOT NULL) - Anonymous unique client ID, not linked to <code>user_id</code></li> <li><code>event_name</code> (TEXT, NOT NULL) - e.g., 'feature_accessed', 'session_duration', 'performance_metric'</li> <li><code>event_data</code> (TEXT, NULLABLE) - JSON string for event-specific metrics, e.g., <code>{\"feature\": \"Journaling\"}</code></li> <li><code>event_timestamp</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>app_version</code> (TEXT, NULLABLE)</li> <li><code>os_version</code> (TEXT, NULLABLE)</li> <li><code>device_type</code> (TEXT, NULLABLE) - e.g., 'mac', 'iphone'</li> </ul> </li> </ul> <p>Indexing Strategy:</p> <ul> <li>Indexes on foreign keys (<code>user_id</code>, <code>conversation_id</code>, <code>personality_profile_id</code>, <code>nudge_preference_id</code>).</li> <li>Indexes on <code>timestamp</code> for time-series data (Conversations, Messages, JournalEntries, NudgeHistory, Tasks, Feedback, UsageAnalyticsEvents).</li> <li>Unique index on <code>MoodSummaries.date</code> per <code>user_id</code>.</li> <li><code>sqlite-vec</code> virtual table for <code>Memories.embedding</code> for efficient vector search [[16, 17], S_R147, S_R153].</li> <li>Indexes on <code>name</code> for PersonalityProfiles for quick lookup.</li> <li>Index on <code>Tasks.user_id</code> and <code>Tasks.status</code> for efficient task retrieval.</li> <li>Index on <code>Feedback.user_id</code> and <code>Feedback.message_id</code>.</li> <li>Index on <code>UsageAnalyticsEvents.client_id</code> and <code>UsageAnalyticsEvents.event_name</code>.</li> </ul> <p>Foreign Key Relationships: Enforce ON DELETE CASCADE for Messages to Conversations, Goals, JournalEntries, MoodSummaries, NudgePreferences, NudgeHistory, Tasks, Feedback to Users.</p> <p>Database Migration/Versioning: Use a lightweight schema migration tool (e.g., a custom Node.js script) to manage database schema changes. Each migration will be versioned and applied sequentially on app launch.</p>"},{"location":"feature_implementation/#comprehensive-api-design","title":"Comprehensive API Design","text":"<p>The primary API will be internal, facilitating communication between the React Native frontend and the Node.js backend services. This will likely involve a local HTTP server or WebSocket server running within the Node.js backend process.</p>"},{"location":"feature_implementation/#react-native-frontend-apis-internal-to-sairasrcservices","title":"React Native Frontend APIs (Internal to <code>Saira/src/services/</code>):","text":"<p>These are TypeScript interfaces and classes that define the communication contracts between the React Native UI and the backend services. They will use <code>fetch</code> for HTTP requests and WebSocket for real-time streaming to the local Node.js backend.</p> <ul> <li>UserManager (TypeScript Class):<ul> <li><code>createUser(name: string, initialPreferences: UserPreferencesDTO) =&gt; Promise&lt;User&gt;</code> (C)</li> <li><code>getUser(id: string) =&gt; Promise&lt;User | null&gt;</code> (R)</li> <li><code>updateUser(id: string, updates: UserUpdateDTO) =&gt; Promise&lt;User&gt;</code> (U)</li> <li><code>deleteUser(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> <li><code>getPrivacySettings(userId: string) =&gt; Promise&lt;PrivacySettingsDTO&gt;</code> (R)</li> <li><code>updatePrivacySettings(userId: string, settings: PrivacySettingsDTO) =&gt; Promise&lt;void&gt;</code> (U)</li> </ul> </li> <li>ConversationManager (TypeScript Class):<ul> <li><code>startConversation(userId: string, personalityId: string) =&gt; Promise&lt;Conversation&gt;</code> (C)</li> <li><code>getConversation(id: string) =&gt; Promise&lt;Conversation | null&gt;</code> (R)</li> <li><code>getConversations(userId: string, pagination: PaginationDTO) =&gt; Promise&lt;Conversation[]&gt;</code> (R)</li> <li><code>updateConversationTitle(id: string, title: string) =&gt; Promise&lt;Conversation&gt;</code> (U)</li> <li><code>endConversation(id: string) =&gt; Promise&lt;Conversation&gt;</code> (U)</li> <li><code>deleteConversation(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> </ul> </li> <li>MessageManager (TypeScript Class):<ul> <li><code>addMessage(conversationId: string, sender: SenderType, content: string, audioPath?: string, emotions?: EmotionDataDTO) =&gt; Promise&lt;Message&gt;</code> (C)</li> <li><code>getMessages(conversationId: string, pagination: PaginationDTO) =&gt; Promise&lt;Message[]&gt;</code> (R)</li> <li><code>updateMessage(id: string, content: string) =&gt; Promise&lt;Message&gt;</code> (U)</li> <li><code>deleteMessage(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> </ul> </li> <li>JournalManager (TypeScript Class) - Note: Direct calls mostly replaced by TaskManagerClient for background processing:<ul> <li><code>getJournalEntry(id: string) =&gt; Promise&lt;JournalEntry | null&gt;</code> (R)</li> <li><code>getJournalEntries(userId: string, dateRange: DateRangeDTO, pagination: PaginationDTO) =&gt; Promise&lt;JournalEntry[]&gt;</code> (R)</li> <li><code>updateJournalEntry(id: string, content: string) =&gt; Promise&lt;JournalEntry&gt;</code> (U)</li> <li><code>deleteJournalEntry(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> <li><code>generateMoodSummary(userId: string, date: Date) =&gt; Promise&lt;MoodSummary&gt;</code> (C)</li> <li><code>getMoodSummaries(userId: string, dateRange: DateRangeDTO) =&gt; Promise&lt;MoodSummary[]&gt;</code> (R)</li> </ul> </li> <li>PersonalityManager (TypeScript Class):<ul> <li><code>getPersonalityProfiles(userId?: string) =&gt; Promise&lt;PersonalityProfile[]&gt;</code> (R)</li> <li><code>createCustomPersonality(userId: string, profile: PersonalityProfileDTO) =&gt; Promise&lt;PersonalityProfile&gt;</code> (C)</li> <li><code>updatePersonalityProfile(id: string, updates: PersonalityProfileUpdateDTO) =&gt; Promise&lt;PersonalityProfile&gt;</code> (U)</li> <li><code>deleteCustomPersonality(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> <li><code>activatePersonalityProfile(userId: string, profileId: string) =&gt; Promise&lt;void&gt;</code> (U)</li> </ul> </li> <li>NudgeManager (TypeScript Class) - Note: Direct calls for setting nudges might be replaced by TaskManagerClient:<ul> <li><code>getNudgePreferences(userId: string) =&gt; Promise&lt;NudgePreference[]&gt;</code> (R)</li> <li><code>updateNudgePreference(id: string, updates: NudgePreferenceUpdateDTO) =&gt; Promise&lt;NudgePreference&gt;</code> (U)</li> <li><code>deleteNudgePreference(id: string) =&gt; Promise&lt;void&gt;</code> (D)</li> <li><code>getNudgeHistory(userId: string, pagination: PaginationDTO) =&gt; Promise&lt;NudgeHistory[]&gt;</code> (R)</li> <li><code>markNudgeInteracted(nudgeHistoryId: string) =&gt; Promise&lt;void&gt;</code> (U)</li> </ul> </li> <li>TaskManagerClient (NEW - TypeScript Class in <code>Saira/src/services/</code>):<ul> <li><code>createTask(userId: string, taskType: TaskType, taskData: any): Promise&lt;Task&gt;</code><ul> <li>Purpose: Initiates a background task (e.g., <code>createJournalEntry</code>, <code>setReminder</code>). Returns a <code>Task</code> object immediately with <code>pending</code> status.</li> </ul> </li> <li><code>getTaskStatus(taskId: string): Promise&lt;TaskStatus&gt;</code><ul> <li>Purpose: Polls for the current status of a given task.</li> </ul> </li> <li><code>onTaskUpdate(callback: (update: TaskUpdateEvent) =&gt; void): () =&gt; void</code><ul> <li>Purpose: Subscribes to WebSocket updates for task status changes (<code>/ws/task_updates</code>). Returns an unsubscribe function.</li> </ul> </li> <li><code>getTaskResult(taskId: string): Promise&lt;any&gt;</code><ul> <li>Purpose: Optional API to fetch the final result of a completed task if not fully provided via WebSocket.</li> </ul> </li> </ul> </li> </ul>"},{"location":"feature_implementation/#nodejs-backend-apis-local-httpwebsocket-server-in-sairabackendservicessrcnode_servicesai_inference_serverts-and-task_managerts","title":"Node.js Backend APIs (Local HTTP/WebSocket Server in <code>SairaBackendServices/src/node_services/ai_inference_server.ts</code> and <code>task_manager.ts</code>):","text":"<ul> <li> <p>HTTP Endpoints (REST-like): These endpoints will be exposed via a local HTTP server (e.g., Express.js) running within the Node.js backend process.</p> <ul> <li><code>POST /ai/llm/generate</code>: Request LLM response.<ul> <li>Request: <code>{ prompt: string, context: string, memories: string, personalityId: string, emotionData: EmotionDataDTO }</code></li> <li>Response: <code>{ text: string }</code></li> </ul> </li> <li><code>POST /ai/asr/transcribe_file</code>: Transcribe a full audio file.<ul> <li>Request: <code>{ filePath: string }</code></li> <li>Response: <code>{ text: string }</code></li> </ul> </li> <li><code>POST /ai/tts/synthesize_text</code>: Synthesize speech from text.<ul> <li>Request: <code>{ text: string }</code></li> <li>Response: <code>{ audioData: string (base64 encoded PCM) }</code></li> </ul> </li> <li><code>POST /ai/ser/detect_file</code>: Detect emotion from an audio file.<ul> <li>Request: <code>{ filePath: string }</code></li> <li>Response: <code>{ arousal: number, valence: number, category: string }</code></li> </ul> </li> <li><code>GET /ai/ser/mood_summary</code> (NEW): Provides an aggregated emotional mood summary over a configurable time window (e.g., last 5 minutes) based on recent <code>Messages</code> entries.<ul> <li>Endpoint:** <code>GET /ai/ser/mood_summary?minutes=5</code> (default 5 minutes)</li> <li>Parameters:** <code>minutes</code> (optional, integer): Duration in minutes for aggregation.</li> <li>Response:**     <code>json     {         \"overall_mood\": \"sad\", // Dominant emotion category over the period         \"mood_intensity\": 0.7, // Average arousal/valence intensity         \"mood_breakdown\": {             \"happy\": 0.2,             \"sad\": 0.5,             \"neutral\": 0.2,             \"angry\": 0.1         }, // Distribution of emotion categories         \"start_timestamp\": \"ISO_DATE_STRING\",         \"end_timestamp\": \"ISO_DATE_STRING\"     }</code></li> <li>Backend Logic: Query <code>Messages Table</code>, filter by timestamp, aggregate <code>emotion_category</code>, <code>emotion_arousal</code>, <code>emotion_valence</code> to derive <code>overall_mood</code> and <code>mood_intensity</code>.</li> </ul> </li> <li><code>POST /ai/embeddings/generate</code>: Generate text embedding.<ul> <li>Request: <code>{ text: string }</code></li> <li>Response: <code>{ embedding: number[] }</code></li> </ul> </li> <li><code>POST /data/query</code>: Execute database query.<ul> <li>Request: <code>{ sql: string, params: any[] }</code></li> <li>Response: <code>{ rows: any[] }</code></li> </ul> </li> <li><code>POST /data/mutate</code>: Execute database mutation.<ul> <li>Request: <code>{ sql: string, params: any[] }</code></li> <li>Response: <code>{ changes: number, lastInsertRowId: number }</code></li> </ul> </li> <li><code>POST /data/search_vectors</code>: Perform vector search.<ul> <li>Request: <code>{ embedding: number[], tableName: string, limit: number }</code></li> <li>Response: <code>{ results: { id: string, score: number }[] }</code></li> </ul> </li> <li><code>POST /models/load</code>: Load AI model.<ul> <li>Request: <code>{ modelType: string, modelPath: string }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>POST /sync/initiate</code>: Initiate cloud sync.<ul> <li>Request: <code>{ userId: string, lastSyncTimestamp: string }</code></li> <li>Response: <code>{ status: string }</code></li> </ul> </li> <li><code>POST /sync/upload</code>: Upload data for sync.<ul> <li>Request: <code>{ userId: string, data: any, type: string }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>POST /sync/download</code>: Download data for sync.<ul> <li>Request: <code>{ userId: string, type: string, lastSyncTimestamp: string }</code></li> <li>Response: <code>{ data: any }</code></li> </ul> </li> <li><code>POST /sync/resolve_conflict</code>: Resolve sync conflict.<ul> <li>Request: <code>{ conflictId: string, resolution: string }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>POST /rag/ingest_document</code>: Initiates document ingestion.<ul> <li>Request: <code>{ userId: string, filePath: string, autoIngest: boolean }</code></li> <li>Response: <code>{ success: boolean, documentId: string }</code></li> </ul> </li> <li><code>POST /tasks/create</code> (NEW - Handled by <code>task_manager.ts</code>):<ul> <li>Request: <code>{ userId: string, taskType: TaskType, taskData: any }</code></li> <li>Response: <code>{ taskId: string, status: 'pending' }</code> (Immediate response)</li> </ul> </li> <li><code>GET /tasks/:taskId/status</code> (NEW - Handled by <code>task_manager.ts</code>):<ul> <li>Request: <code>None</code></li> <li>Response: <code>{ status: TaskStatus, result?: any, error?: string }</code></li> </ul> </li> <li><code>GET /tasks/:taskId/result</code> (NEW - Optional, Handled by <code>task_manager.ts</code>):<ul> <li>Request: <code>None</code></li> <li>Response: <code>{ result: any }</code></li> </ul> </li> <li><code>POST /feedback/submit</code> (NEW): Allows users to submit feedback on Saira's performance, particularly for emotional accuracy or response quality.<ul> <li>Endpoint: <code>POST /feedback/submit</code></li> <li>Request Body:     <code>json     {         \"message_id\": \"msg_123\", // Optional: The ID of the message being reviewed         \"feedback_type\": \"emotion_correction\",         \"is_helpful\": false, // Optional: true/false for thumbs up/down         \"corrected_emotion_category\": \"sad\", // Optional: User's correction for emotion         \"user_comment\": \"Saira thought I was angry, but I was just frustrated.\" // Optional: User's text comment     }</code></li> <li>Response: <code>200 OK</code> or <code>400 Bad Request</code> with error details.</li> <li>Backend Logic: Validate input, insert data into <code>Feedback Table</code>.</li> </ul> </li> <li><code>POST /analytics/log_event</code> (NEW):Receives anonymized usage events from the client, only if the user has opted in.<ul> <li>Endpoint: <code>POST /analytics/log_event</code></li> <li>Request Body:     <code>json     {         \"event_name\": \"DeepTalk_Session_Ended\",         \"event_data\": {             \"duration_seconds\": 360,             \"message_count\": 25,             \"emotion_shift_score\": 0.7         },         \"client_id\": \"anon_client_12345\", // Anonymous, non-PII ID         \"app_version\": \"1.0.0\",         \"os_version\": \"macOS_14.5\",         \"device_type\": \"mac\"     }</code></li> <li>Response: <code>200 OK</code> (no content).</li> <li>Backend Logic: Check <code>analytics_opt_in</code> status for the <code>client_id</code> (if identifiable, otherwise rely on client-side opt-in check). Sanitize <code>event_data</code> to ensure no PII is included. Insert into <code>UsageAnalyticsEvents Table</code>. This endpoint must be distinct and isolated from the main data sync and user data pathways.</li> </ul> </li> </ul> </li> <li> <p>WebSocket Endpoints (for real-time streaming on <code>ai_inference_server.ts</code> and <code>task_manager.ts</code>):</p> <ul> <li><code>/ws/audio_stream</code>: Bidirectional audio streaming for ASR, SER, and TTS.<ul> <li>Client sends raw audio chunks (e.g., PCM ArrayBuffer).</li> <li>Server sends back real-time transcription updates, emotion data, and TTS audio chunks.</li> <li>Example extended payload from Server to Client:     <code>json     {         \"type\": \"audio_analysis\",         \"timestamp\": \"ISO_DATE_STRING\",         \"transcript_partial\": \"...\",         \"emotion_arousal\": 0.8,         \"emotion_valence\": -0.3,         \"emotion_category\": \"angry\"     }</code></li> </ul> </li> <li><code>/ws/wake_word</code>: For continuous wake word detection.<ul> <li>Client streams raw audio.</li> <li>Server sends <code>wakeWordDetected</code> event.</li> </ul> </li> <li><code>/ws/task_updates (NEW - Handled by</code>task_manager.ts`):<ul> <li>Purpose:** Provides real-time updates on task status.</li> <li>Server sends:<ul> <li><code>{ taskId: string, status: 'created', timestamp: number }</code></li> <li><code>{ taskId: string, status: 'in-progress', timestamp: number, progress?: number }</code></li> <li><code>{ taskId: string, status: 'completed', timestamp: number, result: any }</code></li> <li><code>{ taskId: string, status: 'failed', timestamp: number, error: string }</code></li> </ul> </li> </ul> </li> <li><code>/ws/device_context (NEW):</code><ul> <li>Description: A persistent WebSocket connection for real-time exchange of critical, transient device context data (e.g., current active conversation ID, user's real-time emotional state, active personality mode, \"typing\" status).</li> <li>Client (Device A) to Server Messages: <code>json     // When user starts interacting     { \"type\": \"status_update\", \"status\": \"active\", \"conversation_id\": \"conv_abc\", \"device_type\": \"mac\" }     // When emotion changes significantly     { \"type\": \"emotion_update\", \"emotion_category\": \"happy\", \"arousal\": 0.7, \"valence\": 0.6 }     // When user is typing/speaking     { \"type\": \"typing_status\", \"is_typing\": true, \"conversation_id\": \"conv_xyz\" }</code></li> <li>Server to Client (Device B) Messages:<ul> <li>Server broadcasts relevant updates to other active devices of the same user.</li> <li>Example message:     <code>json     {         \"type\": \"context_sync\",         \"source_device_id\": \"device_abc\",         \"data\": {             \"active_conversation_id\": \"conv_abc\",             \"current_mood\": \"happy\",             \"active_personality_id\": \"mom_personality\",             \"last_spoken_text_snippet\": \"Hey Saira, how are you?\"         }     }</code></li> </ul> </li> <li>Backend Logic: Maintain a map of active user connections and their device IDs. On receiving a context update from one device, update the user's ephemeral state and broadcast to other connected devices of that user.</li> </ul> </li> </ul> </li> </ul>"},{"location":"feature_implementation/#nodejs-backend-internal-apis-using-c-native-addons-and-python-child_process","title":"Node.js Backend (Internal APIs using C++ Native Addons and Python <code>child_process</code>):","text":"<ul> <li>audio_io_addon (N-API): Wraps PortAudio or direct Core Audio calls for low-latency audio input/output.<ul> <li><code>startCapture(sampleRate: number, channels: number, callback: (buffer: ArrayBuffer) =&gt; void)</code></li> <li><code>stopCapture()</code></li> <li><code>playAudio(buffer: ArrayBuffer, sampleRate: number, channels: number)</code></li> <li><code>getDevices(): { input: AudioDevice, output: AudioDevice }</code></li> </ul> </li> <li>ai_inference_addon (N-API): Wraps C++ AI libraries.<ul> <li><code>loadModel(type: 'llm' | 'asr' | 'tts' | 'ser' | 'wake_word' | 'embedding', modelPath: string, config?: object)</code></li> <li><code>llmGenerate(prompt: string, context: string, memories: string, personalityPrompt: string, emotionData: EmotionDataDTO): string</code></li> <li><code>asrTranscribe(audioBuffer: ArrayBuffer): string</code></li> <li><code>ttsSynthesize(text: string): ArrayBuffer</code></li> <li><code>serDetect(audioBuffer: ArrayBuffer): EmotionDataDTO</code></li> <li><code>wakeWordProcess(audioBuffer: ArrayBuffer): boolean</code> (returns true if detected)</li> <li><code>embeddingGenerate(text: string): ArrayBuffer</code></li> </ul> </li> <li>sqlite_vec_addon (N-API): Wraps <code>sqlite-vec</code> for vector operations.<ul> <li><code>openDatabase(path: string, encryptionKey: string)</code></li> <li><code>closeDatabase()</code></li> <li><code>executeSQL(query: string, params: any[]): any</code></li> <li><code>searchVectors(embedding: ArrayBuffer, tableName: string, limit: number): { id: string, score: number }[]</code></li> <li><code>insertVector(tableName: string, id: string, embedding: ArrayBuffer): boolean</code></li> <li><code>updateVector(tableName: string, id: string, embedding: ArrayBuffer): boolean</code></li> <li><code>deleteVector(tableName: string, id: string): boolean</code></li> <li><code>runMigrations()</code></li> </ul> </li> <li>Python Scripts (spawned via Node.js <code>child_process</code>):<ul> <li><code>rag_ingestion.py</code>: For document parsing, chunking, and potentially calling embedding models if Python bindings are preferred.</li> <li><code>model_downloader.py</code>: For managing AI model downloads.</li> <li><code>ser_feature_extractor.py</code>: If openSMILE Python bindings are used for feature extraction before a custom Python-based SER classifier.</li> </ul> </li> </ul> <p>Authentication &amp; Authorization:</p> <ul> <li>Local User Authentication: User authentication is local, based on a user-derived key (e.g., passphrase or biometric unlock) used to decrypt the SQLite database (SQLCipher). No external authentication provider for MVP.</li> <li>Authorization: All data is user-specific. Access control is implicit based on the currently authenticated local user. Fine-grained permissions for memory visibility and data recording are managed via user settings in the Users table. Communication between frontend and backend is assumed to be secure due to local IPC (e.g., localhost connections, <code>child_process</code> pipes).</li> </ul> <p>Error Handling: Standard JavaScript Error objects for Node.js services. HTTP endpoints will use appropriate status codes (e.g., 400 for bad requests, 500 for internal errors). WebSocket errors will be communicated via specific error messages. C++ native addon errors will be propagated as JavaScript errors. The <code>TaskManager</code> will also record <code>error_message</code> in the <code>Tasks</code> table for failed tasks and communicate them via WebSocket.</p> <ol> <li> <p>Frontend Architecture (React Native for macOS with TypeScript)</p> <ul> <li>Component Hierarchy:<ul> <li><code>App.tsx</code> (Root Component)<ul> <li><code>OnboardingScreen.tsx</code> (Conditional on <code>onboarding_completed</code> flag)</li> <li><code>AppNavigator.tsx</code> (Handles main tab navigation)<ul> <li><code>ConversationScreen.tsx</code> (Chat interface)<ul> <li><code>MessageBubble.tsx</code></li> <li><code>VoiceInputIndicator.tsx</code></li> </ul> </li> <li><code>JournalingScreen.tsx</code> (Daily Check-ins, Journal Entries)<ul> <li><code>MoodSummaryCard.tsx</code></li> <li><code>JournalEntryList.tsx</code></li> <li><code>JournalEntryDetailView.tsx</code></li> </ul> </li> <li><code>DeepTalkScreen.tsx</code> (Specialized conversation mode)</li> <li><code>ProfileSettingsScreen.tsx</code> (User settings, privacy, personality)<ul> <li><code>PersonalitySelectionView.tsx</code></li> <li><code>PrivacyDashboardView.tsx</code></li> <li><code>ModelManagementView.tsx</code></li> </ul> </li> <li><code>SharedComponents/</code> (e.g., <code>CustomButton.tsx</code>, <code>LoadingIndicator.tsx</code>, <code>AlertView.tsx</code>)</li> </ul> </li> </ul> </li> </ul> </li> <li>Reusable Component Library: Develop a consistent set of React Native components (Views, Text, Buttons, etc.) to ensure a cohesive UI/UX. Use TypeScript for strict type checking and better developer experience.</li> <li>State Management:<ul> <li><code>useState</code> and <code>useEffect</code> hooks for local component state.</li> <li><code>useContext</code> or a library like <code>Zustand</code>/<code>Jotai</code> for global application state (e.g., user profile, active conversation, AI service status) to avoid prop drilling.</li> <li><code>React Query</code> or <code>SWR</code> for data fetching and caching from the local Node.js backend.</li> </ul> </li> <li>Routing and Navigation:<ul> <li><code>React Navigation</code> library for tab-based navigation and stack navigation within tabs.[3]</li> <li>Programmatic navigation for onboarding flow and specific actions (e.g., opening Deep Talk from a nudge).</li> </ul> </li> <li>Responsive Design: Utilize React Native's <code>Flexbox</code> for layout and <code>Dimensions</code> API for adapting to screen size changes. Consider <code>Platform.OS === 'macos'</code> for macOS-specific UI adjustments.</li> </ul> </li> <li> <p>Detailed CRUD Operations</p> <ul> <li>User (CRUD via <code>UserManager</code> in Frontend, interacts with <code>data_manager.ts</code> in Backend)<ul> <li>Create: <code>createUser(name: string, initialPreferences: UserPreferencesDTO)</code><ul> <li>Validation: <code>name</code> not empty, <code>initialPreferences</code> valid.</li> <li>Required Fields: <code>name</code>, <code>gdpr_ccpa_consent</code>, <code>emotion_recording_consent</code>, <code>memory_recording_consent</code>.</li> <li>Flow: Onboarding UI collects data -&gt; <code>UserManager.createUser</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> receives request -&gt; <code>data_manager.ts</code> uses <code>sqlite_vec_addon</code> to insert into <code>Users</code> table.</li> </ul> </li> <li>Read: <code>getUser(id: string)</code>, <code>getPrivacySettings(userId: string)</code><ul> <li>Filtering: By <code>id</code>.</li> <li>Pagination/Sorting: Not applicable for single user retrieval.</li> <li>Flow: Profile Settings UI requests user data -&gt; <code>UserManager.getUser</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> queries <code>Users</code> table.</li> </ul> </li> <li>Update: <code>updateUser(id: string, updates: UserUpdateDTO)</code>, <code>updatePrivacySettings(userId: string, settings: PrivacySettingsDTO)</code><ul> <li>Partial Updates: <code>UserUpdateDTO</code> allows updating specific fields (e.g., <code>preferred_tone_style</code>, <code>sync_enabled</code>).</li> <li>Validation: Ensure updates are valid (e.g., <code>sync_frequency</code> is a valid enum value).</li> <li>Flow: Profile Settings UI modifies data -&gt; <code>UserManager.updateUser</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> updates <code>Users</code> table.</li> </ul> </li> <li>Delete: <code>deleteUser(id: string)</code><ul> <li>Soft Delete: Not applicable for primary user.</li> <li>Hard Delete: Full deletion of user and all associated data (conversations, memories, journals, etc.) from the local database.</li> <li>Flow: Privacy Dashboard UI initiates deletion (with confirmation) -&gt; <code>UserManager.deleteUser</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>Users</code> table and cascades deletes.</li> </ul> </li> </ul> </li> <li>Conversation (CRUD via <code>ConversationManager</code> in Frontend, interacts with <code>data_manager.ts</code> in Backend)<ul> <li>Create: <code>startConversation(userId: string, personalityId: string)</code><ul> <li>Validation: <code>userId</code> and <code>personalityId</code> must exist.</li> <li>Required Fields: <code>user_id</code>, <code>start_time</code>, <code>personality_profile_id</code>.</li> <li>Flow: User initiates new conversation -&gt; <code>ConversationManager.startConversation</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> inserts into <code>Conversations</code>.</li> </ul> </li> <li>Read: <code>getConversation(id: string)</code>, <code>getConversations(userId: string, pagination: PaginationDTO)</code><ul> <li>Filtering: By <code>id</code> or <code>user_id</code>.</li> <li>Pagination: <code>offset</code>, <code>limit</code> for conversation list.</li> <li>Sorting: By <code>start_time</code> (descending).</li> <li>Flow: Conversation History UI loads list -&gt; <code>ConversationManager.getConversations</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> queries <code>Conversations</code>.</li> </ul> </li> <li>Update: <code>updateConversationTitle(id: string, title: string)</code>, <code>endConversation(id: string)</code><ul> <li>Partial Updates: Update title, set <code>end_time</code>.</li> <li>Validation: <code>title</code> not empty.</li> <li>Flow: User edits title or ends conversation -&gt; <code>ConversationManager.updateConversationTitle</code>/<code>endConversation</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> updates <code>Conversations</code>.</li> </ul> </li> <li>Delete: <code>deleteConversation(id: string)</code><ul> <li>Soft Delete: Not applicable.</li> <li>Hard Delete: Deletes conversation and cascades to <code>Messages</code> associated.</li> <li>Flow: User deletes conversation from history -&gt; <code>ConversationManager.deleteConversation</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>Conversations</code>.</li> </ul> </li> </ul> </li> <li> <p>Message (CRUD via <code>MessageManager</code> in Frontend, interacts with <code>data_manager.ts</code> in Backend)</p> <ul> <li>Create: <code>addMessage(conversationId: string, sender: SenderType, content: string, audioPath?: string, emotions?: EmotionDataDTO)</code><ul> <li>Validation: <code>conversationId</code> exists, <code>content</code> not empty.</li> <li>Required Fields: <code>conversation_id</code>, <code>sender_type</code>, <code>content</code>, <code>timestamp</code>.</li> <li>Flow: User speaks/types or AI responds -&gt; <code>MessageManager.addMessage</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> inserts into <code>Messages</code>.</li> </ul> </li> <li>Read: <code>getMessages(conversationId: string, pagination: PaginationDTO)</code><ul> <li>Filtering: By <code>conversationId</code>.</li> <li>Pagination: <code>offset</code>, <code>limit</code> for message history.</li> <li>Sorting: By <code>timestamp</code> (ascending).</li> <li>Flow: Conversation Screen loads messages -&gt; <code>MessageManager.getMessages</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> queries <code>Messages</code>.</li> </ul> </li> <li>Update: <code>updateMessage(id: string, content: string)</code><ul> <li>Partial Updates: Only <code>content</code> can be updated.</li> <li>Validation: <code>content</code> not empty.</li> <li>Flow: User edits their message (e.g., typo correction) -&gt; <code>MessageManager.updateMessage</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> updates <code>Messages</code>.</li> </ul> </li> <li>Delete: <code>deleteMessage(id: string)</code><ul> <li>Soft Delete: Not applicable.</li> <li>Hard Delete: Deletes specific message.</li> <li>Flow: User deletes a specific message... from history -&gt; <code>MessageManager.deleteMessage</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>Messages</code>.</li> </ul> </li> </ul> </li> <li> <p>Memory (CRUD via <code>MemoryManager</code> in Frontend, interacts with <code>data_manager.ts</code> and <code>ai_inference_server.ts</code> in Backend)</p> <ul> <li>Create: <code>createMemory(userId: string, type: MemoryType, content: string, sourceId?: string, sourceType?: SourceType)</code><ul> <li>Validation: <code>userId</code> exists, <code>content</code> not empty.</li> <li>Required Fields: <code>user_id</code>, <code>type</code>, <code>content</code>, <code>embedding</code>, <code>timestamp</code>.</li> <li>Flow: <code>ConversationManager</code> or <code>JournalManager</code> identifies new memory -&gt; <code>MemoryManager.createMemory</code> calls <code>IPCService</code> to <code>ai_inference_server.ts</code> for embedding generation -&gt; <code>ai_inference_server.ts</code> calls <code>data_manager.ts</code> to insert into <code>Memories</code> and <code>sqlite-vec</code> virtual table.</li> </ul> </li> <li>Read: <code>getMemory(id: string)</code>, <code>getMemories(userId: string, type?: MemoryType, isActive?: boolean, pagination?: PaginationDTO)</code>, <code>searchMemories(userId: string, query: string, limit: number)</code><ul> <li>Filtering: By <code>id</code>, <code>user_id</code>, <code>type</code>, <code>is_active</code>.</li> <li>Pagination: <code>offset</code>, <code>limit</code>.</li> <li>Sorting: By <code>timestamp</code> (descending) or <code>relevance_score</code> (for search).</li> <li>Flow: Deep Talk mode or Memory Management UI requests memories -&gt; <code>MemoryManager.getMemories</code>/<code>searchMemories</code> calls <code>IPCService</code> to <code>ai_inference_server.ts</code> for query embedding -&gt; <code>ai_inference_server.ts</code> calls <code>data_manager.ts</code> to search <code>sqlite-vec</code> and retrieve <code>Memories</code>.</li> </ul> </li> <li>Update: <code>updateMemory(id: string, content?: string, isActive?: boolean)</code>, <code>consolidateMemories(userId: string, memoryIds: string, newContent: string)</code><ul> <li>Partial Updates: Update <code>content</code>, <code>is_active</code>.</li> <li>Consolidation: Combine multiple memories into one, updating content and potentially re-embedding.</li> <li>Validation: <code>content</code> not empty.</li> <li>Flow: User edits memory or AI consolidates memories -&gt; <code>MemoryManager.updateMemory</code>/<code>consolidateMemories</code> calls <code>IPCService</code> to <code>ai_inference_server.ts</code> (if content changed for re-embedding) -&gt; <code>ai_inference_server.ts</code> calls <code>data_manager.ts</code> to update <code>Memories</code>.</li> </ul> </li> <li>Delete: <code>deleteMemory(id: string)</code><ul> <li>Soft Delete: Not applicable.</li> <li>Hard Delete: Deletes specific memory and its vector from <code>sqlite-vec</code>.</li> <li>Flow: User deletes memory from profile -&gt; <code>MemoryManager.deleteMemory</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>Memories</code> and <code>sqlite-vec</code>.</li> </ul> </li> </ul> </li> <li> <p>JournalEntry (CRUD via <code>JournalManager</code> in Frontend, interacts with <code>data_manager.ts</code> in Backend)</p> <ul> <li>Create: <code>createJournalEntry(userId: string, content: string, emotions?: EmotionDataDTO)</code><ul> <li>Validation: <code>userId</code> exists, <code>content</code> not empty.</li> <li>Required Fields: <code>user_id</code>, <code>timestamp</code>, <code>content</code>.</li> <li>Flow: User creates new journal entry -&gt; <code>JournalManager.createJournalEntry</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> inserts into <code>JournalEntries</code>.</li> </ul> </li> <li>Read: <code>getJournalEntry(id: string)</code>, <code>getJournalEntries(userId: string, dateRange: DateRangeDTO, pagination: PaginationDTO)</code><ul> <li>Filtering: By <code>id</code>, <code>user_id</code>, <code>dateRange</code>.</li> <li>Pagination: <code>offset</code>, <code>limit</code>.</li> <li>Sorting: By <code>timestamp</code> (descending).</li> <li>Flow: Journaling UI loads entries -&gt; <code>JournalManager.getJournalEntries</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> queries <code>JournalEntries</code>.</li> </ul> </li> <li>Update: <code>updateJournalEntry(id: string, content: string)</code><ul> <li>Partial Updates: Only <code>content</code>.</li> <li>Validation: <code>content</code> not empty.</li> <li>Flow: User edits journal entry -&gt; <code>JournalManager.updateJournalEntry</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> updates <code>JournalEntries</code>.</li> </ul> </li> <li>Delete: <code>deleteJournalEntry(id: string)</code><ul> <li>Soft Delete: Not applicable.</li> <li>Hard Delete: Deletes specific journal entry.</li> <li>Flow: User deletes journal entry -&gt; <code>JournalManager.deleteJournalEntry</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>JournalEntries</code>.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>User Experience Flow</p> <ul> <li>Onboarding Flow:<ol> <li>App Launch (First Time): <code>Saira</code> detects no existing user profile.</li> <li>Welcome Screen: Displays Saira's purpose and privacy commitment.</li> <li>Consent Screen: Presents GDPR/CCPA, emotion recording, and memory recording consents. User must explicitly accept.</li> <li>Personality Selection: User chooses an initial personality mode (e.g., \"Best Friend\", \"Coach\") from pre-defined options.</li> <li>Goal Setting: User defines initial personal goals (e.g., \"Improve sleep\", \"Reduce stress\").</li> <li>Tone Style Preference: User selects preferred AI response tone (e.g., empathetic, direct).</li> <li>Onboarding Complete: Confirmation screen, transition to <code>MainTabView</code>.</li> </ol> </li> <li>Main Interaction Loop (Voice-first):<ol> <li>Idle State: App is running in background, <code>WakeWordDetector</code> (Node.js backend service using C++ addon) is listening for \"Hey Saira\".</li> <li>Wake Word Detected: <code>WakeWordDetector</code> notifies <code>ai_inference_server.ts</code> (via internal IPC), which sends a WebSocket message to <code>IPCService</code> in the frontend.</li> <li>Listening State: UI displays active listening indicator (e.g., pulsing microphone icon). <code>IPCService</code> instructs <code>audio_processor.ts</code> (Node.js backend using C++ addon) to start capturing audio.</li> <li>Speech Input: User speaks. Audio stream is sent via WebSocket to <code>ai_inference_server.ts</code>. <code>ai_inference_server.ts</code> streams audio to ASR (Whisper.cpp via C++ addon) and SER (openSMILE/SenseVoice via C++ addon).</li> <li>Real-time Transcription: Transcribed text and detected emotions (arousal/valence) are streamed back to the frontend via WebSocket and displayed in <code>ConversationScreen</code>.</li> <li>User Stops Speaking: Silence detection or explicit \"stop listening\" command.</li> <li>Processing State: UI indicates AI is thinking. Full transcribed text and aggregated emotion data are sent to <code>ConversationScreen</code>'s ViewModel.</li> <li>LLM Inference: <code>ConversationScreen</code>'s ViewModel sends prompt (with current conversation context and relevant memories from RAG) to <code>ai_inference_server.ts</code> (LLM via C++ addon).</li> <li>Response Generation: LLM generates text response, adapting based on detected emotion and active personality profile.</li> <li>TTS Synthesis &amp; Playback: LLM response text sent to <code>ai_inference_server.ts</code> (Piper via C++ addon) for synthesis -&gt; audio data streamed back to frontend via WebSocket -&gt; <code>audio_processor.ts</code> plays audio via C++ addon.</li> <li>AI Response Display: AI's spoken response is played, and its text appears in <code>ConversationScreen</code>.</li> <li>Memory Formation: <code>ConversationManager</code> processes conversation turn to extract and store new memories (semantic, episodic) via <code>MemoryManager</code>.</li> <li>Idle State: Return to listening for wake word.</li> </ol> </li> <li>Daily Check-ins &amp; Journaling Flow:<ol> <li>Navigate to Tab: User taps \"Journal\" tab.</li> <li>Mood Check-in: User taps \"How are you feeling?\" button.</li> <li>Mood Picker: Presents a visual mood picker (e.g., a grid of emojis or a slider for arousal/valence).</li> <li>Journal Entry: User types or dictates a journal entry.</li> <li>Save: Entry is saved to <code>JournalEntries</code> table via <code>JournalManager</code>.</li> <li>Mood Summary: Daily mood summary is generated/updated based on entries and conversations.</li> </ol> </li> <li>Offline Sync Flow:<ol> <li>Settings UI: User navigates to Profile Settings -&gt; Sync.</li> <li>Enable Sync: User toggles \"Enable Cloud Sync\".</li> <li>Authentication/Setup: If first time, prompts for cloud service credentials (e.g., EteSync account).</li> <li>Initial Sync: <code>SyncService.initiateSync</code> is called. UI shows progress.</li> <li>Conflict Resolution: If conflicts arise during sync, UI presents options (e.g., \"Keep Local\", \"Keep Cloud\", \"Review Manually\").</li> <li>Background Sync: Once enabled, <code>SyncService</code> schedules background syncs based on user-defined frequency. UI shows subtle indicator when syncing.</li> </ol> </li> </ul> </li> <li> <p>Security Considerations</p> <ul> <li>Authentication Flow:<ul> <li>Local-only: User provides a passphrase or uses biometric authentication (Touch ID/Face ID) on app launch.</li> <li>Key Derivation: This passphrase/biometric data is used to derive a key for SQLCipher to decrypt the local SQLite database. The key is never stored directly.</li> <li>Re-authentication: Re-authentication may be required after a period of inactivity or on sensitive actions (e.g., accessing privacy settings, data export).</li> </ul> </li> <li>Authorization Matrix (Local User):<ul> <li>User: Has full read/write access to all their own data.</li> <li>AI: Has read access to <code>Memories</code> (based on <code>memory_visibility_rules</code>), <code>Conversations</code>, <code>Messages</code>, <code>Goals</code>, <code>JournalEntries</code> for contextual understanding and RAG. Write access to <code>Memories</code>, <code>MoodSummaries</code>, <code>NudgeHistory</code>.</li> </ul> </li> <li>Data Validation and Sanitization:<ul> <li>Input Validation: All user inputs (text fields, settings) will be validated at the UI (React Native) and application logic (Node.js backend) layers to prevent invalid data from reaching the database.</li> <li>Sanitization: Text inputs will be sanitized to prevent injection attacks (e.g., SQL injection, although less critical for local SQLite, it's good practice).</li> </ul> </li> <li>Protection against Common Vulnerabilities:<ul> <li>Data at Rest Encryption: SQLite database encrypted using SQLCipher with a user-derived key. AI model files and embeddings stored in sandboxed app directories with file-level encryption.</li> <li>IPC Security: Communication between React Native and Node.js backend will be over <code>localhost</code> (e.g., HTTP/WebSocket). While this is generally secure for local processes, ensure proper validation of incoming requests to the Node.js server. If <code>child_process</code> is used, ensure secure communication channels (e.g., pipes). For macOS-specific background services, consider using XPC services (implemented in Objective-C/Swift/C++) and communicating with them from Node.js via a native addon, as this is the Apple-recommended secure IPC mechanism.</li> <li>App Sandboxing: The macOS application will be sandboxed, limiting its access to system resources and user data to only what is explicitly permitted.</li> <li>System Integrity Protection (SIP): Rely on macOS SIP to protect system files and processes.</li> <li>App Transport Security (ATS): Enforce ATS for all network connections (for optional cloud sync and model updates) to ensure secure communication (HTTPS).</li> <li>Cryptographic Signatures: AI model files downloaded via in-app updates will be verified with cryptographic signatures and checksums to ensure integrity and authenticity.</li> <li>GDPR/CCPA Compliance:<ul> <li>Consent Management: Explicit, granular consent for data processing (emotion, memory recording) during onboarding and in settings.</li> <li>Right to Access/Portability: Provide a mechanism for users to export their data in a common, machine-readable format (e.g., JSON, CSV).</li> <li>Right to Erasure (Right to Be Forgotten): Implement a clear process for users to request and confirm deletion of all their data from the device and optionally from cloud sync.</li> <li>Audit Logs: Maintain local, encrypted audit logs of sensitive actions (e.g., data export, privacy setting changes, consent updates) for user review.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Testing Strategy</p> <ul> <li>Unit Test Requirements:<ul> <li>React Native (<code>Saira/__tests__</code>): Jest for all React/TypeScript components, hooks, and services. Mock backend IPC calls.</li> <li>Node.js (<code>SairaBackendServices/tests</code>): Jest/Mocha for Node.js services (AI inference server, data manager, model manager). Mock C++ native addon calls.</li> <li>Python (<code>SairaBackendServices/tests</code>): Pytest for Python scripts (RAG ingestion, model downloader).</li> <li>C++ Native Addons (<code>SairaBackendServices/native_addons/</code>): Google Test/Catch2 for C++ code within native addons.</li> </ul> </li> <li>Integration Test Scenarios:<ul> <li>Frontend-Backend IPC: Test communication between React Native <code>IPCService</code> and Node.js backend (HTTP/WebSocket endpoints).</li> <li>AI Pipeline: Test the full flow from audio input (simulated) -&gt; ASR -&gt; LLM -&gt; TTS, including emotion detection and memory retrieval, ensuring data passes correctly through Node.js and C++ addons.</li> <li>Data Persistence: Test CRUD operations through <code>DataStoreService</code> to <code>data_manager.ts</code> and <code>sqlite_vec_addon</code>, including encryption and <code>sqlite-vec</code> functionality.</li> <li>Background Services: Test the launching and communication with Node.js background processes (e.g., Wake Word Detector).</li> <li>Offline Sync (Future): Test sync initiation, delta processing, and conflict resolution with a mock cloud backend.</li> </ul> </li> <li>End-to-End Test Flows (e.g., Detox for React Native):<ul> <li>Onboarding: Simulate a new user onboarding, including consent and initial setup.</li> <li>Full Conversation: Simulate a complete voice conversation, including wake word, multiple turns, memory formation, and personality adaptation.</li> <li>Journaling Workflow: Create, view, edit, and delete journal entries.</li> <li>Settings Changes: Verify that changes in privacy settings, personality modes, and nudge preferences are correctly applied and persisted.</li> <li>Model Update: Simulate an in-app model download and hot-swapping.</li> </ul> </li> <li>Performance Testing Thresholds:<ul> <li>Voice Latency: End-to-end latency (user speaks to AI responds) &lt; 2.0 seconds on MacBook M1 (acknowledging potential overhead from Node.js/Python wrappers).</li> <li>ASR Latency: Transcription of 10 seconds of audio &lt; 300ms.[12]</li> <li>TTS Latency: Synthesis of 50 words &lt; 600ms.</li> <li>LLM Inference: Response generation for typical prompt &lt; 1.5 seconds.</li> <li>RAG Retrieval: Memory retrieval for RAG context &lt; 200ms.</li> <li>App Launch Time: Cold launch &lt; 5 seconds.</li> <li>Memory/CPU Usage: Monitor and optimize for low resource consumption, especially for background services.</li> </ul> </li> </ul> </li> <li> <p>Data Management</p> <ul> <li>Data Lifecycle Policies:<ul> <li>Conversation History: Stored indefinitely locally, with user options for manual deletion or automated archival/summarization.</li> <li>Raw Audio: Not stored by default for privacy. If user explicitly consents, stored temporarily for debugging/analysis and then purged after a defined period (e.g., 24 hours).</li> <li>Memories: Stored indefinitely, with user control over visibility (<code>is_active</code> flag).</li> <li>Journal Entries: Stored indefinitely.</li> <li>AI Models: Managed via in-app updates, older versions can be purged after successful update.</li> </ul> </li> <li>Caching Strategies:<ul> <li>UI Data Caching: Use in-memory caches (e.g., <code>Map</code> objects in JavaScript, or state management libraries) for frequently accessed UI data (e.g., recent conversation messages, mood summaries) to improve responsiveness.</li> <li>AI Model Caching: AI models (LLM, ASR, TTS, SER, Embedding) will be loaded into memory by the Node.js backend on demand and kept resident for subsequent inferences, leveraging Apple Silicon's unified memory architecture.</li> <li>Database Query Caching: SQLite's internal caching mechanisms (e.g., page cache) will be utilized. WAL mode will be enabled for improved concurrency and write performance].</li> </ul> </li> <li>Pagination and Infinite Scrolling:<ul> <li>Implement pagination for displaying long lists of data (e.g., conversation history, journal entries, memory list) to optimize memory usage and initial load times.</li> <li>Use infinite scrolling in UI lists to load more data as the user scrolls, providing a smooth experience.</li> </ul> </li> <li>Real-time Data Requirements:<ul> <li>Audio Stream: Real-time, low-latency audio capture and playback via Node.js C++ native addon for Core Audio.</li> <li>ASR/SER Output: Real-time streaming of transcription and emotion data from Node.js backend to React Native frontend via WebSockets.</li> <li>LLM/TTS Output: Streaming of LLM tokens and TTS audio from Node.js backend to React Native frontend via WebSockets for immediate response.</li> <li>Wake Word: Continuous, real-time monitoring in a low-power Node.js background process (using C++ native addon).</li> </ul> </li> </ul> </li> <li> <p>Error Handling &amp; Logging</p> <ul> <li>Structured Logging Format:<ul> <li>Implement a consistent, structured logging format (e.g., JSON-based) for all layers (React Native, Node.js, Python, C++ addons).</li> <li>Logs will include: <code>timestamp</code>, <code>level</code> (DEBUG, INFO, WARN, ERROR, FATAL), <code>component</code> (e.g., \"ASRInference\", \"ConversationScreen\"), <code>message</code>, <code>error_code</code> (if applicable), <code>stack_trace</code> (for errors), and relevant <code>metadata</code> (e.g., <code>user_id</code>, <code>conversation_id</code>).</li> <li>No sensitive user data (raw audio, conversation content) will be logged, only anonymized metadata.</li> </ul> </li> <li>Error Classification and Prioritization:<ul> <li>Fatal Errors: Application crashes, unrecoverable data corruption. Trigger immediate crash reporting (if user opts-in) and prompt for restart.</li> <li>Critical Errors: AI model loading failures, database corruption, persistent audio I/O issues. Display prominent error messages to the user, suggest troubleshooting steps.</li> <li>Warning Errors: Temporary network issues for sync, minor AI inference glitches. Log, but may not require immediate user intervention.</li> <li>Informational/Debug: General operational messages, performance metrics. Used for development and optional diagnostic bundles.</li> </ul> </li> <li>Monitoring and Alerting Thresholds (Local):<ul> <li>Performance Metrics: Monitor AI inference latency, CPU/GPU usage, memory footprint. Log deviations from expected thresholds.</li> <li>Resource Usage: Alert if CPU/memory usage exceeds predefined limits for extended periods.</li> <li>Error Rates: Track frequency of specific error types.</li> </ul> </li> <li>Recovery Mechanisms:<ul> <li>Graceful Degradation: If an AI model fails to load, attempt to use a fallback (e.g., a smaller, less accurate model) or inform the user.</li> <li>Database Corruption: Implement database integrity checks on launch. If corruption detected, attempt automatic repair or prompt user to restore from backup/sync.</li> <li>Offline Operation: Ensure core functionality remains available even if optional cloud sync fails or is disabled.</li> <li>User-Initiated Diagnostics: Provide a \"Diagnostic Report\" feature in settings, allowing users to generate an encrypted <code>.zip</code> file of anonymized logs and system information for support purposes. This will require explicit user consent before generation and sharing.</li> <li>Model Rollback: If a new AI model update causes instability, provide an option to revert to the previous stable version.</li> </ul> </li> </ul> </li> </ol>"},{"location":"feature_implementation/#feature-real-time-voice-interaction-asr-ser-llm-tts-wake-word","title":"Feature: Real-time Voice Interaction (ASR, SER, LLM, TTS, Wake Word)","text":"<p>Feature Goal To enable natural, real-time conversational interaction with Saira through voice, including always-on wake word detection, accurate speech-to-text transcription, real-time emotion detection, intelligent LLM response generation, and natural text-to-speech synthesis. This feature is the core of Saira's \"emotion-first\" AI.</p> <p>Any API Relationships *   Core Application Framework: Receives voice input, displays transcription and AI responses, and manages the conversation flow. *   Local Long-Term Memory (RAG): LLM leverages memories for contextual responses. *   Personality Modes &amp; Customization: LLM adapts responses based on active personality and user tone preferences. *   Daily Check-ins &amp; Journaling: Emotion data from voice interaction can feed into mood summaries.</p> <p>Detailed Feature Requirements</p> <ol> <li>Wake Word Detection:<ul> <li>Always-on Listening: Continuously monitor microphone input for a pre-defined wake word (\"Hey Saira\") with minimal CPU/battery consumption.</li> <li>Low Latency Activation: Activate the full voice processing pipeline immediately upon wake word detection.</li> <li>Accuracy: Minimize false positives (activating without the wake word) and false negatives (failing to detect the wake word).</li> </ul> </li> <li>Speech-to-Text (ASR):<ul> <li>Real-time Transcription: Convert spoken audio into text with low latency, displaying partial results as the user speaks.</li> <li>Accuracy: High accuracy for English speech, even in varying acoustic environments.</li> <li>Offline Capability: Perform all transcription locally on the device.</li> </ul> </li> <li>Speech Emotion Recognition (SER):<ul> <li>Real-time Emotion Detection: Analyze speech audio to detect continuous emotional states (arousal and valence) in real-time.</li> <li>Granular Output: Provide continuous arousal (0-1) and valence (-1 to 1) values, and potentially categorical emotions (e.g., happy, sad, neutral).</li> <li>Integration with LLM: Feed detected emotions to the LLM to influence its response generation.</li> </ul> </li> <li>Large Language Model (LLM) Interaction:<ul> <li>Natural Language Understanding: Interpret user's intent and context from transcribed text.</li> <li>Contextual Response Generation: Generate coherent, relevant, and natural-sounding text responses.</li> <li>Personality Adaptation: Adjust response style, tone, and vocabulary based on the active personality mode (e.g., \"Mom,\" \"Coach\").</li> <li>Emotional Responsiveness: Adapt LLM responses based on the user's detected emotional state (e.g., offer comfort if sadness is detected).</li> <li>Long-Term Memory Integration (RAG): Incorporate relevant memories and goals from the local knowledge base into LLM's context for personalized responses.</li> </ul> </li> <li>Text-to-Speech (TTS):<ul> <li>Natural Sounding Voice: Synthesize AI's text responses into natural-sounding speech.</li> <li>Real-time Streaming: Begin playing audio as soon as the first part of the response is generated, minimizing perceived latency.</li> <li>Offline Capability: Perform all TTS locally on the device.</li> <li>Voice Adaptation: Potentially adjust AI's voice tone/emotion based on the personality mode or user's emotional state.</li> </ul> </li> </ol> <p>Detailed Implementation Guide</p> <ol> <li> <p>System Architecture Overview</p> <ul> <li>High-level Architecture: This feature is primarily handled by the <code>SairaBackendServices</code> (Node.js/Python/C++ addons) due to its performance-critical nature. The <code>Saira</code> (React Native) acts as the client, sending audio input and receiving text/audio output via a local WebSocket server.</li> <li>Technology Stack:<ul> <li>Frontend (UI): React Native for macOS (TypeScript) for microphone access (via native module), displaying real-time transcription, and playing back audio.</li> <li>Backend (Orchestration &amp; IPC): Node.js (<code>ai_inference_server.ts</code>, <code>audio_processor.ts</code>). Node.js will manage the audio streams, orchestrate calls to AI models, and handle WebSocket communication with the frontend. Node.js is generally faster than Python for I/O-bound tasks and concurrent API requests.</li> <li>Core AI Libraries (via C++ Native Addons or Python Bindings):<ul> <li>Wake Word: Porcupine (C library) via Node.js C++ native addon (<code>ai_inference_addon</code>) or Python binding (<code>pvporcupine</code>) exposed via a local Python server. Porcupine is compact, efficient, and cross-platform. Note: Porcupine requires a commercial license for commercial use.</li> <li>ASR: <code>whisper.cpp</code> (C/C++) via Node.js C++ native addon (<code>ai_inference_addon</code> using <code>nodejs-whisper</code>) or Python binding (<code>pywhispercpp</code>) exposed via a local Python server. <code>whisper.cpp</code> is optimized for offline use and Apple Silicon.[13, 12, 14]</li> <li>SER: <code>openSMILE</code> (C++) for feature extraction and a custom MLP model (C++ or ONNX via C++ addon) for classification. SenseVoice (C++) for direct emotion detection. All via Node.js C++ native addon (<code>ai_inference_addon</code>) or Python bindings (<code>openSMILE</code> Python package) exposed via a local Python server. SER aims for granular arousal/valence output].</li> <li>LLM: Mistral 7B (Q4_0) via <code>llama.cpp</code> (C/C++) wrapped by Node.js C++ native addon (<code>ai_inference_addon</code> using <code>node-llama-cpp</code>) or Python binding (<code>llama-cpp-python</code>) exposed via a local Python server. <code>llama.cpp</code> is optimized for local inference. Note: Python bindings can be slower than direct C++ calls.</li> <li>TTS: Piper (C++) via Node.js C++ native addon (<code>ai_inference_addon</code>) or Python binding (<code>RealtimeTTS</code> [15]) exposed via a local Python server. Piper is lightweight and real-time.</li> </ul> </li> <li>Audio I/O: macOS Core Audio API via a Node.js C++ native addon (<code>audio_io_addon</code>) wrapping PortAudio. This is critical as Node.js/Python wrappers for low-latency audio I/O on macOS have documented stability issues or incomplete functionality.</li> </ul> </li> <li>Deployment Architecture: The Node.js backend (<code>ai_inference_server.ts</code>, <code>audio_processor.ts</code>) will run as a persistent background process, potentially launched by the main React Native app. The C++ native addons will be compiled and bundled with the Node.js runtime. Python scripts will be bundled and executed by the Node.js process using <code>child_process.spawn</code> or <code>exec</code>.</li> <li>Integration Points:<ul> <li>React Native: Uses <code>react-native-webrtc</code> or a custom native module (Objective-C/Swift) for microphone access and WebSocket client.</li> <li>Node.js Backend: Communicates with C++ native addons via N-API. Uses <code>ws</code> (WebSocket library) for real-time communication with the frontend. Spawns Python processes for specific AI tasks.</li> </ul> </li> </ul> </li> <li> <p>Database Schema Design     (Refer to the \"Core Application Framework\" feature for <code>Users</code>, <code>Conversations</code>, <code>Messages</code>, <code>Memories</code>, <code>PersonalityProfiles</code> tables, as they are central to storing conversational data and user preferences that influence AI behavior.)</p> </li> <li> <p>Comprehensive API Design</p> <ul> <li>Frontend-Backend IPC (WebSocket <code>ai_inference_server.ts</code>):<ul> <li>Client (Frontend) -&gt; Server (Backend):<ul> <li><code>start_audio_stream</code>: Initiates audio capture and processing.<ul> <li>Payload: <code>{ userId: string, conversationId: string, personalityId: string }</code></li> </ul> </li> <li><code>audio_chunk</code>: Streams raw audio data (e.g., PCM <code>ArrayBuffer</code>).<ul> <li>Payload: <code>ArrayBuffer</code></li> </ul> </li> <li><code>stop_audio_stream</code>: Stops audio capture and processing.<ul> <li>Payload: <code>{}</code></li> </ul> </li> <li><code>send_text_input</code>: Sends text input (for text-based interaction or corrections).<ul> <li>Payload: <code>{ userId: string, conversationId: string, text: string }</code></li> </ul> </li> </ul> </li> <li>Server (Backend) -&gt; Client (Frontend):<ul> <li><code>transcription_update</code>: Partial or full ASR transcription.<ul> <li>Payload: <code>{ text: string, isFinal: boolean }</code></li> </ul> </li> <li><code>emotion_update</code>: Real-time emotion data.<ul> <li>Payload: <code>{ arousal: number, valence: number, timestamp: number }</code></li> </ul> </li> <li><code>ai_response_text</code>: LLM's text response.<ul> <li>Payload: <code>{ text: string }</code></li> </ul> </li> <li><code>ai_response_audio_chunk</code>: Streams TTS audio data.<ul> <li>Payload: <code>ArrayBuffer</code></li> </ul> </li> <li><code>wake_word_detected</code>: Notification that wake word was detected.<ul> <li>Payload: <code>{}</code></li> </ul> </li> <li><code>error</code>: General error message.<ul> <li>Payload: <code>{ code: string, message: string }</code></li> </ul> </li> </ul> </li> </ul> </li> <li>Node.js Backend (Internal APIs using C++ Native Addons and Python <code>child_process</code>):<ul> <li><code>audio_io_addon</code> (N-API):<ul> <li><code>startCapture(sampleRate: number, channels: number, callback: (buffer: ArrayBuffer) =&gt; void)</code></li> <li><code>stopCapture()</code></li> <li><code>playAudio(buffer: ArrayBuffer, sampleRate: number, channels: number)</code></li> <li><code>getDevices(): { input: AudioDevice, output: AudioDevice }</code></li> </ul> </li> <li><code>ai_inference_addon</code> (N-API):<ul> <li><code>loadModel(type: 'llm' | 'asr' | 'tts' | 'ser' | 'wake_word' | 'embedding', modelPath: string, config?: object)</code></li> <li><code>llmGenerate(prompt: string, context: string, memories: string, personalityPrompt: string, emotionData: EmotionDataDTO): string</code></li> <li><code>asrTranscribe(audioBuffer: ArrayBuffer): string</code></li> <li><code>ttsSynthesize(text: string): ArrayBuffer</code></li> <li><code>serDetect(audioBuffer: ArrayBuffer): EmotionDataDTO</code></li> <li><code>wakeWordProcess(audioBuffer: ArrayBuffer): boolean</code> (returns true if detected)</li> <li><code>embeddingGenerate(text: string): ArrayBuffer</code></li> </ul> </li> </ul> </li> <li>Authentication &amp; Authorization: (Refer to \"Core Application Framework\" feature.)</li> <li>Error Handling: WebSocket errors will be sent to the frontend. Internal Node.js errors will be logged. C++ addon errors will be propagated as Node.js exceptions.</li> <li>Rate Limiting/Caching: Not applicable for real-time local processing.</li> </ul> </li> <li> <p>Frontend Architecture (React Native for macOS)</p> <ul> <li><code>ConversationScreen.tsx</code>:<ul> <li>Manages the UI state for the chat interface.</li> <li>Uses <code>react-native-microphone-wrapper</code> (or similar native module) for microphone access.</li> <li>Establishes a WebSocket connection to <code>ai_inference_server.ts</code>.</li> <li>Sends audio chunks and receives transcription/AI responses.</li> <li>Displays real-time transcription, AI text, and plays AI audio.</li> </ul> </li> <li><code>VoiceInputIndicator.tsx</code>: A reusable component to visually represent active listening, processing, and speaking states.</li> <li>State Management: Local component state for input text, chat messages. Global state for AI service status (listening, thinking, speaking).</li> <li>Routing: Direct navigation to <code>ConversationScreen</code> from <code>MainTabView</code>.</li> </ul> </li> <li> <p>Detailed CRUD Operations</p> <ul> <li>Conversation (Interacts with <code>ConversationManager</code> in Frontend, <code>data_manager.ts</code> in Backend):<ul> <li>Create: A new conversation is implicitly created when the user starts interacting with the AI.</li> <li>Read: Messages are read from the <code>Messages</code> table to display conversation history.</li> <li>Update: Conversation title can be updated.</li> <li>Delete: Entire conversation can be deleted.</li> </ul> </li> <li>Message (Interacts with <code>MessageManager</code> in Frontend, <code>data_manager.ts</code> in Backend):<ul> <li>Create: User's spoken/typed input and AI's responses are added as new messages.</li> <li>Read: Messages are retrieved for display.</li> <li>Update: User can edit their own messages (e.g., journal entries).</li> <li>Delete: Individual messages can be deleted.</li> </ul> </li> <li>Memory (Interacts with <code>MemoryManager</code> in Frontend, <code>ai_inference_server.ts</code> and <code>data_manager.ts</code> in Backend):<ul> <li>Create: New memories are extracted from conversations and stored. This involves calling <code>ai_inference_server.ts</code> for embedding generation and then <code>data_manager.ts</code> for storage.</li> <li>Read: Memories are retrieved for RAG context and for user review in settings.</li> <li>Update: Memories can be updated (e.g., <code>is_active</code> status).</li> <li>Delete: Memories can be deleted by the user.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow     (Refer to \"Core Application Framework\" feature for the detailed voice interaction loop.)</p> </li> <li> <p>Security Considerations</p> <ul> <li>Microphone Access: React Native will request microphone permission from the user via standard macOS permission prompts.</li> <li>Data Flow: Raw audio data is streamed directly from the React Native app to the local Node.js backend via WebSocket. This data is processed locally and never leaves the device unless optional cloud sync is enabled.</li> <li>AI Model Integrity: AI models downloaded via in-app updates will be verified with checksums and cryptographic signatures to prevent tampering.</li> <li>Process Isolation: The Node.js backend process and its C++ addons will run in a sandboxed environment where possible, limiting their access to system resources.</li> <li>Privacy: Explicit user consent for emotion and memory recording is paramount. If consent is not given, these features will be disabled, and relevant data will not be processed or stored.</li> </ul> </li> <li> <p>Testing Strategy</p> <ul> <li>Unit Tests:<ul> <li>React Native: Test UI components, state management, and service calls in isolation.</li> <li>Node.js: Test individual backend service modules (e.g., <code>ai_inference_server.ts</code> functions, <code>data_manager.ts</code> methods).</li> <li>Python: Test Python scripts (RAG ingestion, model downloader).</li> <li>C++ Addons: Test C++ functions within addons for correctness and performance.</li> </ul> </li> <li>Integration Tests:<ul> <li>Audio Pipeline: Simulate audio input, verify ASR output, SER output, LLM response, and TTS audio output.</li> <li>IPC: Test WebSocket communication between frontend and backend.</li> <li>AI Model Loading: Verify models load correctly and perform basic inference.</li> </ul> </li> <li>End-to-End Tests: Simulate full voice conversations, including wake word detection, multi-turn interactions, and verification of correct AI responses and memory formation.</li> <li>Performance Tests: Measure end-to-end latency for voice interactions, ASR/TTS/LLM inference times, and resource consumption on target MacBook M1 devices.</li> </ul> </li> <li> <p>Data Management</p> <ul> <li>Audio Data: Raw audio is processed in real-time and generally not persisted. If explicit user consent is given for audio recording (e.g., for debugging or future features), it will be stored temporarily and securely, then purged.</li> <li>AI Model Storage: Models are stored in a dedicated, sandboxed application directory.</li> <li>Memory Management: LLM context (short-term memory) is managed in-memory by the Node.js backend. Long-term memories are stored in SQLite with <code>sqlite-vec</code>.</li> <li>Real-time Data Flow: Audio streams, ASR/SER outputs, and TTS audio are handled as continuous data streams via WebSockets for low latency.</li> </ul> </li> <li> <p>Error Handling &amp; Logging</p> <ul> <li>Structured Logging: Implement structured logging across all layers (React Native, Node.js, Python, C++ addons). Logs will be stored locally and encrypted.</li> <li>Error Propagation: Errors from C++ addons will be caught in Node.js, translated, and propagated to the React Native frontend via WebSocket messages.</li> <li>User Feedback: Clear, user-friendly error messages will be displayed in the UI for critical issues (e.g., \"Microphone not found,\" \"AI model failed to load\").</li> <li>Diagnostic Bundles: Allow users to generate encrypted diagnostic bundles containing anonymized logs for support.</li> </ul> </li> </ol>"},{"location":"feature_implementation/#feature-specifications-continued","title":"Feature Specifications (Continued)","text":""},{"location":"feature_implementation/#feature-local-long-term-memory-rag","title":"Feature: Local Long-Term Memory (RAG)","text":"<p>Feature Goal To enable Saira to remember past conversations, track user goals, and build a comprehensive, evolving long-term memory about the user's life. This memory will be used to ground LLM responses, providing personalized and contextually relevant interactions.</p> <p>Any API Relationships *   Core Application Framework: Provides UI for memory visibility, goal management, and RAG document ingestion. *   Real-time Voice Interaction: LLM leverages memories for contextual responses. New memories are formed from conversations. *   Daily Check-ins &amp; Journaling: Journal entries can be sources for new memories.</p> <p>Detailed Feature Requirements</p> <ol> <li>Memory Formation:<ul> <li>Automatic Extraction: Automatically extract key facts, preferences, and significant events from user conversations and journal entries.</li> <li>Categorization: Classify extracted memories into types (semantic, episodic, procedural).</li> <li>Embedding Generation: Generate vector embeddings for each memory using a local embedding model.</li> </ul> </li> <li>Memory Storage and Retrieval:<ul> <li>Local Vector Database: Store memories and their embeddings in a local SQLite database with <code>sqlite-vec</code> extension.</li> <li>Semantic Search: Enable efficient retrieval of relevant memories based on semantic similarity to the current conversation context or user query.</li> <li>Filtering: Allow filtering memories by type, source, or user-defined tags.</li> </ul> </li> <li>LLM Integration (Retrieval-Augmented Generation - RAG):<ul> <li>Context Augmentation: Dynamically retrieve a small set of highly relevant memories based on the current conversation turn.</li> <li>Prompt Injection: Inject retrieved memories into the LLM's prompt to provide additional context and ground its responses.</li> <li>Relevance Scoring: Prioritize memories based on a relevance score (e.g., cosine similarity).</li> </ul> </li> <li>User Control over Memory:<ul> <li>Memory Visibility Dashboard: Provide a UI for users to view, search, and manage their stored memories.</li> <li>Edit/Delete Memories: Allow users to edit the content of memories or delete them.</li> <li>Active/Inactive Toggle: Enable users to toggle the <code>is_active</code> status of memories, controlling whether they are used by the LLM.</li> </ul> </li> <li>Goal Tracking:<ul> <li>Goal Creation/Editing: Allow users to define, edit, and track personal goals.</li> <li>Progress Updates: Enable manual or AI-assisted updates to goal progress.</li> <li>AI Integration: LLM can proactively ask about goals, offer motivation, or suggest relevant actions based on goals.</li> </ul> </li> <li>RAG Document Ingestion:<ul> <li>Manual Import: Allow users to manually import documents (PDFs, text files, web clippings) to expand the knowledge base.</li> <li>Optional Auto-Ingestion: With explicit user consent, allow the app to scan and process local files from user-configured folders.</li> <li>Background Processing: Chunking and embedding generation for new documents will run asynchronously in the background.</li> </ul> </li> </ol> <p>Detailed Implementation Guide</p> <ol> <li> <p>System Architecture Overview</p> <ul> <li>High-level Architecture: This feature spans both the React Native frontend (for UI management of memories/goals) and the Node.js/Python backend. The Node.js backend orchestrates the RAG process, utilizing Python for heavy data processing (chunking, embedding generation) and Node.js C++ addons for SQLite/vector search.</li> <li>Technology Stack:<ul> <li>Frontend (UI): React Native for macOS (TypeScript) for Memory Visibility Dashboard, Goal Management UI, and Document Ingestion UI.</li> <li>Backend (Orchestration): Node.js (<code>ai_inference_server.ts</code>, <code>data_manager.ts</code>, <code>model_manager.ts</code>).</li> <li>Embedding Generation: Local embedding model (e.g., MiniLM) via Node.js C++ native addon (<code>ai_inference_addon</code>) or a Python script (<code>EmbeddingGenerator.py</code>) called by Node.js. <code>llama.cpp</code> can be used to generate embeddings via its server mode, which can be called from Node.js or Python.[3]</li> <li>Vector Database: SQLite with <code>sqlite-vec</code> extension via Node.js C++ native addon (<code>sqlite_vec_addon</code>). <code>sqlite-vec</code> is a C library that extends SQLite for vector search. Node.js packages like <code>better-sqlite3</code> can load <code>sqlite-vec</code>. <code>op-sqlite</code> for React Native also supports <code>sqlite-vec</code>.[4, 5, 6]</li> <li>Document Processing (Chunking): Python (<code>rag_ingestion.py</code>) for parsing and chunking documents. Python's rich ecosystem of libraries (e.g., <code>LangChain</code> for document loaders and text splitters, <code>NLTK</code> for text processing) makes it well-suited for this task.</li> <li>IPC: Local HTTP/WebSocket for real-time RAG queries, <code>child_process</code> for spawning Python scripts for background ingestion.</li> </ul> </li> <li>Deployment Architecture: Python scripts will be bundled with the Node.js backend. The <code>sqlite-vec</code> extension will be loaded by the Node.js SQLite binding.</li> <li>Integration Points:<ul> <li>React Native: Calls <code>MemoryManager</code> and <code>GoalManager</code> services.</li> <li>Node.js Backend: Calls C++ addons for embedding and vector search. Spawns Python processes for document ingestion.</li> </ul> </li> </ul> </li> <li> <p>Database Schema Design     (Refer to the \"Core Application Framework\" feature for <code>Memories</code> and <code>Goals</code> tables. The <code>Memories</code> table includes an <code>embedding</code> BLOB column and is linked to a <code>sqlite-vec</code> virtual table.)</p> </li> <li> <p>Comprehensive API Design</p> <ul> <li>Frontend-Backend IPC (HTTP Endpoints on <code>ai_inference_server.ts</code>):<ul> <li><code>POST /ai/embeddings/generate</code>: (Already defined in Voice Interaction)</li> <li><code>POST /data/search_vectors</code>: (Already defined in Voice Interaction)</li> <li><code>POST /rag/ingest_document</code>: Initiates document ingestion.<ul> <li>Request: <code>{ userId: string, filePath: string, autoIngest: boolean }</code></li> <li>Response: <code>{ success: boolean, documentId: string }</code></li> </ul> </li> <li><code>GET /memories/search</code>: Semantic search for memories.<ul> <li>Request: <code>{ userId: string, query: string, limit: number }</code></li> <li>Response: <code>{ memories: MemoryDTO }</code></li> </ul> </li> </ul> </li> <li>Node.js Backend (Internal APIs using C++ Native Addons and Python <code>child_process</code>):<ul> <li><code>ai_inference_addon</code> (N-API):<ul> <li><code>embeddingGenerate(text: string): ArrayBuffer</code> (wraps <code>llama.cpp</code> embedding server or other C++ embedding model).</li> </ul> </li> <li><code>sqlite_vec_addon</code> (N-API):<ul> <li><code>searchVectors(embedding: ArrayBuffer, tableName: string, limit: number): { id: string, score: number }</code></li> <li><code>insertVector(tableName: string, id: string, embedding: ArrayBuffer): boolean</code></li> <li><code>updateVector(tableName: string, id: string, embedding: ArrayBuffer): boolean</code></li> <li><code>deleteVector(tableName: string, id: string): boolean</code></li> </ul> </li> <li><code>rag_ingestion.py</code> (Python script, spawned by Node.js <code>child_process</code>):<ul> <li>This script will receive <code>filePath</code> and <code>userId</code> as command-line arguments.</li> <li>It will read the document, chunk it, and then make HTTP calls back to the Node.js <code>ai_inference_server.ts</code> to generate embeddings and store memories.</li> <li> <p>Pseudocode:     ```python     # rag_ingestion.py     import sys     import json     import requests # For HTTP calls back to Node.js server     # Assuming document parsing and chunking libraries (e.g., LangChain, NLTK)</p> <p>NODE_SERVER_URL = \"http://localhost:XXXX\" # Replace with actual Node.js server URL</p> <p>def process_document(file_path, user_id):     # 1. Read document content     content = read_file(file_path) # Implement read_file based on file_type (PDF, TXT, etc.)</p> <pre><code># 2. Chunk text\n# Example using a hypothetical chunking function\nchunks = chunk_text(content, chunk_size=500, overlap=50)\n\n# 3. Process each chunk\nfor chunk in chunks:\n    # Call Node.js backend for embedding generation\n    embedding_response = requests.post(f\"{NODE_SERVER_URL}/ai/embeddings/generate\", json={\"text\": chunk})\n    embedding_response.raise_for_status()\n    embedding_data = embedding_response.json()[\"embedding\"]\n\n    # Call Node.js backend to store memory (chunk + embedding)\n    memory_payload = {\n        \"userId\": user_id,\n        \"type\": \"semantic\", # Or derive type\n        \"content\": chunk,\n        \"embedding\": embedding_data,\n        \"sourceType\": \"document\",\n        \"sourceId\": file_path # Or a unique document ID\n    }\n    store_memory_response = requests.post(f\"{NODE_SERVER_URL}/data/create_memory\", json=memory_payload)\n    store_memory_response.raise_for_status()\n    print(f\"Stored memory for chunk: {chunk[:50]}...\")\n</code></pre> <p>if name == \"main\":     file_path = sys.argv[1]     user_id = sys.argv[2]     process_document(file_path, user_id) ```     *   Authentication &amp; Authorization: (Refer to \"Core Application Framework\" feature.)     *   Error Handling: Errors during embedding generation or vector search will be logged and potentially surfaced to the user if they impact core functionality. Document ingestion errors (e.g., malformed PDF) will be logged and reported in the UI.     *   Rate Limiting/Caching: Not applicable for local-first.</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Frontend Architecture (React Native for macOS)</p> <ul> <li><code>DeepTalkScreen.tsx</code>:<ul> <li>A specialized chat interface that emphasizes RAG.</li> <li>When user asks a question, it first performs a semantic search on memories by calling <code>IPCService</code> to <code>ai_inference_server.ts</code>'s <code>/memories/search</code> endpoint.</li> <li>The retrieved memories are then passed as context to the LLM.</li> </ul> </li> <li><code>ProfileSettingsScreen.tsx</code> (Memory Visibility &amp; Goal Management):<ul> <li>Displays lists of memories and goals.</li> <li>Provides search/filter functionality for memories.</li> <li>Allows editing/deleting memories and goals.</li> <li>Includes a \"Document Ingestion\" section for manual import and auto-ingestion settings.</li> <li>Uses <code>react-native-document-picker</code> or similar for file selection.</li> </ul> </li> <li>State Management: Local state for search queries, selected memories/goals. Global state for RAG processing status (e.g., \"Indexing documents...\").</li> <li>Routing: Direct navigation to <code>DeepTalkScreen</code> and <code>ProfileSettingsScreen</code>.</li> </ul> </li> <li> <p>Detailed CRUD Operations</p> <ul> <li>Memory (Interacts with <code>MemoryManager</code> in Frontend, <code>ai_inference_server.ts</code> and <code>data_manager.ts</code> in Backend):<ul> <li>Create:<ul> <li>Automatic: Triggered by <code>ConversationManager</code> or <code>JournalManager</code> after a conversation turn or journal entry. Calls <code>IPCService</code> to <code>ai_inference_server.ts</code>'s <code>/ai/embeddings/generate</code> and then <code>data_manager.ts</code> to insert into <code>Memories</code> and <code>sqlite-vec</code>.</li> <li>Manual (via Document Ingestion): User imports a document. Frontend calls <code>ai_inference_server.ts</code>'s <code>/rag/ingest_document</code>. Backend spawns <code>rag_ingestion.py</code> which then calls back to <code>ai_inference_server.ts</code> to create memories.</li> <li>Validation: <code>userId</code> exists, <code>content</code> not empty.</li> <li>Required Fields: <code>user_id</code>, <code>type</code>, <code>content</code>, <code>embedding</code>, <code>timestamp</code>.</li> </ul> </li> <li>Read: <code>getMemories</code>, <code>searchMemories</code> for display in UI and for RAG.<ul> <li>Filtering: By <code>id</code>, <code>user_id</code>, <code>type</code>, <code>is_active</code>.</li> <li>Pagination: <code>offset</code>, <code>limit</code>.</li> <li>Sorting: By <code>timestamp</code> (descending) or <code>relevance_score</code> (for search).</li> <li>Flow: UI requests memories -&gt; <code>MemoryManager.getMemories</code>/<code>searchMemories</code> calls <code>IPCService</code> to <code>ai_inference_server.ts</code> for query embedding (if search) -&gt; <code>ai_inference_server.ts</code> calls <code>data_manager.ts</code> to search <code>sqlite-vec</code> and retrieve <code>Memories</code>.</li> </ul> </li> <li>Update: <code>updateMemory(id: string, content?: string, isActive?: boolean)</code>, <code>consolidateMemories(userId: string, memoryIds: string, newContent: string)</code><ul> <li>Partial Updates: Update <code>content</code>, <code>is_active</code>.</li> <li>Consolidation: Combine multiple memories into one, updating content and potentially re-embedding.</li> <li>Validation: <code>content</code> not empty.</li> <li>Flow: User edits memory or AI consolidates memories -&gt; <code>MemoryManager.updateMemory</code>/<code>consolidateMemories</code> calls <code>IPCService</code> to <code>ai_inference_server.ts</code> (if content changed for re-embedding) -&gt; <code>ai_inference_server.ts</code> calls <code>data_manager.ts</code> to update <code>Memories</code>.</li> </ul> </li> <li>Delete: <code>deleteMemory(id: string)</code><ul> <li>Soft Delete: Not applicable.</li> <li>Hard Delete: Deletes specific memory and its vector from <code>sqlite-vec</code>.</li> <li>Flow: User deletes memory from profile -&gt; <code>MemoryManager.deleteMemory</code> calls <code>IPCService</code> -&gt; <code>data_manager.ts</code> deletes from <code>Memories</code> and <code>sqlite-vec</code>.</li> </ul> </li> </ul> </li> <li>Goal (Interacts with <code>GoalManager</code> in Frontend, <code>data_manager.ts</code> in Backend):<ul> <li>Create: <code>createGoal</code> via UI.</li> <li>Read: <code>getGoals</code> for display.</li> <li>Update: <code>updateGoal</code> (e.g., status, progress, description).</li> <li>Delete: <code>deleteGoal</code> (hard delete).</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow</p> <ul> <li>RAG-Enhanced Conversation:<ol> <li>User asks a question in <code>ConversationScreen</code> or <code>DeepTalkScreen</code>.</li> <li>Frontend sends query to <code>ai_inference_server.ts</code>.</li> <li>Backend generates embedding for query.</li> <li>Backend performs <code>sqlite-vec</code> search on <code>Memories</code> table. Performance for <code>sqlite-vec</code> with 250,000 records on M1 Max MacBook can be slow, requiring performance tuning.</li> <li>Top-k relevant memories are retrieved.</li> <li>These memories are injected into the LLM prompt.</li> <li>LLM generates response, grounded by retrieved memories.</li> </ol> </li> <li>Memory Management:<ol> <li>User navigates to Profile Settings -&gt; Memories.</li> <li>Displays a list of all memories.</li> <li>User can search, filter, or toggle <code>is_active</code> for memories.</li> <li>User can tap on a memory to view/edit its content.</li> </ol> </li> <li>Document Ingestion:<ol> <li>User navigates to Profile Settings -&gt; Document Ingestion.</li> <li>User selects files for manual import or configures auto-ingestion folders.</li> <li>Frontend sends request to <code>ai_inference_server.ts</code>'s <code>/rag/ingest_document</code> endpoint.</li> <li>Backend spawns <code>rag_ingestion.py</code> in a child process.</li> <li>Python script processes documents, sends chunks to Node.js for embedding and storage.</li> <li>UI displays \"Indexing...\" status and completion notification.</li> </ol> </li> </ul> </li> <li> <p>Security Considerations</p> <ul> <li>Data Encryption: All memories and embeddings stored in SQLite will be encrypted using SQLCipher.</li> <li>File System Access: Auto-ingestion will require explicit user consent for file system access to specified directories. The app will operate within macOS Sandbox entitlements.</li> <li>IPC for RAG: Communication between Node.js and Python for RAG processing will use secure <code>child_process</code> pipes or local sockets.</li> <li>Privacy: Ensure that only user-consented data is used for memory formation and RAG. Memory visibility controls are critical for user privacy.</li> </ul> </li> <li> <p>Testing Strategy</p> <ul> <li>Unit Tests: Test <code>rag_ingestion.py</code> for correct chunking and embedding generation (mocking embedding service). Test <code>data_manager.ts</code> for <code>sqlite-vec</code> interactions.</li> <li>Integration Tests:<ul> <li>Full RAG Pipeline: Test from document ingestion -&gt; embedding -&gt; storage -&gt; retrieval -&gt; LLM context injection.</li> <li>Memory CRUD: Verify all memory CRUD operations function correctly, including <code>sqlite-vec</code> updates.</li> </ul> </li> <li>End-to-End Tests: Simulate user importing documents, asking questions that require RAG, and verifying the LLM's grounded responses.</li> <li>Performance Tests: Measure latency for RAG queries (embedding query + vector search), and throughput for document ingestion. Monitor memory usage during large ingestion tasks.</li> </ul> </li> <li> <p>Data Management</p> <ul> <li>Memory Storage: Memories and their embeddings are stored persistently in the encrypted SQLite database.</li> <li>RAG Document Storage: Original documents are not necessarily stored in the database; only their chunked text and embeddings are.</li> <li>Indexing Strategy: <code>sqlite-vec</code> provides efficient brute-force vector search. For very large datasets (e.g., 250,000+ records), performance tuning may be required.</li> <li>Data Lifecycle: Users have control over deleting memories. Old or less relevant memories could potentially be archived or summarized by the AI over time (future feature).</li> </ul> </li> <li> <p>Error Handling &amp; Logging</p> <ul> <li>RAG-Specific Errors: Log errors during document parsing, embedding generation failures, or <code>sqlite-vec</code> issues.</li> <li>User Feedback: Inform users about failed document imports or indexing issues.</li> <li>Performance Monitoring: Log RAG query times and memory consumption to identify bottlenecks.</li> </ul> </li> </ol>"},{"location":"feature_implementation/#feature-offline-first-data-synchronization","title":"Feature: Offline-First Data Synchronization","text":"<p>Feature Goal To provide seamless data continuity across user devices (Mac, future iOS) by offering optional, end-to-end encrypted cloud synchronization of user data (memories, preferences, emotional data), ensuring privacy and data availability even when offline.</p> <p>Any API Relationships *   Core Application Framework: Provides UI for sync configuration and status. *   Local Long-Term Memory (RAG): Memories and goals are key data points for synchronization. *   Daily Check-ins &amp; Journaling: Journal entries and mood summaries are synchronized. *   Personality Modes &amp; Customization: Personality profiles and user preferences are synchronized.</p> <p>Detailed Feature Requirements</p> <ol> <li>Optional Cloud Sync:<ul> <li>User Opt-in: Sync is strictly opt-in, requiring explicit user consent.</li> <li>Privacy-First: All data synchronized must be end-to-end encrypted (E2EE), ensuring zero-knowledge for the cloud provider.</li> </ul> </li> <li>Data Selection for Sync:<ul> <li>Core Data: Synchronize user profiles, conversation summaries (not full content by default), memories, goals, journal entries, mood summaries, and personality profiles.</li> <li>Exclusions: Raw audio data and full conversation transcripts are not synced by default due to privacy and size concerns, unless explicitly configured by the user (future feature).</li> </ul> </li> <li>Synchronization Protocol:<ul> <li>Delta Sync: Only synchronize changes (deltas) to minimize bandwidth and improve efficiency.</li> <li>Change Journal: Maintain a local change journal to track modifications for efficient delta calculation.</li> <li>Conflict Resolution: Implement conflict resolution strategies (e.g., timestamp-based \"last write wins,\" or user-guided manual review for critical conflicts).</li> </ul> </li> <li>User Control &amp; Transparency:<ul> <li>Sync Frequency: Allow users to configure sync frequency (e.g., manual, daily, weekly, on app close).</li> <li>Sync Status: Display clear sync status indicators (e.g., \"Synced,\" \"Syncing,\" \"Offline,\" \"Conflicts\").</li> <li>Audit Logs: Record sync events in local audit logs for user review.</li> </ul> </li> <li>Error Handling &amp; Recovery:<ul> <li>Robustness: Handle network interruptions gracefully, with retry mechanisms.</li> <li>Data Integrity: Ensure data integrity during sync, preventing data loss or corruption.</li> <li>Recovery Key: Provide a mechanism for users to generate and securely store a recovery key for their E2EE data.</li> </ul> </li> </ol> <p>Detailed Implementation Guide</p> <ol> <li> <p>System Architecture Overview</p> <ul> <li>High-level Architecture: The <code>SyncService.ts</code> in the React Native frontend will orchestrate the sync process. It will communicate with a local Node.js <code>SyncManager</code> service (within <code>SairaBackendServices/src/node_services/</code>) which handles the E2EE and interaction with the chosen cloud sync provider.</li> <li>Technology Stack:<ul> <li>Frontend (UI): React Native for macOS (TypeScript) for sync settings UI, status indicators, and conflict resolution modals.</li> <li>Backend (Orchestration &amp; E2EE): Node.js (TypeScript) for managing the sync process, including E2EE logic. Node.js has robust cryptographic libraries (e.g., Node.js <code>crypto</code> module, <code>libsodium-wrappers</code> for advanced crypto).</li> <li>Local Database: SQLite with SQLCipher for encrypted local data storage.</li> <li>Cloud Sync Protocol: Utilize an existing open-source E2EE sync protocol like EteSync (which offers an SDK and backend) or implement a custom protocol. EteSync is open-source, provides zero-knowledge E2EE, and maintains a change journal.[7, 8, 9] Dropbox also offers E2EE for sensitive files.[10, 11]</li> <li>IPC: Local HTTP/WebSocket between React Native and Node.js for triggering syncs and receiving status updates.</li> </ul> </li> <li>Deployment Architecture: The Node.js <code>SyncManager</code> will run as a background process, potentially as a <code>child_process</code> spawned by the main React Native app, or as a separate daemon if more persistent background operation is needed.</li> <li>Integration Points:<ul> <li>React Native: Calls <code>SyncService</code> methods.</li> <li>Node.js Backend: Interacts with the local SQLite database (<code>data_manager.ts</code>) to read/write data and change logs. Communicates with the external E2EE cloud service API (e.g., EteSync's API) over HTTPS.</li> </ul> </li> </ul> </li> <li> <p>Database Schema Design     (Refer to \"Core Application Framework\" feature for <code>Users</code> table, specifically <code>sync_enabled</code>, <code>last_synced_at</code>, <code>sync_frequency</code> fields.)</p> <ul> <li> <p><code>ChangeLog</code> Table: This table is crucial for tracking local modifications for delta synchronization.</p> <ul> <li><code>id</code> (TEXT, PRIMARY KEY, UUID)</li> <li><code>user_id</code> (TEXT, FOREIGN KEY to <code>Users.id</code>, NOT NULL)</li> <li><code>entity_type</code> (TEXT, ENUM: 'conversation', 'message', 'memory', 'goal', 'journal_entry', 'mood_summary', 'personality_profile', 'user_preference', NOT NULL)</li> <li><code>entity_id</code> (TEXT, NOT NULL) - ID of the record that was changed.</li> <li><code>change_type</code> (TEXT, ENUM: 'CREATE', 'UPDATE', 'DELETE', NOT NULL)</li> <li><code>timestamp</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>payload</code> (BLOB, NULLABLE) - Encrypted JSON representation of the changed fields (for UPDATE) or the full new record (for CREATE). For DELETE, payload might be empty or contain just the ID.</li> <li><code>synced</code> (INTEGER, BOOLEAN, DEFAULT 0) - Flag to indicate if this change has been successfully synced.</li> <li><code>sync_version</code> (TEXT, NULLABLE) - Version identifier from the cloud sync for this change.</li> <li><code>conflict_resolved_at</code> (TIMESTAMP, NULLABLE) - Timestamp if this change was part of a conflict resolution.</li> </ul> </li> <li> <p><code>SyncState</code> Table: To store metadata about sync sessions and the last known state from the cloud.</p> <ul> <li><code>user_id</code> (TEXT, PRIMARY KEY, FOREIGN KEY to <code>Users.id</code>)</li> <li><code>last_successful_sync_timestamp</code> (TIMESTAMP, NULLABLE)</li> <li><code>last_sync_token</code> (TEXT, NULLABLE) - A token or cursor provided by the cloud service to indicate the last synced state.</li> <li><code>current_sync_status</code> (TEXT, ENUM: 'idle', 'syncing', 'conflicts', 'error', NOT NULL, DEFAULT 'idle')</li> <li><code>recovery_key_hash</code> (TEXT, NULLABLE) - Hash of the user's E2EE recovery key.</li> <li><code>created_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> <li><code>updated_at</code> (TIMESTAMP, NOT NULL, DEFAULT CURRENT_TIMESTAMP)</li> </ul> </li> <li> <p>Indexing Strategy:</p> <ul> <li>Indexes on <code>ChangeLog.user_id</code>, <code>ChangeLog.synced</code>, <code>ChangeLog.timestamp</code> for efficient retrieval of pending changes.</li> <li>Index on <code>SyncState.user_id</code>.</li> </ul> </li> <li>Foreign Key Relationships: Enforce <code>ON DELETE CASCADE</code> for <code>ChangeLog</code> and <code>SyncState</code> to <code>Users</code>.</li> <li>Database Migration/Versioning: (Refer to \"Core Application Framework\" feature.)</li> </ul> </li> <li> <p>Comprehensive API Design</p> <ul> <li> <p>Frontend (React Native) -&gt; Backend (Node.js) IPC (HTTP Endpoints on <code>ai_inference_server.ts</code>):</p> <ul> <li><code>POST /sync/start</code>: Initiate a synchronization cycle.<ul> <li>Request: <code>{ userId: string }</code></li> <li>Response: <code>{ status: string, message: string }</code></li> </ul> </li> <li><code>GET /sync/status</code>: Get the current synchronization status.<ul> <li>Request: <code>{ userId: string }</code></li> <li>Response: <code>{ status: string, lastSyncedAt: string | null, pendingChanges: number, conflicts: number }</code></li> </ul> </li> <li><code>POST /sync/configure</code>: Update synchronization settings.<ul> <li>Request: <code>{ userId: string, syncEnabled: boolean, syncFrequency: string, cloudCredentials?: object }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>POST /sync/resolve_conflict</code>: Resolve a specific data conflict.<ul> <li>Request: <code>{ userId: string, conflictId: string, resolution: 'keep_local' | 'keep_cloud' | 'merge', mergedData?: object }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>POST /sync/generate_recovery_key</code>: Generate and store an E2EE recovery key.<ul> <li>Request: <code>{ userId: string }</code></li> <li>Response: <code>{ recoveryKey: string }</code></li> </ul> </li> <li><code>POST /sync/export_data</code>: Export all user data for portability.<ul> <li>Request: <code>{ userId: string, format: 'json' | 'csv' }</code></li> <li>Response: <code>{ filePath: string }</code></li> </ul> </li> <li><code>POST /sync/delete_all_data</code>: Request deletion of all user data.<ul> <li>Request: <code>{ userId: string, confirmationToken: string }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> </ul> </li> <li> <p>Node.js Backend (Internal APIs using Node.js <code>crypto</code> and external cloud service SDK/API):</p> <ul> <li><code>SyncManager</code> (TypeScript Class in <code>SairaBackendServices/src/node_services/</code>):<ul> <li><code>initiateSync(userId: string)</code>: Main sync orchestration method.</li> <li><code>uploadChanges(userId: string)</code>: Reads <code>ChangeLog</code>, encrypts, sends to cloud.</li> <li><code>downloadChanges(userId: string)</code>: Fetches deltas from cloud, decrypts, applies locally.</li> <li><code>handleConflicts(userId: string, conflicts: any)</code>: Manages conflict detection and resolution.</li> <li><code>encryptData(data: object, encryptionKey: Buffer): Buffer</code>: Uses E2EE library.</li> <li><code>decryptData(encryptedData: Buffer, encryptionKey: Buffer): object</code>: Uses E2EE library.</li> <li><code>generateE2EEKeys(passphrase: string): { publicKey: Buffer, privateKey: Buffer }</code>: Key derivation.</li> <li><code>connectToCloudService(credentials: object)</code>: Establishes connection to the chosen cloud sync provider (e.g., EteSync API client).</li> </ul> </li> <li><code>data_manager.ts</code>: (Already defined, but will be extended to interact with <code>ChangeLog</code> and <code>SyncState</code> tables).<ul> <li><code>recordChange(userId: string, entityType: string, entityId: string, changeType: string, payload: object)</code>: Inserts into <code>ChangeLog</code>.</li> <li><code>getPendingChanges(userId: string): ChangeLogEntry</code>: Retrieves unsynced changes.</li> <li><code>markChangesAsSynced(changeIds: string): void</code>: Updates <code>synced</code> flag in <code>ChangeLog</code>.</li> <li><code>getSyncState(userId: string): SyncState | null</code>: Retrieves current sync state.</li> <li><code>updateSyncState(userId: string, updates: SyncStateUpdateDTO): void</code>: Updates sync state.</li> </ul> </li> </ul> </li> <li> <p>Authentication &amp; Authorization: (Refer to \"Core Application Framework\" feature.)</p> </li> <li>Error Handling: Network errors, E2EE failures, and data integrity issues will be caught and logged. Sync status will be updated to 'error' and communicated to the frontend.</li> <li>Rate Limiting/Caching: Not applicable for local-first. Cloud sync service might have its own rate limits, which the <code>SyncManager</code> should respect with retry-after headers and exponential backoff.</li> </ul> </li> <li> <p>Frontend Architecture (React Native for macOS)</p> <ul> <li><code>ProfileSettingsScreen.tsx</code> (Sync Configuration Section):<ul> <li>Toggle for \"Enable Cloud Sync\".</li> <li>Input fields for cloud service credentials (if applicable).</li> <li>Dropdown for \"Sync Frequency\" (Manual, Daily, Weekly).</li> <li>Display of \"Last Synced At\" timestamp and current sync status.</li> <li>Button for \"Sync Now\".</li> <li>Button for \"Generate Recovery Key\".</li> <li>Button for \"Export All Data\".</li> <li>Button for \"Delete All My Data\".</li> </ul> </li> <li><code>SyncStatusIndicator.tsx</code>: A small, persistent UI component (e.g., in the app's menu bar or a corner of the main window) that shows the current sync status (e.g., green checkmark for synced, spinning icon for syncing, red exclamation for error/conflicts).</li> <li><code>ConflictResolutionModal.tsx</code>: A modal dialog that appears when sync conflicts are detected, presenting the user with options to resolve them (e.g., \"Keep Local Version,\" \"Keep Cloud Version,\" \"Merge Manually\").</li> <li>State Management: Global state for <code>syncStatus</code>, <code>lastSyncedAt</code>, <code>pendingChangesCount</code>, <code>conflictsCount</code>.</li> <li>Routing: Direct navigation to <code>ProfileSettingsScreen</code>.</li> </ul> </li> <li> <p>Detailed CRUD Operations</p> <ul> <li>Sync Initiation (Read/Update <code>ChangeLog</code>, <code>SyncState</code>):<ul> <li>Flow: User clicks \"Sync Now\" or scheduled sync triggers -&gt; <code>SyncService.initiateSync(userId)</code> is called.</li> <li><code>SyncManager</code> reads <code>SyncState</code> to get <code>last_successful_sync_timestamp</code> and <code>last_sync_token</code>.</li> <li><code>SyncManager</code> reads <code>ChangeLog</code> to get all <code>synced = 0</code> entries.</li> <li>UI updates <code>syncStatus</code> to 'syncing'.</li> </ul> </li> <li>Data Upload (Create/Update/Delete via <code>ChangeLog</code>):<ul> <li>Flow: <code>SyncManager.uploadChanges(userId)</code> iterates through pending <code>ChangeLog</code> entries.</li> <li>For each entry:<ul> <li>Retrieves <code>payload</code>.</li> <li>Encrypts <code>payload</code> using E2EE keys.</li> <li>Sends encrypted data to the cloud sync service API (e.g., <code>POST /cloud/delta_upload</code>).</li> <li>On successful upload, <code>data_manager.ts.markChangesAsSynced(changeId)</code> updates the <code>ChangeLog</code> entry.</li> </ul> </li> <li>Validation: Ensure <code>payload</code> is valid JSON/BLOB. Cloud service will validate E2EE integrity.</li> </ul> </li> <li>Data Download (Read from Cloud, Create/Update/Delete Local DB):<ul> <li>Flow: <code>SyncManager.downloadChanges(userId)</code> calls cloud sync service API (e.g., <code>GET /cloud/delta_download?since=last_sync_token</code>).</li> <li>Receives encrypted deltas from cloud.</li> <li>Decrypts each delta.</li> <li>For each decrypted delta:<ul> <li>Identifies <code>entity_type</code>, <code>entity_id</code>, <code>change_type</code>, <code>payload</code>.</li> <li>Applies change to local SQLite database via <code>data_manager.ts</code> (e.g., <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on relevant tables).</li> <li>If a conflict is detected (e.g., local record modified since last sync, and cloud also has a conflicting update), the conflict is recorded in a temporary in-memory store or a <code>Conflicts</code> table.</li> </ul> </li> <li>Updates <code>SyncState.last_successful_sync_timestamp</code> and <code>last_sync_token</code>.</li> </ul> </li> <li>Conflict Resolution (Read/Update <code>Conflicts</code> table, Update Local DB):<ul> <li>Flow: If conflicts exist after download, <code>SyncManager.handleConflicts(userId, conflicts)</code> is called.</li> <li><code>ConflictResolutionModal.tsx</code> is displayed in the frontend.</li> <li>User selects resolution for each conflict.</li> <li>Frontend sends resolution choice to <code>ai_inference_server.ts</code>'s <code>/sync/resolve_conflict</code>.</li> <li>Backend applies the chosen resolution to the local database and updates the <code>ChangeLog</code> (e.g., marking the original conflicting local change as resolved).</li> <li>If \"Merge Manually\" is chosen, the UI provides an editor for the user to combine versions, and the merged data is sent back as an update.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow</p> <ul> <li>Initial Sync Setup:<ol> <li>User navigates to Profile Settings -&gt; Sync.</li> <li>Toggles \"Enable Cloud Sync\".</li> <li>A modal appears for cloud service authentication (e.g., EteSync username/password).</li> <li>Upon successful authentication, the app performs an initial full sync. UI shows a progress bar or \"Syncing...\" status.</li> <li>Upon completion, status changes to \"Synced\".</li> </ol> </li> <li>Background Sync:<ol> <li>App runs in background.</li> <li>At configured frequency (e.g., daily), <code>SyncManager</code> initiates a background sync.</li> <li><code>SyncStatusIndicator</code> changes to \"Syncing\".</li> <li>If successful, reverts to \"Synced\".</li> <li>If network error, status changes to \"Offline\" or \"Error\", with a retry mechanism.</li> </ol> </li> <li>Conflict Resolution:<ol> <li>During a sync, if a conflict is detected (e.g., same memory edited locally and on another device).</li> <li><code>SyncStatusIndicator</code> changes to \"Conflicts Detected\".</li> <li><code>ConflictResolutionModal.tsx</code> appears, listing conflicts.</li> <li>For each conflict, user sees both versions and chooses \"Keep Local,\" \"Keep Cloud,\" or \"Merge.\"</li> <li>After resolving all, sync completes.</li> </ol> </li> <li>Data Portability/Deletion:<ol> <li>User navigates to Privacy Dashboard.</li> <li>Clicks \"Export All Data\". App generates encrypted <code>.zip</code> file of data and provides download path.</li> <li>Clicks \"Delete All My Data\". App prompts for confirmation (e.g., re-enter password). Upon confirmation, all local and synced cloud data is purged.</li> </ol> </li> </ul> </li> <li> <p>Security Considerations</p> <ul> <li>End-to-End Encryption (E2EE):<ul> <li>Key Management: Encryption keys for E2EE will be derived locally from a user-provided passphrase (or biometric unlock) using a strong Key Derivation Function (KDF) like PBKDF2 or Argon2. These keys are never sent to the cloud.</li> <li>Data Encryption: All data sent to the cloud will be encrypted on the client-side (Node.js backend) before transmission, using a robust symmetric encryption algorithm (e.g., AES-256 in GCM mode). The EteSync protocol is a strong reference for this, providing zero-knowledge security.[7, 8, 9]</li> <li>Integrity: Use authenticated encryption (e.g., AES-GCM) to ensure data integrity and authenticity during transit and at rest in the cloud.</li> </ul> </li> <li>Local Data at Rest: The entire SQLite database will be encrypted using SQLCipher, with the encryption key derived from the user's local authentication.</li> <li>GDPR/CCPA Compliance:<ul> <li>Consent: Explicit consent for sync and data types synced.</li> <li>Right to Access/Portability: The \"Export All Data\" feature provides data portability. The exported data will be encrypted and require the user's recovery key to decrypt.</li> <li>Right to Erasure: The \"Delete All My Data\" feature ensures all user data is purged locally and from the cloud sync service.</li> <li>Audit Logs: Local, encrypted audit logs will record sync events, data exports, and deletion requests.</li> </ul> </li> <li>Network Security: All communication with the cloud sync service will use HTTPS with strict TLS certificate pinning (if feasible for a desktop app) to prevent Man-in-the-Middle attacks.</li> <li>Recovery Key: Users will be prompted to generate and securely store a recovery key (e.g., a mnemonic phrase or a long alphanumeric string) that can be used to restore access to their encrypted data if they forget their passphrase. This key must be generated and stored locally by the user, never by Saira.</li> </ul> </li> <li> <p>Testing Strategy</p> <ul> <li>Unit Tests:<ul> <li>Node.js: Test <code>SyncManager</code> logic, E2EE encryption/decryption functions, delta calculation, and conflict resolution algorithms in isolation.</li> <li>React Native: Test UI components for sync settings, status display, and conflict resolution modals.</li> </ul> </li> <li>Integration Tests:<ul> <li>Local-to-Cloud Sync: Test full sync cycles with a mock cloud service, simulating various scenarios: initial sync, delta syncs, offline modifications, network interruptions, and concurrent modifications from multiple simulated devices.</li> <li>E2EE Verification: Test that data is correctly encrypted before upload and decrypted after download, and that the cloud service cannot access plaintext data.</li> <li>Conflict Scenarios: Develop specific test cases for different conflict types (e.g., same record updated by two devices, one deleted and one updated) and verify chosen resolution strategies.</li> <li>Data Integrity: Verify that data remains consistent and uncorrupted after multiple sync cycles and error conditions.</li> </ul> </li> <li>End-to-End Tests:<ul> <li>Simulate a user enabling sync, making changes offline, going online, and verifying data consistency across multiple simulated instances of Saira.</li> <li>Test data export and deletion workflows.</li> </ul> </li> <li>Performance Tests:<ul> <li>Measure sync duration for various data volumes (e.g., initial sync of 1000 memories, daily delta sync of 10 memories).</li> <li>Measure bandwidth usage during sync.</li> <li>Monitor CPU/memory impact of background sync processes.</li> </ul> </li> </ul> </li> <li> <p>Data Management</p> <ul> <li>Change Journal Management: The <code>ChangeLog</code> table will grow over time. Implement a policy to periodically prune or summarize old, synced <code>ChangeLog</code> entries to prevent the database from becoming excessively large. For example, entries older than 30 days and marked <code>synced=1</code> could be deleted.</li> <li>Data Purging: When a user requests data deletion, ensure all associated data in the local SQLite database is securely wiped (e.g., by deleting the database file and recreating it, or using SQLite's <code>VACUUM</code> command after deletion).</li> <li>Handling Large Data Volumes: For initial syncs or large document imports, implement chunking and streaming mechanisms to avoid memory exhaustion.</li> <li>Offline Data Consistency: Ensure that local data remains consistent and usable even if the sync service is temporarily unavailable or disabled.</li> </ul> </li> <li> <p>Error Handling &amp; Logging</p> <ul> <li>Structured Logging: (Refer to \"Core Application Framework\" feature.) Log sync events, including start/end times, data volumes, and any errors.</li> <li>Error Classification:<ul> <li>Network Errors: Transient, trigger retries with exponential backoff.</li> <li>Authentication Errors: Require user re-authentication.</li> <li>Data Integrity Errors: Critical, may require user intervention or a full re-sync.</li> <li>Conflict Errors: Handled by the conflict resolution flow.</li> </ul> </li> <li>User Notifications: Provide clear, actionable notifications to the user for sync failures, conflicts, or successful completion.</li> <li>Recovery Mechanisms:<ul> <li>Retry Logic: Automatic retries for transient network issues.</li> <li>Conflict Resolution: User-guided resolution for data conflicts.</li> <li>Recovery Key: Mechanism to restore access to encrypted data.</li> <li>Full Re-sync Option: Provide a manual option in settings to force a full re-sync from the cloud, as a last resort for data inconsistencies.</li> </ul> </li> </ul> </li> </ol> <p>This concludes the detailed technical specification for the core features of Saira, adhering to your requirements for using Node.js, Python, and JavaScript frameworks, with C++ only for critical native bindings. This document should provide a highly granular foundation for your upcoming task planning.</p>"},{"location":"milestones/","title":"Milestones","text":"<p>\ud83d\udccd Milestone Tracking \u2013 Saira Project (Start Date: June 7, 2025)</p>"},{"location":"milestones/#phase-1-planning-setup-june-7-june-20-2025","title":"\u2705 Phase 1: Planning &amp; Setup (June 7 \u2013 June 20, 2025)","text":"<p>Milestone 1 \u2013 Project Groundwork Complete</p> <ul> <li>Expected Date: June 20, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>Tech stack finalized for all major components (voice, emotion, UI, memory)</p> </li> <li>Monorepo and folder structure established</li> <li>Notion/Linear board fully set up with tasks and labels</li> <li>Technical documentation templates ready</li> <li>Cross-platform design system planned</li> <li>Role-switching AI personalities defined (Mom, Coach, etc.)</li> <li>Local development environment functional on Mac</li> <li>Security model documented with encryption and sync policies</li> </ul>"},{"location":"milestones/#phase-2-core-architecture-development-june-21-july-8-2025","title":"\ud83c\udfd7\ufe0f Phase 2: Core Architecture Development (June 21 \u2013 July 8, 2025)","text":"<p>Milestone 2 \u2013 Foundational Systems Online</p> <ul> <li>Expected Date: July 8, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>Local speech-to-text fully running on device</p> </li> <li>Wake-word detection integrated</li> <li>Local emotional detection works on audio input</li> <li>Vector DB + embeddings system integrated and functional</li> <li>Context manager handling dialogue session memory</li> <li>First-gen response generator replying to inputs</li> <li>Daily logging and event tracking module working</li> </ul>"},{"location":"milestones/#phase-31-feature-integration-proactive-intelligence-july-9-july-24-2025","title":"\ud83d\ude80 Phase 3.1: Feature Integration &amp; Proactive Intelligence (July 9 \u2013 July 24, 2025)","text":"<p>Milestone 3 \u2013 Emotional Assistant Online</p> <ul> <li>Expected Date: July 24, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>Daily check-in and bedtime reflection active</p> </li> <li>Proactive mood-based suggestions implemented</li> <li>Calendar and reminders two-way synced</li> <li>Memory recall commands responding accurately</li> <li>Tone/personality customization functioning</li> <li>Smart journaling saving entries</li> <li>Long-term memory structure created</li> <li>Emotion-adaptive UI displaying correct themes</li> </ul>"},{"location":"milestones/#phase-32-unified-intelligence-core-july-9-july-25-2025","title":"\ud83e\udde0 Phase 3.2: Unified Intelligence Core (July 9 \u2013 July 25, 2025)","text":"<p>Milestone 4 \u2013 Unified Brain &amp; Behavior</p> <ul> <li>Expected Date: July 25, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>All core pipelines connected: speech, emotion, memory</p> </li> <li>Contextual task/calendar suggestions live</li> <li>Daily summary generation active</li> <li>Memory anchoring for major events</li> <li>Life goal tracker with encouragement dialogue</li> <li>Emotion feedback influencing tone and prompts</li> <li>Personalized prompt adaptation working</li> </ul>"},{"location":"milestones/#phase-4-emotional-ux-multi-modal-output-july-26-august-10-2025","title":"\ud83c\udfa8 Phase 4: Emotional UX &amp; Multi-Modal Output (July 26 \u2013 August 10, 2025)","text":"<p>Milestone 5 \u2013 Emotion-Aware Sensory Interface</p> <ul> <li>Expected Date: August 10, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>Text-to-speech engine modulating emotional output</p> </li> <li>Animated avatar showing expressive feedback</li> <li>Mood-based ambient sounds active</li> <li>Physical feedback via light/haptics integrated</li> <li>Emotion-based theme switching operational</li> <li>Voice persona selector usable in-app</li> <li>Emotion model self-adapting via feedback loop</li> <li>Visual effects (toasts/confetti) responding to triggers</li> </ul>"},{"location":"milestones/#phase-5-cross-platform-deployment-sync-august-11-august-28-2025","title":"\ud83d\udce6 Phase 5: Cross-Platform Deployment &amp; Sync (August 11 \u2013 August 28, 2025)","text":"<p>Milestone 6 \u2013 Multi-Device Ecosystem Live</p> <ul> <li>Expected Date: August 28, 2025</li> <li> <p>Criteria for Completion:</p> </li> <li> <p>Electron/macOS app built and distributed</p> </li> <li>iPhone app MVP launched for journaling &amp; check-ins</li> <li>Smart speaker interface built with wake-word</li> <li>All platforms synced with encrypted memory</li> <li>Shared contextual memory accessible from any device</li> </ul> <p>Each milestone marks a checkpoint to validate system integrity, emotional alignment, and cross-platform coherence. Tracking progress by these helps ensure feature completeness and experience consistency.</p>"},{"location":"product_requirnmnts/","title":"Product Requirements Document","text":""},{"location":"product_requirnmnts/#saira-the-emotion-aware-life-companion-ai","title":"Saira - The Emotion-Aware Life Companion AI","text":"<p>Document Version: 1.0 Last Updated: June 10, 2025 Document Owner: Product Team Project Timeline: June 7, 2025 - September 7, 2025  </p>"},{"location":"product_requirnmnts/#1-executive-summary","title":"1. Executive Summary","text":""},{"location":"product_requirnmnts/#11-product-vision","title":"1.1 Product Vision","text":"<p>Saira is an intelligent, empathetic AI companion that understands users' emotions, remembers their conversations, and adapts to their needs across multiple devices. Unlike traditional AI assistants, Saira focuses on emotional intelligence and long-term relationship building, acting as a trusted companion rather than just a task executor.</p>"},{"location":"product_requirnmnts/#12-product-mission","title":"1.2 Product Mission","text":"<p>To create the world's first truly empathetic AI companion that enhances users' emotional well-being through intelligent conversation, proactive support, and adaptive personality modes while maintaining complete privacy and security.</p>"},{"location":"product_requirnmnts/#13-success-metrics","title":"1.3 Success Metrics","text":"<ul> <li>User Engagement: 80%+ daily active usage within first month</li> <li>Emotional Connection: 90%+ users report feeling emotionally supported</li> <li>Privacy Trust: 95%+ users feel confident about data privacy</li> <li>Cross-Platform Adoption: 70%+ users utilize multiple devices</li> </ul>"},{"location":"product_requirnmnts/#2-product-overview","title":"2. Product Overview","text":""},{"location":"product_requirnmnts/#21-core-value-proposition","title":"2.1 Core Value Proposition","text":"<ul> <li>Emotional Intelligence: Detects and responds to emotional states through advanced voice analysis</li> <li>Persistent Memory: Remembers conversations, goals, and preferences using vector embeddings</li> <li>Adaptive Personality: Switches between mom, girlfriend, best friend, or coach modes</li> <li>Privacy-First: All processing happens locally with encrypted synchronization</li> <li>Multi-Platform: Seamless experience across Mac, iPhone, and Smart Speakers</li> </ul>"},{"location":"product_requirnmnts/#22-target-users","title":"2.2 Target Users","text":""},{"location":"product_requirnmnts/#primary-persona-the-emotional-seeker-ages-25-45","title":"Primary Persona: \"The Emotional Seeker\" (Ages 25-45)","text":"<ul> <li>Professionals seeking emotional support and life coaching</li> <li>Values privacy and personal growth</li> <li>Comfortable with technology but wants emotional connection</li> <li>Willing to invest in premium experiences</li> </ul>"},{"location":"product_requirnmnts/#secondary-persona-the-productivity-optimizer-ages-30-50","title":"Secondary Persona: \"The Productivity Optimizer\" (Ages 30-50)","text":"<ul> <li>Busy individuals who want intelligent assistance with emotional awareness</li> <li>Values efficiency but desires more human-like interactions</li> <li>Uses multiple devices throughout the day</li> </ul>"},{"location":"product_requirnmnts/#23-market-positioning","title":"2.3 Market Positioning","text":"<p>Positioned between traditional AI assistants (Siri, Alexa) and therapy/coaching apps, focusing on the emotional intelligence gap in personal AI interactions.</p>"},{"location":"product_requirnmnts/#3-functional-requirements","title":"3. Functional Requirements","text":""},{"location":"product_requirnmnts/#31-core-features","title":"3.1 Core Features","text":""},{"location":"product_requirnmnts/#311-voice-interaction-system","title":"3.1.1 Voice Interaction System","text":"<p>Priority: P0 (Critical)</p> <p>Requirements: - Local speech-to-text processing with 95%+ accuracy - Wake word detection (\"Hey Saira\") with &lt;500ms response time - Text-to-speech with emotional tone adaptation - Support for continuous conversation flow - Noise cancellation and audio optimization</p> <p>Acceptance Criteria: - \u2705 User can initiate conversation with wake word - \u2705 Speech recognition works in various noise environments - \u2705 Voice output matches detected emotional context - \u2705 Conversation maintains natural flow without awkward pauses</p>"},{"location":"product_requirnmnts/#312-emotion-detection-engine","title":"3.1.2 Emotion Detection Engine","text":"<p>Priority: P0 (Critical)</p> <p>Requirements: - Real-time voice emotion analysis - Support for primary emotions: happy, sad, angry, anxious, excited, calm - Confidence scoring for emotion detection - Contextual emotion understanding (not just tone) - Learning from user feedback to improve accuracy</p> <p>Acceptance Criteria: - \u2705 Emotion detection accuracy &gt;80% in controlled tests - \u2705 System responds appropriately to detected emotions - \u2705 False positive rate &lt;15% - \u2705 Emotion history tracking for pattern recognition</p>"},{"location":"product_requirnmnts/#313-memory-management-system","title":"3.1.3 Memory Management System","text":"<p>Priority: P0 (Critical)</p> <p>Requirements: - Vector-based memory storage for conversation context - Long-term memory graph connecting related topics - Personal information extraction and storage - Goal and preference tracking - Memory recall with natural language queries</p> <p>Acceptance Criteria: - \u2705 User can ask \"What did I say about X last week?\" and get accurate responses - \u2705 System remembers personal goals and tracks progress - \u2705 Related memories are surfaced contextually - \u2705 Memory search returns relevant results within 2 seconds</p>"},{"location":"product_requirnmnts/#314-adaptive-personality-system","title":"3.1.4 Adaptive Personality System","text":"<p>Priority: P1 (High)</p> <p>Requirements: - Four distinct personality modes: Mom, Girlfriend, Best Friend, Coach - Automatic mode suggestion based on emotional state - Manual mode switching with immediate adaptation - Personality-specific response patterns and vocabulary - Contextual personality persistence</p> <p>Acceptance Criteria: - \u2705 Each personality mode has distinct communication style - \u2705 Mode switching is smooth and immediate - \u2705 System suggests appropriate modes based on context - \u2705 Personality preferences are remembered per situation type</p>"},{"location":"product_requirnmnts/#32-platform-specific-features","title":"3.2 Platform-Specific Features","text":""},{"location":"product_requirnmnts/#321-mac-application","title":"3.2.1 Mac Application","text":"<p>Priority: P0 (Critical)</p> <p>Requirements: - Native macOS application with system integration - Visual avatar with emotional expressions - Menu bar quick access - Keyboard shortcuts for common actions - Screen sharing awareness for contextual help</p> <p>Acceptance Criteria: - \u2705 App launches quickly (&lt;3 seconds) and runs smoothly - \u2705 Visual avatar reflects conversation emotion appropriately - \u2705 System integration works without permissions issues - \u2705 UI is responsive and follows macOS design guidelines</p>"},{"location":"product_requirnmnts/#322-iphone-companion-app","title":"3.2.2 iPhone Companion App","text":"<p>Priority: P1 (High)</p> <p>Requirements: - Native iOS app with voice and text interaction - Daily check-in notifications - Journal entry creation and review - Quick emotion logging - Seamless sync with Mac app</p> <p>Acceptance Criteria: - \u2705 App works offline for basic functions - \u2705 Notifications are timely and contextually appropriate - \u2705 Data syncs within 30 seconds across devices - \u2705 Battery usage is optimized (&lt;5% drain per hour of active use)</p>"},{"location":"product_requirnmnts/#323-smart-speaker-integration","title":"3.2.3 Smart Speaker Integration","text":"<p>Priority: P2 (Medium)</p> <p>Requirements: - Voice-only interface with wake word activation - Ambient light feedback for emotional states - Integration with home automation systems - Multiple speaker synchronization - Privacy-focused always-listening mode</p> <p>Acceptance Criteria: - \u2705 Wake word detection works reliably from 10+ feet away - \u2705 Light feedback is subtle and contextually appropriate - \u2705 Privacy controls are clearly explained and accessible - \u2705 Multi-room functionality works seamlessly</p>"},{"location":"product_requirnmnts/#33-advanced-features","title":"3.3 Advanced Features","text":""},{"location":"product_requirnmnts/#331-proactive-intelligence","title":"3.3.1 Proactive Intelligence","text":"<p>Priority: P1 (High)</p> <p>Requirements: - Daily check-in routines (morning/evening) - Mood-based activity suggestions - Calendar and reminder integration - Predictive emotional support - Goal progress tracking and motivation</p> <p>Acceptance Criteria: - \u2705 Check-ins happen at user-preferred times - \u2705 Suggestions are relevant and helpful &gt;70% of the time - \u2705 Calendar integration works bidirectionally - \u2705 Predictive support triggers at appropriate moments</p>"},{"location":"product_requirnmnts/#332-journaling-and-reflection","title":"3.3.2 Journaling and Reflection","text":"<p>Priority: P1 (High)</p> <p>Requirements: - AI-assisted journal entry creation - Daily emotional summaries - Weekly/monthly reflection reports - Goal tracking and progress visualization - Mood pattern analysis</p> <p>Acceptance Criteria: - \u2705 Journal entries capture conversation essence accurately - \u2705 Summaries are insightful and actionable - \u2705 Reports help users understand emotional patterns - \u2705 Goal tracking motivates continued engagement</p>"},{"location":"product_requirnmnts/#4-non-functional-requirements","title":"4. Non-Functional Requirements","text":""},{"location":"product_requirnmnts/#41-performance-requirements","title":"4.1 Performance Requirements","text":"<ul> <li>Response Time: Voice responses within 1 second of user input completion</li> <li>Memory Usage: &lt;500MB RAM on Mac, &lt;100MB on iPhone</li> <li>Battery Life: iPhone app should not drain &gt;10% battery per day with normal usage</li> <li>Accuracy: Emotion detection &gt;80%, Speech recognition &gt;95%</li> </ul>"},{"location":"product_requirnmnts/#42-security-privacy-requirements","title":"4.2 Security &amp; Privacy Requirements","text":"<ul> <li>Data Processing: All sensitive processing happens locally on device</li> <li>Encryption: End-to-end encryption for all synced data</li> <li>Data Retention: User controls over data retention periods</li> <li>Compliance: GDPR and CCPA compliant by design</li> <li>Audit Trail: Complete audit logs for all data access and processing</li> </ul>"},{"location":"product_requirnmnts/#43-scalability-requirements","title":"4.3 Scalability Requirements","text":"<ul> <li>Concurrent Users: Support 10,000+ concurrent users at launch</li> <li>Data Growth: Handle 1GB+ of conversation data per user</li> <li>Feature Expansion: Architecture supports rapid feature addition</li> <li>Multi-Language: Framework ready for localization</li> </ul>"},{"location":"product_requirnmnts/#44-reliability-requirements","title":"4.4 Reliability Requirements","text":"<ul> <li>Uptime: 99.9% service availability</li> <li>Error Recovery: Graceful degradation when features are unavailable</li> <li>Data Integrity: Zero data loss with automatic backups</li> <li>Cross-Platform Sync: 99.5% sync success rate</li> </ul>"},{"location":"product_requirnmnts/#5-user-experience-requirements","title":"5. User Experience Requirements","text":""},{"location":"product_requirnmnts/#51-design-principles","title":"5.1 Design Principles","text":"<ul> <li>Emotional Resonance: UI should reflect and respond to user emotions</li> <li>Simplicity: Complex AI capabilities with simple, intuitive interface</li> <li>Consistency: Unified experience across all platforms</li> <li>Accessibility: Support for users with disabilities</li> <li>Personalization: Interface adapts to user preferences and usage patterns</li> </ul>"},{"location":"product_requirnmnts/#52-interaction-design","title":"5.2 Interaction Design","text":"<ul> <li>Conversation Flow: Natural, uninterrupted dialogue experience</li> <li>Visual Feedback: Clear indicators for listening, processing, and responding states</li> <li>Error Handling: Graceful recovery from misunderstandings</li> <li>Context Switching: Smooth transitions between topics and personality modes</li> <li>Multi-Modal: Seamless blend of voice, text, and visual interactions</li> </ul>"},{"location":"product_requirnmnts/#53-detailed-ui-specifications-for-wireframing","title":"5.3 Detailed UI Specifications for Wireframing","text":""},{"location":"product_requirnmnts/#531-mac-application-layout","title":"5.3.1 Mac Application Layout","text":"<p>Main Interface Structure: - Header Bar (60px height): Saira avatar (40px circle), personality mode selector, settings icon - Central Chat Area (70% of window): Conversation bubbles with timestamps, emotional indicators - Input Area (100px height): Microphone button (primary), text input field, send button - Sidebar (300px width, collapsible): Memory recalls, daily summary, mood tracker - Status Indicators: Listening (pulsing blue), Processing (spinning), Speaking (wave animation)</p> <p>Conversation Bubble Design: - User messages: Right-aligned, blue (#007AFF), rounded corners (12px) - Saira responses: Left-aligned, adaptive color based on personality mode, avatar thumbnail - Emotion indicators: Small colored dots (red=angry, blue=sad, green=happy, yellow=excited) - Timestamp: 12px gray text below each bubble</p> <p>Avatar Specifications: - Size: 40px in header, 24px in chat bubbles - Expression States: Happy, sad, thinking, speaking, listening (5 distinct facial expressions) - Animation: Subtle breathing effect, mouth movement during speech - Personality Modes: Different color schemes and styling per mode</p>"},{"location":"product_requirnmnts/#532-iphone-app-layout","title":"5.3.2 iPhone App Layout","text":"<p>Navigation Structure: - Tab Bar (80px height): Chat, Journal, Memories, Settings (4 main tabs) - Header (100px height): Avatar, personality toggle, voice/text mode switch - Content Area: Adaptive based on current tab</p> <p>Chat Tab: - Conversation View: Full-screen chat with floating voice button - Voice Button: 80px floating action button, center bottom, with waveform animation - Quick Actions: Emotion check-in, daily summary, memory search (horizontal scroll)</p> <p>Journal Tab: - Entry List: Card-based layout with date, mood indicator, preview text - Entry Editor: Full-screen with mood selector, voice-to-text, AI suggestions - Mood Timeline: Visual graph showing emotional patterns over time</p> <p>Memories Tab: - Search Bar: Natural language search (\"What did I say about work?\") - Memory Cards: Date, context, related emotions, quick actions - Filters: By date, emotion, personality mode, topics</p>"},{"location":"product_requirnmnts/#533-smart-speaker-interface-visual-elements","title":"5.3.3 Smart Speaker Interface (Visual Elements)","text":"<p>LED Ring Patterns: - Idle: Soft breathing white light - Listening: Blue wave pattern - Processing: Yellow spinning pattern - Speaking: Green wave synchronized with speech - Different Emotions: Color shifts based on detected emotion</p> <p>Physical Controls: - Mute Button: Top button with LED indicator - Volume Ring: Touch-sensitive outer ring - Personality Switch: Physical toggle or voice command</p>"},{"location":"product_requirnmnts/#534-emotional-ui-theming-system","title":"5.3.4 Emotional UI Theming System","text":"<p>Color Palettes by Personality Mode:</p> <p>Mom Mode: - Primary: Warm orange (#FF8C42) - Secondary: Soft pink (#FFB3BA) - Background: Cream (#FFF8E7) - Text: Deep brown (#5D4037)</p> <p>Girlfriend Mode: - Primary: Romantic red (#E91E63) - Secondary: Soft purple (#CE93D8) - Background: Light rose (#FCE4EC) - Text: Dark purple (#4A148C)</p> <p>Best Friend Mode: - Primary: Friendly blue (#2196F3) - Secondary: Bright green (#4CAF50) - Background: Light blue (#E3F2FD) - Text: Dark blue (#0D47A1)</p> <p>Coach Mode: - Primary: Energetic orange (#FF5722) - Secondary: Strong red (#F44336) - Background: Light orange (#FFF3E0) - Text: Dark orange (#BF360C)</p>"},{"location":"product_requirnmnts/#535-component-specifications","title":"5.3.5 Component Specifications","text":"<p>Buttons: - Primary Action: 48px height, rounded corners (24px), personality color - Secondary Action: 40px height, outline style, gray border - Voice Button: 80px circle, floating, with microphone icon and pulse animation - Disabled State: 50% opacity, no hover effects</p> <p>Form Elements: - Text Input: 48px height, rounded corners (8px), border on focus - Dropdown: Native system style, personality color accent - Switches: iOS-style toggle switches, personality color when active - Sliders: Personality color track, white handle with shadow</p> <p>Cards: - Memory Cards: 16px padding, 8px border radius, subtle shadow, white background - Journal Entries: 20px padding, left border in mood color, gray background - Summary Cards: Gradient background based on overall mood, white text</p> <p>Navigation: - Tab Bar: iOS style with icons, active tab in personality color - Navigation Bar: Clean, minimal, back button and title - Breadcrumbs: For deep navigation, shows current path</p>"},{"location":"product_requirnmnts/#536-responsive-design-breakpoints","title":"5.3.6 Responsive Design Breakpoints","text":"<ul> <li>Mobile: 320px - 768px (iPhone layouts)</li> <li>Tablet: 768px - 1024px (iPad layouts, if supported)</li> <li>Desktop: 1024px+ (Mac app window sizing)</li> <li>Minimum Window Size: 800x600px for Mac app</li> </ul>"},{"location":"product_requirnmnts/#537-loading-and-error-states","title":"5.3.7 Loading and Error States","text":"<p>Loading States: - Initial Load: Skeleton screens with personality colors - Voice Processing: Animated waveform with processing message - Memory Search: Animated search icon with \"Thinking...\" text - Sync Status: Small indicator showing sync progress</p> <p>Error States: - Network Error: Friendly message with retry button - Voice Recognition Failed: \"I didn't catch that\" with retry option - Memory Not Found: \"I don't remember that\" with search suggestions - System Error: Graceful degradation with manual alternatives</p>"},{"location":"product_requirnmnts/#538-accessibility-specifications","title":"5.3.8 Accessibility Specifications","text":"<ul> <li>Color Contrast: WCAG AA compliance (4.5:1 ratio minimum)</li> <li>Font Sizes: 16px minimum, scalable up to 24px</li> <li>Touch Targets: 44px minimum for all interactive elements</li> <li>Screen Reader: Semantic HTML, proper ARIA labels</li> <li>Keyboard Navigation: Tab order, focus indicators, keyboard shortcuts</li> </ul>"},{"location":"product_requirnmnts/#53-accessibility-requirements","title":"5.3 Accessibility Requirements","text":"<ul> <li>Voice Control: Complete functionality available through voice</li> <li>Visual Indicators: Alternative feedback for hearing impaired users</li> <li>Screen Reader: Full compatibility with accessibility tools</li> <li>Keyboard Navigation: Complete keyboard-only navigation support</li> <li>Font Scaling: Support for dynamic text sizing</li> </ul>"},{"location":"product_requirnmnts/#6-technical-architecture","title":"6. Technical Architecture","text":""},{"location":"product_requirnmnts/#61-system-architecture","title":"6.1 System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Mac App       \u2502    \u2502  iPhone App     \u2502    \u2502 Smart Speaker   \u2502\n\u2502  (Electron)     \u2502    \u2502  (Native iOS)   \u2502    \u2502   (Custom HW)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                      \u2502                      \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Core AI Engine         \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n                    \u2502  \u2502 Voice Processing    \u2502\u2502\n                    \u2502  \u2502 Emotion Detection   \u2502\u2502\n                    \u2502  \u2502 Memory Management   \u2502\u2502\n                    \u2502  \u2502 Personality Engine  \u2502\u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Sync &amp; Storage Layer   \u2502\n                    \u2502  (Encrypted)            \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"product_requirnmnts/#62-technology-stack","title":"6.2 Technology Stack","text":"<ul> <li>Frontend: Electron (Mac), React Native (iOS), Custom embedded (Speaker)</li> <li>AI/ML: PyTorch, Transformers, Local LLM inference</li> <li>Voice: Whisper (STT), Coqui TTS, Picovoice (Wake word)</li> <li>Database: Vector DB (Chroma/Pinecone), SQLite for metadata</li> <li>Sync: Custom encrypted protocol over HTTPS</li> <li>Infrastructure: Self-hosted with CDN for model distribution</li> </ul>"},{"location":"product_requirnmnts/#63-security-architecture","title":"6.3 Security Architecture","text":"<ul> <li>Local Processing: All sensitive AI processing on-device</li> <li>Encrypted Sync: AES-256 encryption for all synchronized data</li> <li>Key Management: Device-specific keys with secure key exchange</li> <li>Network Security: Certificate pinning, request signing</li> <li>Privacy by Design: Minimal data collection, user consent for all features</li> </ul>"},{"location":"product_requirnmnts/#7-development-timeline","title":"7. Development Timeline","text":""},{"location":"product_requirnmnts/#phase-1-foundation-june-7-20-2025","title":"Phase 1: Foundation (June 7-20, 2025)","text":"<ul> <li>Tech stack finalization and project setup</li> <li>Basic architecture and development environment</li> <li>Design system and personality framework definition</li> </ul>"},{"location":"product_requirnmnts/#phase-2-core-engine-june-21-july-8-2025","title":"Phase 2: Core Engine (June 21 - July 8, 2025)","text":"<ul> <li>Voice processing pipeline implementation</li> <li>Emotion detection engine development</li> <li>Memory system and vector storage setup</li> </ul>"},{"location":"product_requirnmnts/#phase-3-feature-integration-july-9-25-2025","title":"Phase 3: Feature Integration (July 9-25, 2025)","text":"<ul> <li>Personality modes and adaptive behavior</li> <li>Proactive intelligence and daily routines</li> <li>Long-term memory graph implementation</li> </ul>"},{"location":"product_requirnmnts/#phase-4-multi-modal-experience-july-26-august-10-2025","title":"Phase 4: Multi-Modal Experience (July 26 - August 10, 2025)","text":"<ul> <li>Text-to-speech with emotional adaptation</li> <li>Visual avatar and UI theming system</li> <li>Ambient feedback and haptic integration</li> </ul>"},{"location":"product_requirnmnts/#phase-5-cross-platform-deployment-august-11-25-2025","title":"Phase 5: Cross-Platform Deployment (August 11-25, 2025)","text":"<ul> <li>Mac, iPhone, and Speaker app development</li> <li>Secure synchronization implementation</li> <li>Cross-device context preservation</li> </ul>"},{"location":"product_requirnmnts/#phase-6-polish-testing-august-26-september-1-2025","title":"Phase 6: Polish &amp; Testing (August 26 - September 1, 2025)","text":"<ul> <li>Comprehensive testing and optimization</li> <li>UI/UX refinement and accessibility</li> <li>Security audit and privacy validation</li> </ul>"},{"location":"product_requirnmnts/#phase-7-launch-september-2-7-2025","title":"Phase 7: Launch (September 2-7, 2025)","text":"<ul> <li>Public beta release</li> <li>Feedback collection and rapid iteration</li> <li>Marketing and user acquisition</li> </ul>"},{"location":"product_requirnmnts/#8-success-metrics-kpis","title":"8. Success Metrics &amp; KPIs","text":""},{"location":"product_requirnmnts/#81-user-engagement-metrics","title":"8.1 User Engagement Metrics","text":"<ul> <li>Daily Active Users (DAU): Target 80% of registered users</li> <li>Session Duration: Average 15+ minutes per session</li> <li>Retention Rate: 70% after 30 days, 50% after 90 days</li> <li>Feature Adoption: 60%+ users try all personality modes</li> </ul>"},{"location":"product_requirnmnts/#82-quality-metrics","title":"8.2 Quality Metrics","text":"<ul> <li>Emotion Detection Accuracy: &gt;80% validated accuracy</li> <li>User Satisfaction: &gt;4.5/5 average rating</li> <li>Support Ticket Volume: &lt;5% of active users per month</li> <li>Crash Rate: &lt;0.1% of sessions</li> </ul>"},{"location":"product_requirnmnts/#83-business-metrics","title":"8.3 Business Metrics","text":"<ul> <li>User Acquisition Cost: &lt;$50 per acquired user</li> <li>Lifetime Value: &gt;$200 per user</li> <li>Privacy Trust Score: &gt;90% users feel data is secure</li> <li>Cross-Platform Usage: 70%+ users active on multiple devices</li> </ul>"},{"location":"product_requirnmnts/#84-technical-metrics","title":"8.4 Technical Metrics","text":"<ul> <li>Response Time: 95% of interactions &lt;2 seconds</li> <li>Uptime: 99.9% service availability</li> <li>Sync Success Rate: &gt;99% cross-device synchronization</li> <li>Battery Impact: &lt;10% daily drain on mobile devices</li> </ul>"},{"location":"product_requirnmnts/#9-risk-assessment-mitigation","title":"9. Risk Assessment &amp; Mitigation","text":""},{"location":"product_requirnmnts/#91-technical-risks","title":"9.1 Technical Risks","text":"<p>High Risk: Local AI Performance - Risk: On-device AI models may be too slow or inaccurate - Mitigation: Hybrid approach with cloud fallback, model optimization - Contingency: Progressive model complexity based on device capabilities</p> <p>Medium Risk: Cross-Platform Sync Complexity - Risk: Maintaining consistency across devices proves challenging - Mitigation: Robust conflict resolution, comprehensive testing - Contingency: Simplify sync initially, add complexity incrementally</p>"},{"location":"product_requirnmnts/#92-product-risks","title":"9.2 Product Risks","text":"<p>High Risk: User Emotional Attachment Concerns - Risk: Users become overly dependent on AI companion - Mitigation: Built-in wellness checks, professional therapy referrals - Contingency: Clear communication about AI limitations</p> <p>Medium Risk: Privacy Perception Issues - Risk: Users distrust emotional AI despite privacy measures - Mitigation: Transparent privacy communication, third-party audits - Contingency: Open-source core components, community verification</p>"},{"location":"product_requirnmnts/#93-business-risks","title":"9.3 Business Risks","text":"<p>Medium Risk: Regulatory Changes - Risk: AI regulation impacts development or deployment - Mitigation: Proactive compliance, legal consultation - Contingency: Adaptable architecture for regulatory requirements</p> <p>Low Risk: Competition from Big Tech - Risk: Major companies launch similar products - Mitigation: Focus on privacy differentiation, rapid iteration - Contingency: Pivot to B2B or specialized markets</p>"},{"location":"product_requirnmnts/#10-launch-strategy","title":"10. Launch Strategy","text":""},{"location":"product_requirnmnts/#101-beta-launch-september-2-7-2025","title":"10.1 Beta Launch (September 2-7, 2025)","text":"<ul> <li>Target Audience: 1,000 selected early adopters</li> <li>Platform: Mac app only initially</li> <li>Feedback Channels: In-app feedback, weekly surveys, user interviews</li> <li>Success Criteria: &gt;4.0 rating, &lt;20% churn in first week</li> </ul>"},{"location":"product_requirnmnts/#102-public-launch-q4-2025","title":"10.2 Public Launch (Q4 2025)","text":"<ul> <li>Platform Strategy: Mac first, iOS within 30 days, Speaker within 60 days</li> <li>Pricing Model: Freemium with premium personality features</li> <li>Marketing Focus: Privacy-first emotional AI, authentic testimonials</li> <li>Launch Goals: 10,000 registrations in first month</li> </ul>"},{"location":"product_requirnmnts/#103-post-launch-iteration","title":"10.3 Post-Launch Iteration","text":"<ul> <li>Feedback Integration: Bi-weekly feature updates based on user input</li> <li>Feature Expansion: Advanced emotion analysis, group conversations</li> <li>Platform Growth: Android app, web interface, API for developers</li> <li>Scaling Strategy: Geographic expansion, language localization</li> </ul>"},{"location":"product_requirnmnts/#11-appendices","title":"11. Appendices","text":""},{"location":"product_requirnmnts/#111-competitive-analysis","title":"11.1 Competitive Analysis","text":"<ul> <li>Replika: Emotional AI companion but lacks multi-platform and privacy focus</li> <li>Xiaoice: Advanced conversational AI but limited to chat interface</li> <li>Apple Siri: Multi-platform but lacks emotional intelligence</li> <li>Opportunity: Combine emotional intelligence with privacy and multi-platform seamlessness</li> </ul>"},{"location":"product_requirnmnts/#112-user-research-summary","title":"11.2 User Research Summary","text":"<ul> <li>Survey Results: 78% of users want more emotional intelligence in AI assistants</li> <li>Interview Insights: Privacy concerns are the #1 barrier to adoption</li> <li>Usage Patterns: Users prefer voice in private settings, text in public</li> <li>Feature Requests: Memory persistence across devices, proactive emotional support</li> </ul>"},{"location":"product_requirnmnts/#114-wireframing-specifications","title":"11.4 Wireframing Specifications","text":""},{"location":"product_requirnmnts/#1141-screen-flow-diagrams","title":"11.4.1 Screen Flow Diagrams","text":"<p>Mac App User Flows: 1. Onboarding Flow: Welcome \u2192 Personality Setup \u2192 Voice Calibration \u2192 Privacy Settings \u2192 First Conversation 2. Daily Usage Flow: Launch \u2192 Quick Greeting \u2192 Conversation \u2192 Memory Creation \u2192 Session End 3. Memory Recall Flow: Memory Search \u2192 Results Display \u2192 Conversation Context \u2192 Related Memories 4. Settings Flow: Main Settings \u2192 Personality Modes \u2192 Privacy Controls \u2192 Voice Settings \u2192 Sync Status</p> <p>iPhone App User Flows: 1. First Launch: Splash \u2192 Permissions \u2192 Account Setup \u2192 Sync with Mac \u2192 Tutorial 2. Daily Check-in: Notification \u2192 Open App \u2192 Mood Selection \u2192 Voice/Text Response \u2192 Summary 3. Journal Entry: Journal Tab \u2192 New Entry \u2192 Mood Selector \u2192 Voice Input \u2192 AI Suggestions \u2192 Save 4. Quick Conversation: Voice Button Hold \u2192 Speak \u2192 Response \u2192 Follow-up Options</p>"},{"location":"product_requirnmnts/#1142-key-screen-specifications","title":"11.4.2 Key Screen Specifications","text":"<p>Mac App - Main Chat Screen:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 [Avatar] Saira - Best Friend Mode          [Settings] [Close]   \u2502 60px\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  Hey! How are you feeling today? \ud83d\ude0a                    10:30 AM \u2502 \n\u2502                                                                 \u2502\n\u2502                               I'm a bit stressed about work \ud83d\ude14  \u2502\n\u2502                                                        10:31 AM \u2502\n\u2502                                                                 \u2502\n\u2502  I understand. Want to talk about what's                        \u2502\n\u2502  causing the stress? I'm here to listen \ud83d\udc99         10:31 AM     \u2502\n\u2502                                                                 \u2502\n\u2502                     [Mood: Anxious] [Related Memory: Work]      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [\ud83c\udfa4] Type a message...                              [Send]      \u2502 100px\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>iPhone App - Chat Interface:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u25c0 Saira    Mom Mode \u2502 60px\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                     \u2502\n\u2502 Good morning        \u2502\n\u2502 sweetie! How did    \u2502\n\u2502 you sleep? \ud83c\udf05       \u2502\n\u2502           9:15 AM   \u2502\n\u2502                     \u2502\n\u2502       Pretty well!  \u2502\n\u2502       Thanks mom \ud83d\udc95 \u2502\n\u2502           9:16 AM   \u2502\n\u2502                     \u2502\n\u2502 That makes me so    \u2502\n\u2502 happy to hear! \ud83d\ude0a   \u2502\n\u2502           9:16 AM   \u2502\n\u2502                     \u2502\n\u2502                     \u2502\n\u2502                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      [\ud83c\udfa4]           \u2502 80px\n\u2502   Hold to speak     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>iPhone App - Journal Entry:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u25c0 New Journal Entry \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 How are you feeling?\u2502\n\u2502 \ud83d\ude0a \ud83d\ude14 \ud83d\ude20 \ud83d\ude30 \ud83d\ude34        \u2502\n\u2502                     \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Today I had a   \u2502 \u2502\n\u2502 \u2502 really great    \u2502 \u2502\n\u2502 \u2502 meeting with... \u2502 \u2502\n\u2502 \u2502                 \u2502 \u2502\n\u2502 \u2502 [\ud83c\udfa4 Voice input]\u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                     \u2502\n\u2502 AI Suggestions:     \u2502\n\u2502 \u2022 Reflect on what   \u2502\n\u2502   made it great     \u2502\n\u2502 \u2022 Set goals for     \u2502\n\u2502   tomorrow          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [Cancel]    [Save]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"product_requirnmnts/#1143-component-library-for-wireframing","title":"11.4.3 Component Library for Wireframing","text":"<p>Standard Components: - Navigation bars with specific heights and elements - Button styles with exact dimensions and states - Input fields with validation states - Card layouts with spacing specifications - Avatar components with expression variants - Loading states and animations - Modal dialogs and overlays</p> <p>Custom Components: - Personality mode selector - Emotion indicator badges - Voice waveform visualizer - Memory relationship graphs - Mood timeline charts - Conversation bubble variants</p>"},{"location":"product_requirnmnts/#1144-design-system-export","title":"11.4.4 Design System Export","text":"<p>For AI Wireframing Tools: - Component dimensions in pixels - Color hex codes for all themes - Typography scale (font sizes, weights) - Spacing system (8px grid) - Border radius values - Shadow specifications - Animation timing curves - Icon library requirements</p> <p>Document Approval: - [ ] Product Manager - [ ] Engineering Lead - [ ] Design Lead - [ ] Privacy Officer - [ ] Business Stakeholder</p> <p>This document will be updated regularly as the product evolves through development phases.</p>"},{"location":"tech-stack/","title":"Tech Stack &amp; Architecture Documentation","text":"<p>Project: Saira \u2013 The Emotion-Aware Life Companion AI Date: 2025-06-09</p>"},{"location":"tech-stack/#1-introduction","title":"1. Introduction","text":"<p>Saira is a privacy-focused, emotion-aware AI assistant designed to run fully offline on macOS initially, with future plans for iOS and a custom smart speaker integration. The goal is to deliver an intelligent, conversational experience that respects user privacy by performing all processing locally, without sending data to the cloud.</p> <p>This document outlines the chosen technology stack, the reasons behind these choices, and how they fit together to form the foundation of Saira.</p>"},{"location":"tech-stack/#2-project-goals-constraints","title":"2. Project Goals &amp; Constraints","text":"<ul> <li>Fully offline operation: No reliance on cloud APIs or external servers. All AI models and data processing run locally.  </li> <li>Cross-platform readiness: Primary focus on macOS, with code reuse and architecture considerations for future iOS and custom hardware support.  </li> <li>Privacy-first: Sensitive user data never leaves the device; all storage and inference are local.  </li> <li>Performance: Efficient use of system resources, leveraging Apple Silicon optimizations where possible.  </li> <li>Developer productivity: Leverage existing expertise (React, Node.js) to accelerate development.  </li> <li>Extensibility: Modular design to allow future integration of new AI models and features.</li> </ul>"},{"location":"tech-stack/#3-technology-stack-overview","title":"3. Technology Stack Overview","text":"Component Chosen Technology Justification / Benefits Frontend UI React Native + TypeScript Cross-platform, code reuse, developer familiarity Language Model Mistral 7B (Q4_0) via llama-node Local LLM, open-weight, efficient inference Speech-to-Text Whisper.cpp (base.en) Accurate, optimized for Apple Silicon, fully offline Text-to-Speech Piper (RHVoice model) Lightweight, real-time TTS, suitable for offline use Audio I/O node-portaudio + speaker Stable audio input/output, cross-platform support Local Storage SQLite Lightweight, embedded DB, fully offline capable"},{"location":"tech-stack/#4-detailed-component-rationale","title":"4. Detailed Component Rationale","text":""},{"location":"tech-stack/#41-frontend-ui-react-native-typescript","title":"4.1 Frontend UI: React Native + TypeScript","text":"<ul> <li> <p>Why React Native?   React Native allows us to write one codebase that works across macOS and iOS, providing native UI performance and access to system APIs. It is mature enough for desktop support via community macOS extensions and aligns with our goal for multi-platform deployment.  </p> </li> <li> <p>Why TypeScript?   TypeScript enhances code quality through static typing, reduces runtime bugs, and improves developer experience, particularly on a complex app involving voice, AI, and local storage.</p> </li> <li> <p>Developer Expertise:   The team is already proficient in React and Node.js, making React Native a natural choice, reducing ramp-up time and facilitating rapid iteration.</p> </li> </ul>"},{"location":"tech-stack/#42-language-model-mistral-7b-q4_0-via-llama-node","title":"4.2 Language Model: Mistral 7B (Q4_0) via llama-node","text":"<ul> <li> <p>Why Mistral 7B?   Mistral 7B is a compact, efficient large language model with excellent performance relative to its size. The Q4_0 quantization format further reduces memory footprint and inference latency.  </p> </li> <li> <p>Why llama-node?   This library enables local inference of LLaMA-based models using optimized runtimes. It supports Apple Silicon hardware acceleration and works entirely offline.</p> </li> <li> <p>Privacy &amp; Performance:   Running locally ensures no data leakage, and the quantized model enables real-time interactions on consumer hardware.</p> </li> </ul>"},{"location":"tech-stack/#43-speech-to-text-whispercpp-baseen","title":"4.3 Speech-to-Text: Whisper.cpp (base.en)","text":"<ul> <li> <p>Why Whisper.cpp?   Whisper.cpp is a C++ reimplementation of OpenAI\u2019s Whisper model optimized for offline use on edge devices. The base English model provides good accuracy while maintaining real-time transcription speed.</p> </li> <li> <p>Apple Silicon Optimization:   The codebase includes optimizations specific to Apple\u2019s hardware, ensuring smooth, low-latency transcription.</p> </li> <li> <p>Offline Capability:   Fully runs on-device with no internet connection required.</p> </li> </ul>"},{"location":"tech-stack/#44-text-to-speech-piper-rhvoice-model","title":"4.4 Text-to-Speech: Piper (RHVoice model)","text":"<ul> <li> <p>Why Piper?   Piper offers efficient, real-time text-to-speech synthesis suitable for embedded and offline applications.  </p> </li> <li> <p>Model Choice:   The RHVoice model is chosen for its naturalness and light resource consumption.</p> </li> <li> <p>Real-Time Interaction:   Enables quick, conversational voice responses without noticeable delay.</p> </li> </ul>"},{"location":"tech-stack/#45-audio-inputoutput-node-portaudio-speaker","title":"4.5 Audio Input/Output: node-portaudio + speaker","text":"<ul> <li> <p>Why node-portaudio?   A stable, well-maintained Node.js binding for PortAudio providing cross-platform access to audio I/O devices.  </p> </li> <li> <p>Use Case:   Essential for capturing microphone input and playing back synthesized speech reliably.</p> </li> <li> <p>Cross-Platform Compatibility:   Facilitates future extension to other platforms beyond macOS.</p> </li> </ul>"},{"location":"tech-stack/#46-local-data-storage-sqlite","title":"4.6 Local Data Storage: SQLite","text":"<ul> <li> <p>Why SQLite?   SQLite is an embedded, zero-configuration relational database ideal for local data persistence.  </p> </li> <li> <p>Use Case:   Storing user preferences, conversation logs, calendar events, and other data locally to maintain privacy and ensure offline availability.</p> </li> <li> <p>Lightweight &amp; Reliable:   Requires minimal dependencies and is well-suited for desktop and mobile environments.</p> </li> </ul>"},{"location":"tech-stack/#5-architecture-data-flow-summary","title":"5. Architecture &amp; Data Flow (Summary)","text":"<ol> <li>User Interaction: Voice or text input is received by the React Native UI.  </li> <li>Speech Recognition: Audio is streamed to Whisper.cpp for transcription (if voice input).  </li> <li>Natural Language Processing: The transcribed or typed input is sent to Mistral 7B running locally via llama-node.  </li> <li>Response Generation: The LLM generates a reply based on context and memory.  </li> <li>Text-to-Speech: Piper converts the reply text to audio output.  </li> <li>Audio Playback: node-portaudio plays the synthesized voice.  </li> <li>Data Storage: All interactions and relevant metadata are stored in SQLite for persistence and context retention.</li> </ol>"},{"location":"tech-stack/#6-future-considerations","title":"6. Future Considerations","text":"<ul> <li>iOS Integration: React Native codebase can be extended to iOS with platform-specific UI adjustments and API calls.  </li> <li>Smart Speaker: The modular AI and audio pipeline allow integration into custom hardware with similar system libraries.  </li> <li>Syncing: Secure cloud syncing of user data to enable multi-device experience, preserving privacy through end-to-end encryption (planned for later phases).  </li> <li>Model Updates: Support for dynamically updating AI models while keeping offline functionality intact.</li> </ul>"},{"location":"tech-stack/#7-summary","title":"7. Summary","text":"<p>This tech stack balances modern cross-platform development with privacy and offline-first principles. By leveraging open-source AI models optimized for local execution and a React Native frontend, we achieve both developer productivity and user-centric design goals. The architecture provides a strong foundation for future expansion while respecting user privacy and delivering a natural, responsive conversational AI experience.</p>"},{"location":"ui_implemenation/","title":"Ui implemenation","text":""},{"location":"ui_implemenation/#feature-specifications-ui-focused","title":"Feature Specifications (UI-Focused)","text":""},{"location":"ui_implemenation/#feature-onboarding-flow","title":"Feature: Onboarding Flow","text":"<p>Feature Goal (UI-specific) To guide new users through an intuitive, multi-step setup process, collecting essential preferences and obtaining necessary consents to personalize their Saira experience from the first interaction.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>UserManager</code> (Frontend Service): Calls <code>createUser</code> with collected preferences and consent data.</li> <li><code>PersonalityManager</code> (Frontend Service): Fetches pre-defined personality profiles for selection.</li> <li><code>DataStoreService</code> (Frontend Service): Persists initial user data locally.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Welcome Screen (<code>OnboardingScreen.tsx</code> - Step 1):</p> <ul> <li>Visuals: Full-screen splash with Saira logo, a welcoming tagline (e.g., \"Your Emotion-Aware Life Companion\"), and a brief, compelling description of Saira's purpose.</li> <li>Elements:<ul> <li>Large, centered Saira logo/icon.</li> <li>Headline text: \"Welcome to Saira.\"</li> <li>Body text: \"An AI that understands your emotions, grows with you, and helps you manage your mental, emotional, and personal life through intelligent, natural conversations.\"</li> <li>Primary Call-to-Action (CTA) button: \"Get Started\" (or \"Begin Your Journey\").</li> </ul> </li> <li>Interactions: Tapping \"Get Started\" navigates to the Consent Screen.</li> <li>States: Initial load state.</li> </ul> </li> <li> <p>Consent Screen (<code>OnboardingScreen.tsx</code> - Step 2):</p> <ul> <li>Visuals: A clear, concise screen presenting privacy commitments and requiring explicit user consent for sensitive data processing.</li> <li>Elements:<ul> <li>Headline: \"Your Privacy, Our Priority.\"</li> <li>Body text explaining Saira's privacy-first approach (local processing, E2EE sync).</li> <li>Checkbox: \"I agree to Saira's Privacy Policy and Terms of Service.\" (Link to full policy).</li> <li>Checkbox: \"Allow Saira to record and analyze my emotional tone from voice for personalized support.\" (Optional, with clear explanation).</li> <li>Checkbox: \"Allow Saira to record and store memories from our conversations to build long-term context.\" (Optional, with clear explanation).</li> <li>Primary CTA button: \"Continue\" (disabled until mandatory consents are checked).</li> <li>Secondary button: \"Learn More About Privacy\" (opens external link or in-app modal).</li> </ul> </li> <li>Interactions:<ul> <li>Tapping checkboxes toggles their state.</li> <li>\"Continue\" button enables only when \"Privacy Policy\" checkbox is checked.</li> <li>Tapping \"Continue\" navigates to Personality Selection.</li> </ul> </li> <li>States: Button disabled/enabled states.</li> </ul> </li> <li> <p>Personality Selection Screen (<code>OnboardingScreen.tsx</code> - Step 3):</p> <ul> <li>Visuals: A grid or carousel of pre-defined personality profiles with brief descriptions.</li> <li>Elements:<ul> <li>Headline: \"Choose Saira's Initial Personality.\"</li> <li>Description: \"You can change this anytime in settings.\"</li> <li>List/Grid of <code>PersonalityProfileCard</code> components:<ul> <li>Each card displays: Personality Name (e.g., \"The Coach,\" \"The Best Friend,\" \"The Mom\").</li> <li>Short description of personality traits and interaction style.</li> <li>Visual indicator for selection (e.g., border, checkmark).</li> </ul> </li> <li>Primary CTA button: \"Next\".</li> </ul> </li> <li>Interactions:<ul> <li>Tapping a <code>PersonalityProfileCard</code> selects it. Only one can be active.</li> <li>\"Next\" button navigates to Goal Setting.</li> </ul> </li> <li>States: Selected/unselected card states.</li> </ul> </li> <li> <p>Goal Setting Screen (<code>OnboardingScreen.tsx</code> - Step 4):</p> <ul> <li>Visuals: A screen for users to define initial personal goals.</li> <li>Elements:<ul> <li>Headline: \"What are your initial goals with Saira?\"</li> <li>Description: \"Saira can help you track and achieve personal goals. You can add more later.\"</li> <li><code>TextInput</code> field: \"Add a new goal...\" (e.g., \"Improve sleep,\" \"Reduce stress,\" \"Learn a new skill\").</li> <li>Button: \"Add Goal\" (adds text from input to a list).</li> <li><code>FlatList</code> or <code>ScrollView</code> displaying added goals:<ul> <li>Each goal item: Text of the goal, small \"x\" button to remove.</li> </ul> </li> <li>Primary CTA button: \"Next\".</li> <li>Secondary button: \"Skip for now\".</li> </ul> </li> <li>Interactions:<ul> <li>Typing in <code>TextInput</code> updates its value.</li> <li>Tapping \"Add Goal\" adds the goal to the list.</li> <li>Tapping \"x\" removes a goal.</li> <li>\"Next\" navigates to Tone Style Preference.</li> <li>\"Skip for now\" navigates to Tone Style Preference.</li> </ul> </li> <li>States: Empty input, populated list.</li> </ul> </li> <li> <p>Tone Style Preference Screen (<code>OnboardingScreen.tsx</code> - Step 5):</p> <ul> <li>Visuals: A screen for users to select Saira's preferred response tone.</li> <li>Elements:<ul> <li>Headline: \"How should Saira speak to you?\"</li> <li>Description: \"Choose a general tone style for Saira's responses.\"</li> <li>Radio buttons or segmented control for tone options: \"Empathetic,\" \"Direct,\" \"Neutral,\" \"Playful.\"</li> <li>Short example text for each tone option.</li> <li>Primary CTA button: \"Finish Setup\".</li> </ul> </li> <li>Interactions: Tapping a radio button selects the tone. \"Finish Setup\" button navigates to the Main Application.</li> <li>States: Selected/unselected tone options.</li> </ul> </li> <li> <p>Onboarding Complete Screen (<code>OnboardingScreen.tsx</code> - Final Step):</p> <ul> <li>Visuals: A celebratory screen confirming setup completion.</li> <li>Elements:<ul> <li>Large success icon/animation.</li> <li>Headline: \"You're All Set!\"</li> <li>Body text: \"Saira is ready to begin your journey together.\"</li> <li>Primary CTA button: \"Start Talking to Saira\".</li> </ul> </li> <li>Interactions: Tapping \"Start Talking to Saira\" dismisses the onboarding flow and navigates to the <code>ConversationScreen</code>.</li> <li>States: Success animation.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>OnboardingScreen.tsx</code> will be a single React Native component that manages the state for which step of the onboarding the user is on. It will conditionally render sub-components for each step (e.g., <code>WelcomeStep</code>, <code>ConsentStep</code>, <code>PersonalityStep</code>, <code>GoalStep</code>, <code>ToneStep</code>, <code>CompletionStep</code>).</li> <li>Reusable Component Library: Utilize <code>Button</code>, <code>TextInput</code>, <code>Checkbox</code>, <code>RadioGroup</code>, <code>Card</code> components from a shared library.</li> <li>State Management:<ul> <li><code>useState</code> hook within <code>OnboardingScreen.tsx</code> to manage the <code>currentStep</code> (integer).</li> <li><code>useState</code> for form inputs (e.g., <code>userName</code>, <code>selectedPersonalityId</code>, <code>goalsList</code>, <code>selectedTone</code>).</li> <li><code>useState</code> for consent checkbox states.</li> <li><code>useEffect</code> to enable/disable \"Continue\" buttons based on validation.</li> <li>Global state (e.g., <code>Zustand</code> store) to persist onboarding data temporarily before final submission.</li> </ul> </li> <li>Routing:<ul> <li><code>AppNavigator.tsx</code> will check an <code>onboarding_completed</code> flag (from <code>UserManager</code>) on app launch. If <code>false</code>, it navigates to <code>OnboardingScreen</code>.</li> <li>Internal navigation within <code>OnboardingScreen</code> will update <code>currentStep</code> state.</li> <li>Upon \"Finish Setup,\" <code>AppNavigator</code> will replace the onboarding stack with the <code>MainTabView</code> stack.</li> </ul> </li> <li>Responsive Design: Use <code>Flexbox</code> for layout. Ensure content scrolls on smaller window sizes if necessary. Text sizes should scale appropriately.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Create User: The entire onboarding flow is a \"Create\" operation for the <code>User</code> entity. Data is collected across multiple screens and submitted once on \"Finish Setup.\"<ul> <li>Validation: Frontend validation for required fields (e.g., name not empty, consent checked).</li> <li>Flow:<ol> <li>User inputs data in each step.</li> <li>On \"Finish Setup,\" <code>OnboardingScreen</code> collects all <code>useState</code> data.</li> <li>Calls <code>UserManager.createUser(collectedData)</code>.</li> <li><code>UserManager</code> sends this data via <code>IPCService</code> to the Node.js <code>data_manager.ts</code> to insert into the <code>Users</code> table.</li> <li><code>UserManager</code> updates the <code>onboarding_completed</code> flag in local storage.</li> </ol> </li> </ul> </li> <li>Read Personality Profiles: <code>PersonalitySelectionScreen</code> performs a \"Read\" operation to fetch available <code>PersonalityProfiles</code> from <code>PersonalityManager</code>.<ul> <li>Flow: <code>PersonalitySelectionScreen</code> <code>useEffect</code> calls <code>PersonalityManager.getPersonalityProfiles()</code> -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>PersonalityProfiles</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Visual Progress: A simple progress indicator (e.g., \"Step X of Y\" or a progress bar) at the top of each onboarding screen.</li> <li>Clear Instructions: Concise headings and body text to guide the user.</li> <li>Feedback: Immediate visual feedback on button presses (e.g., loading spinner on \"Finish Setup\").</li> <li>Error Handling: If <code>UserManager.createUser</code> fails (e.g., database error), display a user-friendly error message (e.g., \"Setup failed. Please try again.\") and allow retries.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Consent Display: Clearly present privacy policy and consent checkboxes. Ensure they are prominent and require explicit interaction.</li> <li>Input Masking: If any sensitive input (e.g., future password for sync) is added, ensure it's masked.</li> <li>No Sensitive Data in UI State: Avoid storing sensitive user data directly in easily inspectable UI state. Pass it to backend services immediately.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test individual <code>OnboardingScreen</code> sub-components for rendering, state updates, and button interactions.</li> <li>Integration Tests: Test the full multi-step flow, ensuring data is correctly collected and passed between steps. Mock <code>UserManager</code> calls.</li> <li>End-to-End Tests: Simulate a full user onboarding from app launch to <code>MainTabView</code> display, verifying all UI elements and interactions.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Frontend Validation Errors: Display inline error messages for invalid inputs (e.g., \"Name cannot be empty\").</li> <li>Backend Errors: If <code>createUser</code> API call fails, display a generic error message (e.g., \"Failed to save preferences. Please try again.\") in a toast or alert. Log detailed error to <code>Logger.ts</code>.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-main-application-navigation","title":"Feature: Main Application Navigation","text":"<p>Feature Goal (UI-specific) To provide a clear, intuitive, and persistent navigation structure that allows users to easily switch between the primary sections of the Saira application.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>AppStateManager</code> (Frontend Service): May update global state based on active tab (e.g., to pause/resume background processes).</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Tab Bar Display:</p> <ul> <li>Visuals: A persistent tab bar at the bottom (or left sidebar for macOS desktop feel) of the main application window.</li> <li>Elements:<ul> <li>Icons and text labels for each primary section: \"Conversation,\" \"Journal,\" \"Deep Talk,\" \"Profile.\"</li> <li>Visual indication of the currently active tab.</li> </ul> </li> <li>Interactions: Tapping a tab icon/label switches to the corresponding screen.</li> <li>States: Active/inactive tab states.</li> </ul> </li> <li> <p>Screen Switching:</p> <ul> <li>Visuals: Smooth transitions between screens when switching tabs.</li> <li>Behavior: Each tab maintains its own navigation stack (e.g., navigating deep within \"Journal\" and then switching to \"Conversation\" and back, returns to the deep state in \"Journal\").</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>App.tsx</code> (root) renders <code>AppNavigator.tsx</code>. <code>AppNavigator.tsx</code> uses <code>React Navigation</code>'s <code>createBottomTabNavigator</code> (for mobile) or <code>createMaterialTopTabNavigator</code> (can be styled for desktop sidebar) to define the main tab structure. Each tab will render its own root screen component (e.g., <code>ConversationScreen</code>, <code>JournalingScreen</code>).</li> <li>Reusable Component Library: Standard React Native <code>Tab.Navigator</code> and <code>Tab.Screen</code> components. Custom icons for tabs.</li> <li>State Management: <code>React Navigation</code> handles internal state for active tab and navigation stacks.</li> <li>Routing: <code>React Navigation</code>'s declarative API defines the tab routes.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>This feature is purely navigational and does not directly involve CRUD operations on data entities. It facilitates access to other features that perform CRUD.</li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Immediate Feedback: Tapping a tab should instantly switch the view.</li> <li>Consistency: Tab bar remains visible and in the same position across all main screens.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>No direct security implications for the navigation component itself. Access to sensitive tabs (e.g., Profile Settings) is implicitly protected by the overall app's local authentication.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>AppNavigator</code> component for correct tab rendering and initial route.</li> <li>Integration Tests: Verify that tapping tabs correctly switches screens and that navigation stacks are maintained.</li> <li>End-to-End Tests: Simulate user navigating between all main tabs.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>No specific error handling for navigation itself, beyond standard React Native error boundaries.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-conversation-screen","title":"Feature: Conversation Screen","text":"<p>Feature Goal (UI-specific) To provide a dynamic and intuitive chat interface for real-time voice and text interaction with Saira, displaying conversational history, real-time transcription, and AI responses.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>IPCService</code> (Frontend Service): Sends audio chunks, text input, and receives transcription updates, emotion data, LLM text, and TTS audio.</li> <li><code>MessageManager</code> (Frontend Service): Adds new messages (user and AI) to the local database.</li> <li><code>ConversationManager</code> (Frontend Service): Manages the current conversation session (e.g., starting a new one, updating title).</li> <li><code>MemoryManager</code> (Frontend Service): Triggers memory formation based on conversation turns.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Chat History Display:</p> <ul> <li>Visuals: A scrollable list of chat bubbles, visually distinguishing between user messages and AI responses.</li> <li>Elements:<ul> <li><code>FlatList</code> or <code>ScrollView</code> to display <code>MessageBubble</code> components.</li> <li>Each <code>MessageBubble</code> displays text content.</li> <li>User messages aligned right, AI messages aligned left.</li> <li>Timestamps for messages (optional, on hover or grouped).</li> </ul> </li> <li>Interactions: Auto-scroll to bottom on new message. User can manually scroll up to view history.</li> <li>States: Empty state (e.g., \"Say 'Hey Saira' to begin!\"), loading state for initial history.</li> </ul> </li> <li> <p>Voice Input Indicator:</p> <ul> <li>Visuals: A prominent, dynamic indicator (e.g., a pulsing microphone icon, a waveform visualization) that changes based on the AI's listening/processing state.</li> <li>Elements:<ul> <li>Microphone icon.</li> <li>Optional: Simple waveform animation or pulsating circle.</li> <li>Text status: \"Listening...\", \"Thinking...\", \"Speaking...\", \"Idle.\"</li> </ul> </li> <li>Interactions: Tapping the microphone icon can manually toggle listening (if not using wake word).</li> <li>States: Idle, Listening, Processing (AI thinking), Speaking (AI responding).</li> </ul> </li> <li> <p>Real-time Transcription Display:</p> <ul> <li>Visuals: Text area above the input bar that updates as the user speaks.</li> <li>Elements:<ul> <li><code>Text</code> component displaying the current partial or final transcription.</li> <li>Text color/style might change to indicate finality.</li> </ul> </li> <li>Interactions: Updates dynamically.</li> <li>States: Empty, partial transcription, final transcription.</li> </ul> </li> <li> <p>Input Bar:</p> <ul> <li>Visuals: A persistent input area at the bottom of the screen.</li> <li>Elements:<ul> <li><code>TextInput</code> for typing messages.</li> <li>Microphone button (to initiate voice input if wake word is off, or to manually stop/start recording).</li> <li>Send button (for text input).</li> </ul> </li> <li>Interactions:<ul> <li>Typing in <code>TextInput</code> updates its value.</li> <li>Tapping microphone button toggles voice input.</li> <li>Tapping send button sends text message.</li> </ul> </li> <li>States: Empty, text entered, sending.</li> </ul> </li> <li> <p>AI Response Playback:</p> <ul> <li>Behavior: Audio plays automatically as AI response text appears.</li> <li>Visuals: No explicit UI for playback, but the <code>VoiceInputIndicator</code> should show \"Speaking...\" state.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>ConversationScreen.tsx</code> will be the main screen. It will contain <code>MessageList</code> (a <code>FlatList</code> of <code>MessageBubble</code> components), <code>VoiceInputIndicator</code>, and <code>InputBar</code>.</li> <li>Reusable Component Library: <code>MessageBubble</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code> components.</li> <li>State Management:<ul> <li><code>useState</code> for <code>currentTranscription</code>, <code>aiResponseText</code>, <code>inputMessageText</code>.</li> <li><code>useState</code> for <code>conversationState</code> (e.g., 'idle', 'listening', 'processing', 'speaking').</li> <li><code>useState</code> for <code>messages</code> array (local display cache).</li> <li><code>useEffect</code> to manage WebSocket connection to <code>ai_inference_server.ts</code>.</li> <li><code>useEffect</code> to auto-scroll <code>FlatList</code> when new messages arrive.</li> <li>Global state (e.g., <code>Zustand</code> store) for <code>currentConversationId</code>, <code>activePersonalityId</code>.</li> </ul> </li> <li>Routing: <code>ConversationScreen</code> is the default tab in <code>MainTabView</code>.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Read Messages: <code>ConversationScreen</code> performs a \"Read\" operation to fetch initial conversation history.<ul> <li>Flow: On component mount, <code>ConversationScreen</code> calls <code>MessageManager.getMessages(currentConversationId, pagination)</code> -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>Messages</code> table.</li> </ul> </li> <li>Create Message (User):<ul> <li>Flow (Voice): <code>audio_processor.ts</code> streams audio to <code>ai_inference_server.ts</code> -&gt; <code>ai_inference_server.ts</code> sends <code>transcription_update</code> via WebSocket -&gt; <code>ConversationScreen</code> updates <code>currentTranscription</code>. When <code>isFinal</code> is true, <code>MessageManager.addMessage</code> is called.</li> <li>Flow (Text): User types and presses send -&gt; <code>MessageManager.addMessage</code> is called.</li> <li><code>MessageManager</code> sends message data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>Messages</code> table.</li> </ul> </li> <li>Create Message (AI):<ul> <li>Flow: <code>ai_inference_server.ts</code> sends <code>ai_response_text</code> via WebSocket -&gt; <code>ConversationScreen</code> updates UI. Once full response received, <code>MessageManager.addMessage</code> is called to store AI's message.</li> <li><code>MessageManager</code> sends message data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>Messages</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Real-time Feedback: Visual and textual indicators for AI's state (listening, thinking, speaking).</li> <li>Smooth Scrolling: Ensure <code>FlatList</code> performs well with many messages.</li> <li>Typing vs. Voice: Seamless switch between typing and voice input modes.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Microphone Permission: React Native will trigger the macOS system prompt for microphone access. The UI should guide the user if permission is denied.</li> <li>No Raw Audio Display: Raw audio data is processed by the backend and never displayed in the UI.</li> <li>Consent Adherence: If emotion/memory recording consent is denied, the UI should reflect this (e.g., disable related indicators or features).</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>MessageBubble</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code> components for rendering and basic interactions.</li> <li>Integration Tests: Test WebSocket connection, real-time transcription updates, and message display. Mock backend responses.</li> <li>End-to-End Tests: Simulate full voice conversations, verifying UI updates, message persistence, and AI response playback.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Connection Errors: If WebSocket connection to backend fails, display a prominent error message (e.g., \"Saira's brain is offline. Please restart the app.\") and disable voice input.</li> <li>Transcription Errors: If ASR fails, display \"Could not understand. Please try again.\"</li> <li>LLM Errors: If LLM fails to generate a response, display \"Saira is having trouble thinking. Please try again.\"</li> <li>Audio Playback Errors: Log errors if TTS audio fails to play.</li> <li>Logging: Detailed logs of audio stream events, AI processing steps, and message storage.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-daily-check-ins-journaling-screen","title":"Feature: Daily Check-ins &amp; Journaling Screen","text":"<p>Feature Goal (UI-specific) To provide a dedicated space for users to reflect on their emotional state, log daily moods, and create personal journal entries, with visual summaries of emotional patterns over time.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>JournalManager</code> (Frontend Service): Creates, reads, updates, and deletes journal entries and mood summaries.</li> <li><code>MessageManager</code> (Frontend Service): May retrieve emotion data from past messages for mood summaries.</li> <li><code>MemoryManager</code> (Frontend Service): Journal entries can trigger memory formation.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Daily Mood Summary (<code>JournalingScreen.tsx</code> - Main View):</p> <ul> <li>Visuals: A calendar-like view or a series of daily cards displaying aggregated mood data.</li> <li>Elements:<ul> <li>Date navigation (e.g., left/right arrows to change day/week/month).</li> <li>For each day:<ul> <li>Date display.</li> <li>Aggregated mood representation (e.g., a dominant emoji, a color gradient, or a simple text summary like \"Mostly Neutral\").</li> <li>Count of journal entries for that day.</li> <li>Link/button to view/add journal entry for that day.</li> </ul> </li> </ul> </li> <li>Interactions: Tapping a day navigates to its journal entry list/detail.</li> <li>States: Loading state, empty state for days with no data.</li> </ul> </li> <li> <p>Emotional Check-in Interface (<code>JournalingScreen.tsx</code> - Button/Modal):</p> <ul> <li>Visuals: A prominent button on the main journaling screen, or a modal that appears when triggered.</li> <li>Elements:<ul> <li>Button: \"How are you feeling today?\"</li> <li>Modal/Screen:<ul> <li>Headline: \"Log Your Mood.\"</li> <li>Mood picker: A visual slider for Arousal (Calm to Excited) and Valence (Negative to Positive), or a selection of emotion emojis/categories (e.g., Happy, Sad, Angry, Neutral).</li> <li>Optional <code>TextInput</code>: \"Add a note about your mood...\"</li> <li>Button: \"Save Mood\".</li> </ul> </li> </ul> </li> <li>Interactions: Tapping the button opens the modal. Interacting with sliders/pickers updates mood selection. Tapping \"Save Mood\" submits the data.</li> <li>States: Default, selected mood, saving.</li> </ul> </li> <li> <p>Journal Entry List (<code>JournalingScreen.tsx</code> - Sub-section/Screen):</p> <ul> <li>Visuals: A scrollable list of journal entries, ordered by date.</li> <li>Elements:<ul> <li><code>FlatList</code> displaying <code>JournalEntryCard</code> components.</li> <li>Each <code>JournalEntryCard</code> displays: Date, a snippet of the entry content, and a small mood indicator (if available).</li> <li>Button: \"New Journal Entry\" (floating action button or prominent header button).</li> </ul> </li> <li>Interactions: Tapping a <code>JournalEntryCard</code> navigates to <code>JournalEntryDetailView</code>. Tapping \"New Journal Entry\" navigates to <code>JournalEntryCreationView</code>.</li> <li>States: Loading, empty list.</li> </ul> </li> <li> <p>Journal Entry Creation/Detail (<code>JournalEntryCreationView.tsx</code>/<code>JournalEntryDetailView.tsx</code>):</p> <ul> <li>Visuals: A dedicated screen for writing/editing a journal entry.</li> <li>Elements:<ul> <li>Date/Time display (auto-filled, editable).</li> <li>Large <code>TextInput</code> for journal content.</li> <li>Optional: Mood picker (similar to check-in) if not already set.</li> <li>Button: \"Save Entry\".</li> <li>For detail view: \"Edit\" button, \"Delete\" button.</li> </ul> </li> <li>Interactions: Typing updates content. Tapping \"Save Entry\" persists. \"Edit\" makes content editable. \"Delete\" prompts confirmation then deletes.</li> <li>States: Editing, viewing, saving, deleting.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>JournalingScreen.tsx</code> will be the main tab screen. It will contain <code>MoodSummaryCalendar</code> and <code>JournalEntryList</code> components. <code>JournalEntryCreationView.tsx</code> and <code>JournalEntryDetailView.tsx</code> will be separate screens navigated to from <code>JournalingScreen</code>.</li> <li>Reusable Component Library: <code>MoodSummaryCard</code>, <code>JournalEntryCard</code>, <code>DatePicker</code>, <code>Slider</code>, <code>EmojiPicker</code> (custom).</li> <li>State Management:<ul> <li><code>useState</code> for <code>selectedDate</code>, <code>journalEntries</code> array, <code>moodSummaries</code> array.</li> <li><code>useState</code> for <code>currentJournalContent</code>, <code>selectedMood</code>.</li> <li><code>useEffect</code> to fetch data from <code>JournalManager</code> based on <code>selectedDate</code> or filters.</li> <li>Global state for <code>userId</code>.</li> </ul> </li> <li>Routing: <code>JournalingScreen</code> is a tab in <code>MainTabView</code>. <code>JournalEntryCreationView</code> and <code>JournalEntryDetailView</code> are pushed onto the <code>JournalingScreen</code>'s navigation stack.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Create Journal Entry:<ul> <li>Flow: User types in <code>JournalEntryCreationView</code> -&gt; taps \"Save Entry\" -&gt; <code>JournalManager.createJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>JournalEntries</code> table.</li> <li>Also triggers <code>JournalManager.generateMoodSummary</code> for the relevant date.</li> </ul> </li> <li>Read Journal Entries:<ul> <li>Flow: <code>JournalingScreen</code> or <code>JournalEntryList</code> mounts -&gt; <code>JournalManager.getJournalEntries</code> is called (with date range/pagination) -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>JournalEntries</code>.</li> <li><code>JournalManager.getMoodSummaries</code> is called for the calendar view.</li> </ul> </li> <li>Update Journal Entry:<ul> <li>Flow: User edits in <code>JournalEntryDetailView</code> -&gt; taps \"Save\" -&gt; <code>JournalManager.updateJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to update <code>JournalEntries</code> table.</li> </ul> </li> <li>Delete Journal Entry:<ul> <li>Flow: User taps \"Delete\" in <code>JournalEntryDetailView</code> -&gt; confirmation dialog -&gt; <code>JournalManager.deleteJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to delete from <code>JournalEntries</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Intuitive Logging: Easy access to mood check-in and new journal entry creation.</li> <li>Visual Summaries: Mood calendar provides a quick overview of emotional trends.</li> <li>Seamless Editing: Smooth transition from viewing to editing journal entries.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Data Privacy: Remind users about the privacy of their journal entries.</li> <li>Consent Adherence: Ensure emotion data is only recorded/displayed if consent was given.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>MoodSummaryCard</code>, <code>JournalEntryCard</code>, <code>MoodPicker</code> components.</li> <li>Integration Tests: Test the full flow of creating, viewing, editing, and deleting journal entries. Verify mood summary generation.</li> <li>End-to-End Tests: Simulate a user logging moods and creating multiple journal entries over several days.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Save Errors: If saving a journal entry fails, display \"Failed to save entry. Please try again.\"</li> <li>Load Errors: If journal entries fail to load, display \"Could not load journal entries.\"</li> <li>Logging: Log all CRUD operations on journal entries and mood summaries.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-deep-talk-screen","title":"Feature: Deep Talk Screen","text":"<p>Feature Goal (UI-specific) To provide a specialized conversational mode that encourages deeper exploration of complex personal topics, leveraging Saira's long-term memory (RAG) to provide more insightful and context-rich responses.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>IPCService</code> (Frontend Service): Sends audio/text input, receives AI responses, specifically optimized for RAG queries.</li> <li><code>MemoryManager</code> (Frontend Service): Triggers semantic search for relevant memories based on conversation context.</li> <li><code>ConversationManager</code> (Frontend Service): Manages the conversation session, potentially tagging it as a \"Deep Talk\" session.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Specialized Conversation Interface:</p> <ul> <li>Visuals: Similar to the <code>ConversationScreen</code>, but with subtle visual cues indicating \"Deep Talk Mode\" (e.g., a different background color, a unique icon in the header).</li> <li>Elements:<ul> <li>Chat history display (same as <code>ConversationScreen</code>).</li> <li>Voice input indicator (same as <code>ConversationScreen</code>).</li> <li>Real-time transcription display (same as <code>ConversationScreen</code>).</li> <li>Input bar (same as <code>ConversationScreen</code>).</li> <li>Optional: \"Suggested Topics\" or \"Memory Prompts\" area (e.g., a carousel of buttons with memory snippets or questions like \"Tell me more about X event\").</li> </ul> </li> <li>Interactions: Standard voice/text interaction. Tapping suggested topics injects them as a prompt.</li> <li>States: Active Deep Talk session.</li> </ul> </li> <li> <p>Memory Integration Indicators (Subtle):</p> <ul> <li>Visuals: When Saira's response is heavily influenced by a retrieved memory, a subtle visual indicator might appear (e.g., a small \"memory\" icon next to the AI's message, or a tooltip on hover revealing the source memory snippet). This is for transparency and to highlight Saira's RAG capability.</li> <li>Elements: Small icon, tooltip/popover.</li> <li>Interactions: Hovering/tapping the icon reveals the source memory.</li> <li>States: Memory-augmented response, non-augmented response.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>DeepTalkScreen.tsx</code> will largely reuse components from <code>ConversationScreen.tsx</code> (e.g., <code>MessageList</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code>). It will wrap these in a container that applies Deep Talk specific styling.</li> <li>Reusable Component Library: Existing chat components. New <code>MemoryHint</code> component.</li> <li>State Management:<ul> <li>Inherits <code>conversationState</code> and <code>messages</code> state from a shared context or uses its own.</li> <li><code>useState</code> for <code>suggestedTopics</code> (populated by AI or based on recent memories).</li> <li>Global state for <code>currentConversationId</code>.</li> </ul> </li> <li>Routing: <code>DeepTalkScreen</code> is a tab in <code>MainTabView</code>.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Read Memories (for RAG): This is the core \"Read\" operation.<ul> <li>Flow: When a user speaks/types in Deep Talk mode, the <code>ConversationScreen</code>'s ViewModel (or a dedicated <code>DeepTalkViewModel</code>) will:<ol> <li>Send the user's query to <code>ai_inference_server.ts</code>'s <code>/ai/embeddings/generate</code> to get an embedding.</li> <li>Send the embedding to <code>ai_inference_server.ts</code>'s <code>/data/search_vectors</code> to retrieve relevant <code>Memories</code> from <code>sqlite-vec</code>.</li> <li>These retrieved memories are then included in the prompt sent to the LLM.</li> </ol> </li> </ul> </li> <li>Create/Read Messages: Same as <code>ConversationScreen</code>.</li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Focused Interaction: The UI should encourage deeper, more reflective conversations.</li> <li>Transparency: Subtle indicators help the user understand when Saira is drawing from its memory.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Memory Visibility: Ensure that only memories marked <code>is_active</code> and allowed by <code>memory_visibility_rules</code> are retrieved and used for RAG. The UI should reflect these settings.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Integration Tests: Test the RAG pipeline: simulate a query, verify that relevant memories are retrieved from the mocked backend, and that the LLM response incorporates them.</li> <li>End-to-End Tests: Simulate a Deep Talk session where the user asks about a past event, and Saira responds with details from a previously ingested memory.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>RAG Failure: If memory retrieval or embedding generation fails, the LLM should still attempt to respond, but the UI might show a subtle warning that \"Saira couldn't access all its memories.\"</li> <li>Performance: Monitor RAG query latency. If too slow, provide feedback (e.g., \"Saira is thinking deeply...\").</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-profile-settings-screen","title":"Feature: Profile Settings Screen","text":"<p>Feature Goal (UI-specific) To provide a centralized hub for users to customize Saira's behavior, manage their data, configure privacy settings, and update AI models.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>UserManager</code> (Frontend Service): Reads/updates user preferences, privacy settings, and triggers data export/deletion.</li> <li><code>PersonalityManager</code> (Frontend Service): Reads/updates personality profiles.</li> <li><code>ModelManager</code> (Frontend Service): Reads model versions, triggers model downloads.</li> <li><code>SyncService</code> (Frontend Service): Configures sync, initiates sync, handles recovery keys.</li> <li><code>NudgeManager</code> (Frontend Service): Reads/updates nudge preferences.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Main Settings Navigation (<code>ProfileSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: A list of categories (e.g., \"General,\" \"Personality,\" \"Privacy &amp; Data,\" \"AI Models,\" \"Notifications,\" \"Sync\").</li> <li>Elements:<ul> <li><code>FlatList</code> of <code>SettingsCategoryItem</code> components.</li> <li>Each item: Icon, Category Name, disclosure indicator (\"&gt;\").</li> </ul> </li> <li>Interactions: Tapping a category navigates to its dedicated sub-screen.</li> <li>States: Loading, default.</li> </ul> </li> <li> <p>General Settings (<code>GeneralSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Basic user profile information.</li> <li>Elements:<ul> <li>User's Name (<code>TextInput</code>).</li> <li>Profile Picture (display, button to change).</li> <li>Preferred Tone Style (radio buttons/dropdown).</li> </ul> </li> <li>Interactions: Editing fields updates user data.</li> <li>States: Editing, saving.</li> </ul> </li> <li> <p>Personality Settings (<code>PersonalitySettingsScreen.tsx</code>):</p> <ul> <li>Visuals: List of available personality profiles, with the active one highlighted.</li> <li>Elements:<ul> <li>Headline: \"Saira's Personality.\"</li> <li><code>FlatList</code> of <code>PersonalityProfileCard</code> components:<ul> <li>Each card: Name, description, \"Activate\" button (if not active).</li> <li>Active card has a distinct visual style.</li> </ul> </li> <li>Button: \"Create Custom Personality\" (navigates to <code>CustomPersonalityCreationScreen</code>).</li> </ul> </li> <li>Interactions: Tapping \"Activate\" changes Saira's active personality.</li> <li>States: Loading, active/inactive profiles.</li> </ul> </li> <li> <p>Privacy &amp; Data Settings (<code>PrivacyDashboardScreen.tsx</code>):</p> <ul> <li>Visuals: A dashboard for managing data consents and privacy actions.</li> <li>Elements:<ul> <li>Toggle: \"Allow emotion recording from voice.\"</li> <li>Toggle: \"Allow memory recording from conversations.\"</li> <li>Button: \"View Audit Logs\" (opens <code>AuditLogsScreen</code>).</li> <li>Button: \"Export All My Data\" (triggers data export).</li> <li>Button: \"Delete All My Data\" (triggers data deletion, with confirmation).</li> <li>Link to Privacy Policy.</li> </ul> </li> <li>Interactions: Toggling changes consent. Buttons trigger respective actions.</li> <li>States: Toggles on/off, processing states for export/delete.</li> </ul> </li> <li> <p>AI Models Settings (<code>AIModelManagementScreen.tsx</code>):</p> <ul> <li>Visuals: List of currently loaded AI models and their versions.</li> <li>Elements:<ul> <li>For each model (LLM, ASR, TTS, SER, Embedding):<ul> <li>Model Name (e.g., \"Mistral 7B,\" \"Whisper.cpp\").</li> <li>Current Version.</li> <li>Status (e.g., \"Loaded,\" \"Updating,\" \"Available Update\").</li> <li>Button: \"Check for Updates\" (for all models).</li> <li>Button: \"Download Update\" (if available for a specific model).</li> <li>Button: \"Rollback to Previous Version\" (if applicable).</li> </ul> </li> </ul> </li> <li>Interactions: Tapping \"Download Update\" initiates download.</li> <li>States: Loading, up-to-date, update available, downloading, installing, error.</li> </ul> </li> <li> <p>Notifications Settings (<code>NotificationSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Controls for proactive nudges.</li> <li>Elements:<ul> <li>Toggle: \"Enable Proactive Nudges.\"</li> <li>List of Nudge Types (e.g., \"Reminders,\" \"Positive Reinforcement,\" \"Wellness Suggestions\").</li> <li>For each type: Toggle to enable/disable, frequency picker (Daily, Weekly, Event-based), time picker.</li> </ul> </li> <li>Interactions: Toggling enables/disables nudges. Adjusting frequency/time updates preferences.</li> <li>States: Enabled/disabled, configured.</li> </ul> </li> <li> <p>Sync Settings (<code>SyncSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Controls for optional cloud synchronization.</li> <li>Elements:<ul> <li>Toggle: \"Enable Cloud Sync.\"</li> <li>Input fields for cloud service credentials (if using a generic E2EE service like EteSync).</li> <li>Dropdown: \"Sync Frequency\" (Manual, Daily, Weekly, On App Close).</li> <li>Display: \"Last Synced At:\"</li> <li>Display: \"Current Status:\"</li> <li>Button: \"Sync Now\".</li> <li>Button: \"Generate Recovery Key\" (for E2EE data).</li> <li>Button: \"Resolve Conflicts\" (if conflicts detected).</li> </ul> </li> <li>Interactions: Toggling enables/disables sync. Buttons trigger sync, key generation, or conflict resolution.</li> <li>States: Enabled/disabled, syncing, synced, conflicts, error.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>ProfileSettingsScreen.tsx</code> will be the root of the settings tab, using <code>React Navigation</code>'s stack navigator to push sub-screens (e.g., <code>GeneralSettingsScreen</code>, <code>PrivacyDashboardScreen</code>).</li> <li>Reusable Component Library: <code>ToggleSwitch</code>, <code>TextInput</code>, <code>Button</code>, <code>Picker</code>, <code>List</code> components.</li> <li>State Management:<ul> <li><code>useState</code> for local form inputs within each settings sub-screen.</li> <li><code>useEffect</code> to load initial settings data from relevant managers (<code>UserManager</code>, <code>PersonalityManager</code>, etc.).</li> <li><code>useCallback</code> for debouncing input changes before saving to backend.</li> <li>Global state for <code>userId</code>, <code>currentPersonalityId</code>, <code>syncStatus</code>.</li> </ul> </li> <li>Routing: <code>ProfileSettingsScreen</code> is a tab in <code>MainTabView</code>. Sub-screens are pushed onto its own navigation stack.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>User Preferences (Update <code>Users</code> table):<ul> <li>Flow: User changes name, tone, sync settings, nudge preferences -&gt; <code>UserManager.updateUser</code> or <code>NudgeManager.updateNudgePreference</code> is called.</li> <li><code>IPCService</code> sends updates to <code>data_manager.ts</code> to update <code>Users</code> or <code>NudgePreferences</code> table.</li> </ul> </li> <li>Personality Profiles (Read/Update <code>PersonalityProfiles</code> table):<ul> <li>Flow: <code>PersonalitySettingsScreen</code> loads <code>PersonalityManager.getPersonalityProfiles()</code>. User taps \"Activate\" -&gt; <code>PersonalityManager.activatePersonalityProfile</code> updates <code>Users.current_personality_profile_id</code>.</li> </ul> </li> <li>Privacy Consents (Update <code>Users</code> table):<ul> <li>Flow: Toggling consent switches calls <code>UserManager.updatePrivacySettings</code>.</li> </ul> </li> <li>Data Export/Deletion (Trigger Backend Process):<ul> <li>Flow: User clicks \"Export\" -&gt; <code>UserManager.exportData()</code> calls <code>IPCService</code> to trigger a Python script (<code>data_exporter.py</code>) in the backend.</li> <li>User clicks \"Delete\" -&gt; confirmation -&gt; <code>UserManager.deleteUser()</code> calls <code>IPCService</code> to <code>data_manager.ts</code> to delete user data.</li> </ul> </li> <li>AI Model Management (Read/Update Model Files):<ul> <li>Flow: <code>AIModelManagementScreen</code> loads <code>ModelManager.getLoadedModels()</code>. User clicks \"Download Update\" -&gt; <code>ModelManager.downloadModelUpdate()</code> calls <code>IPCService</code> to <code>model_manager.ts</code> (Node.js) which uses <code>model_downloader.py</code> (Python) to download the model.</li> <li><code>model_manager.ts</code> then loads the new model into the <code>ai_inference_addon</code>.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Clear Categorization: Settings are logically grouped to prevent overwhelming the user.</li> <li>Instant Feedback: Toggles and input fields update immediately.</li> <li>Confirmation: Critical actions (e.g., data deletion) require explicit confirmation.</li> <li>Progress Indicators: Show progress for long-running operations like model downloads or data exports.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Sensitive Data Input: If cloud sync requires credentials, ensure input fields are secure (e.g., password masking).</li> <li>Consent Visibility: Clearly display current consent status for sensitive data processing.</li> <li>Confirmation for Destructive Actions: Double-confirm data deletion.</li> <li>Recovery Key Display: When generating a recovery key, display it prominently with warnings about secure storage.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test individual settings components (e.g., <code>ToggleSwitch</code>, <code>Picker</code>).</li> <li>Integration Tests: Test data loading and saving for each settings section. Simulate model downloads and sync processes.</li> <li>End-to-End Tests: Simulate a user changing various settings, verifying persistence and impact on app behavior (e.g., personality change affects LLM response).</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Save Errors: Display \"Failed to save settings. Please try again.\" for backend save failures.</li> <li>Download Errors: Display \"Failed to download model update.\" for model download failures.</li> <li>Sync Errors: Display \"Sync failed. Check network or credentials.\" for sync issues.</li> <li>Deletion Errors: Display \"Failed to delete data.\"</li> <li>Logging: Log all settings changes, sync events, model updates, and data export/deletion attempts.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-proactive-nudges-notifications","title":"Feature: Proactive Nudges &amp; Notifications","text":"<p>Feature Goal (UI-specific) To proactively engage the user with timely reminders, positive reinforcement, and wellness suggestions, delivered through macOS system notifications and an in-app history.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>NotificationService</code> (Frontend Service): Registers for and displays macOS system notifications.</li> <li><code>NudgeManager</code> (Frontend Service): Manages nudge preferences and records nudge history.</li> <li><code>ai_inference_server.ts</code> (Node.js Backend): The LLM in the backend will generate nudge content based on user data and preferences.</li> <li><code>data_manager.ts</code> (Node.js Backend): Stores nudge preferences and history.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Notification Display (macOS System Level):</p> <ul> <li>Visuals: Standard macOS notification banners/alerts.</li> <li>Elements:<ul> <li>App icon.</li> <li>Title (e.g., \"Saira Check-in,\" \"Saira Motivation\").</li> <li>Body text (the nudge message, e.g., \"How are you feeling today?\").</li> <li>Optional: Action buttons (e.g., \"Log Mood,\" \"Dismiss\").</li> </ul> </li> <li>Interactions: User can click on the notification to open the app to a relevant screen (e.g., Journaling screen for mood check-in).</li> <li>States: Delivered, interacted.</li> </ul> </li> <li> <p>In-App Nudge History (<code>NotificationSettingsScreen.tsx</code> - Sub-section):</p> <ul> <li>Visuals: A scrollable list of past nudges.</li> <li>Elements:<ul> <li><code>FlatList</code> displaying <code>NudgeHistoryItem</code> components.</li> <li>Each item: Date/Time, Nudge Type, Message Content.</li> <li>Optional: Icon indicating if user interacted with it.</li> </ul> </li> <li>Interactions: User can review past nudges.</li> <li>States: Loading, empty list.</li> </ul> </li> <li> <p>Nudge Configuration (Part of <code>ProfileSettingsScreen.tsx</code> -&gt; <code>NotificationSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Toggles and selectors for customizing nudge behavior.</li> <li>Elements:<ul> <li>Toggle: \"Enable Proactive Nudges.\"</li> <li>Section for \"Nudge Types\":<ul> <li>For each type (e.g., \"Reminders,\" \"Positive Reinforcement,\" \"Wellness Suggestions\"):<ul> <li>Toggle to enable/disable.</li> <li>Dropdown/Picker for \"Frequency\" (Daily, Weekly, Event-based).</li> <li>Time picker for daily/weekly nudges.</li> </ul> </li> </ul> </li> <li>Button: \"Customize Nudge Content\" (future feature, opens a modal for prompt templates).</li> </ul> </li> <li>Interactions: Toggling enables/disables. Selecting frequency/time updates preferences.</li> <li>States: Enabled/disabled, configured.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>System Architecture Overview:</p> <ul> <li>High-level Architecture: Nudge generation logic resides in the Node.js backend (LLM). The Node.js backend will schedule nudges and send them to the React Native frontend via IPC. The React Native frontend will then use macOS native modules to display system notifications.</li> <li>Technology Stack:<ul> <li>Frontend (UI): React Native for macOS (TypeScript) for <code>NotificationSettingsScreen</code> and handling incoming notifications.</li> <li>Backend (Nudge Generation &amp; Scheduling): Node.js (<code>ai_inference_server.ts</code> for LLM, <code>nudge_scheduler.ts</code> for scheduling).</li> <li>Local Database: SQLite for <code>NudgePreferences</code> and <code>NudgeHistory</code>.</li> <li>macOS Notifications: React Native native module (Objective-C/Swift) to interact with <code>UNUserNotificationCenter</code> for system notifications.</li> <li>IPC: Local HTTP/WebSocket for backend to send nudge content to frontend.</li> </ul> </li> </ul> </li> <li> <p>Database Schema Design:     (Refer to \"Core Application Framework\" feature for <code>NudgePreferences</code> and <code>NudgeHistory</code> tables.)</p> </li> <li> <p>Comprehensive API Design:</p> <ul> <li>Frontend (React Native) -&gt; Backend (Node.js) IPC (HTTP Endpoints on <code>ai_inference_server.ts</code>):<ul> <li><code>POST /nudges/preferences</code>: Update nudge preferences.<ul> <li>Request: <code>{ userId: string, preferences: NudgePreferenceDTO }</code></li> <li>Response: <code>{ success: boolean }</code></li> </ul> </li> <li><code>GET /nudges/history</code>: Retrieve nudge history.<ul> <li>Request: <code>{ userId: string, pagination: PaginationDTO }</code></li> <li>Response: <code>{ history: NudgeHistoryDTO }</code></li> </ul> </li> </ul> </li> <li>Backend (Node.js) -&gt; Frontend (React Native) IPC (WebSocket <code>ai_inference_server.ts</code> or custom local IPC):<ul> <li><code>nudge_delivered</code>: Event sent when a nudge is generated and ready for display.<ul> <li>Payload: <code>{ nudgeId: string, message: string, actionType: string }</code></li> </ul> </li> </ul> </li> <li>Node.js Backend (Internal APIs):<ul> <li><code>NudgeScheduler</code> (TypeScript Class):<ul> <li><code>scheduleNudges(userId: string, preferences: NudgePreferenceDTO)</code>: Sets up timers/cron jobs.</li> <li><code>generateNudgeContent(userId: string, type: string): Promise&lt;string&gt;</code>: Calls LLM to generate content.</li> <li><code>recordNudgeDelivery(nudgeId: string, userId: string, message: string)</code>: Inserts into <code>NudgeHistory</code>.</li> </ul> </li> <li><code>ai_inference_server.ts</code>: Exposes LLM for nudge content generation.</li> </ul> </li> </ul> </li> <li> <p>Frontend Architecture (React Native for macOS):</p> <ul> <li><code>NotificationSettingsScreen.tsx</code>:<ul> <li>Displays nudge configuration UI.</li> <li>Fetches <code>NudgePreferences</code> and <code>NudgeHistory</code> from <code>NudgeManager</code>.</li> <li>Sends updates to <code>NudgeManager</code>.</li> </ul> </li> <li><code>App.tsx</code> (or <code>AppStateManager.ts</code>): Listens for <code>nudge_delivered</code> events from the Node.js backend.</li> <li>Native Module (Objective-C/Swift): A custom React Native native module (e.g., <code>NativeNotificationModule</code>) will be developed to interact with macOS <code>UNUserNotificationCenter</code> to display system notifications.<ul> <li><code>NativeNotificationModule.displayNotification(title: string, body: string, userInfo: object)</code></li> <li><code>NativeNotificationModule.onNotificationClicked(callback: (userInfo: object) =&gt; void)</code></li> </ul> </li> <li>State Management: <code>useState</code> for nudge preferences and history. Global state to manage notification permissions.</li> <li>Routing: <code>NotificationSettingsScreen</code> is a sub-screen of <code>ProfileSettingsScreen</code>.</li> </ul> </li> <li> <p>Detailed CRUD Operations:</p> <ul> <li>Create Nudge Preference: <code>NudgeManager.createNudgePreference</code> (via UI).</li> <li>Read Nudge Preferences: <code>NudgeManager.getNudgePreferences</code> (for UI display).</li> <li>Update Nudge Preference: <code>NudgeManager.updateNudgePreference</code> (via UI toggles/pickers).</li> <li>Delete Nudge Preference: <code>NudgeManager.deleteNudgePreference</code> (if user removes a custom nudge type).</li> <li>Create Nudge History: <code>NudgeScheduler.recordNudgeDelivery</code> (backend-initiated).</li> <li>Read Nudge History: <code>NudgeManager.getNudgeHistory</code> (for UI display).</li> <li>Update Nudge History: <code>NudgeManager.markNudgeInteracted</code> (when user clicks a notification).</li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Opt-in: User explicitly enables proactive nudges in settings.</li> <li>System Notifications: Nudges appear as standard macOS notifications.</li> <li>Click-through: Clicking a notification opens Saira to a relevant screen (e.g., Journaling for a mood check-in nudge).</li> <li>In-app Review: Users can review past nudges in settings.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Notification Permissions: The app will request macOS notification permissions.</li> <li>Privacy: Nudge content should be general and not expose sensitive user data in the notification itself.</li> <li>Consent Adherence: Nudges related to emotion/memory will only be sent if the user has given consent for those data types.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>NotificationSettingsScreen</code> components.</li> <li>Integration Tests: Test the flow from backend nudge generation -&gt; IPC -&gt; frontend notification display. Test click-through behavior.</li> <li>End-to-End Tests: Simulate scheduled nudges appearing and user interaction.</li> </ul> </li> <li> <p>Data Management:</p> <ul> <li>Nudge Preferences: Stored in <code>NudgePreferences</code> table.</li> <li>Nudge History: Stored in <code>NudgeHistory</code> table. Consider purging old history after a certain period to manage database size.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Notification Delivery Failures: Log if macOS fails to display a notification.</li> <li>Nudge Generation Errors: If LLM fails to generate nudge content, log the error and skip that nudge.</li> <li>User Feedback: If nudge scheduling fails, inform the user in settings.</li> </ul> </li> </ol> <p>This concludes the UI-focused feature specification, providing a granular breakdown of each screen and its associated requirements and implementation considerations. This level of detail should be highly beneficial for your task planning.\\&lt;ctrl95&gt;## Feature Specifications (UI-Focused)</p>"},{"location":"ui_implemenation/#feature-onboarding-flow_1","title":"Feature: Onboarding Flow","text":"<p>Feature Goal (UI-specific) To guide new users through an intuitive, multi-step setup process, collecting essential preferences and obtaining necessary consents to personalize their Saira experience from the first interaction.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>UserManager</code> (Frontend Service): Calls <code>createUser</code> with collected preferences and consent data.</li> <li><code>PersonalityManager</code> (Frontend Service): Fetches pre-defined personality profiles for selection.</li> <li><code>DataStoreService</code> (Frontend Service): Persists initial user data locally.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Welcome Screen (<code>OnboardingScreen.tsx</code> - Step 1):</p> <ul> <li>Visuals: Full-screen splash with Saira logo, a welcoming tagline (e.g., \"Your Emotion-Aware Life Companion\"), and a brief, compelling description of Saira's purpose.</li> <li>Elements:<ul> <li>Large, centered Saira logo/icon.</li> <li>Headline text: \"Welcome to Saira.\"</li> <li>Body text: \"An AI that understands your emotions, grows with you, and helps you manage your mental, emotional, and personal life through intelligent, natural conversations.\"</li> <li>Primary Call-to-Action (CTA) button: \"Get Started\" (or \"Begin Your Journey\").</li> </ul> </li> <li>Interactions: Tapping \"Get Started\" navigates to the Consent Screen.</li> <li>States: Initial load state.</li> </ul> </li> <li> <p>Consent Screen (<code>OnboardingScreen.tsx</code> - Step 2):</p> <ul> <li>Visuals: A clear, concise screen presenting privacy commitments and requiring explicit user consent for sensitive data processing.</li> <li>Elements:<ul> <li>Headline: \"Your Privacy, Our Priority.\"</li> <li>Body text explaining Saira's privacy-first approach (local processing, E2EE sync).</li> <li>Checkbox: \"I agree to Saira's Privacy Policy and Terms of Service.\" (Link to full policy).</li> <li>Checkbox: \"Allow Saira to record and analyze my emotional tone from voice for personalized support.\" (Optional, with clear explanation).</li> <li>Checkbox: \"Allow Saira to record and store memories from our conversations to build long-term context.\" (Optional, with clear explanation).</li> <li>Primary CTA button: \"Continue\" (disabled until mandatory consents are checked).</li> <li>Secondary button: \"Learn More About Privacy\" (opens external link or in-app modal).</li> </ul> </li> <li>Interactions:<ul> <li>Tapping checkboxes toggles their state.</li> <li>\"Continue\" button enables only when \"Privacy Policy\" checkbox is checked.</li> <li>Tapping \"Continue\" navigates to Personality Selection.</li> </ul> </li> <li>States: Button disabled/enabled states.</li> </ul> </li> <li> <p>Personality Selection Screen (<code>OnboardingScreen.tsx</code> - Step 3):</p> <ul> <li>Visuals: A grid or carousel of pre-defined personality profiles with brief descriptions.</li> <li>Elements:<ul> <li>Headline: \"Choose Saira's Initial Personality.\"</li> <li>Description: \"You can change this anytime in settings.\"</li> <li>List/Grid of <code>PersonalityProfileCard</code> components:<ul> <li>Each card displays: Personality Name (e.g., \"The Coach,\" \"The Best Friend,\" \"The Mom\").</li> <li>Short description of personality traits and interaction style.</li> <li>Visual indicator for selection (e.g., border, checkmark).</li> </ul> </li> <li>Primary CTA button: \"Next\".</li> </ul> </li> <li>Interactions:<ul> <li>Tapping a <code>PersonalityProfileCard</code> selects it. Only one can be active.</li> <li>\"Next\" button navigates to Goal Setting.</li> </ul> </li> <li>States: Selected/unselected card states.</li> </ul> </li> <li> <p>Goal Setting Screen (<code>OnboardingScreen.tsx</code> - Step 4):</p> <ul> <li>Visuals: A screen for users to define initial personal goals.</li> <li>Elements:<ul> <li>Headline: \"What are your initial goals with Saira?\"</li> <li>Description: \"Saira can help you track and achieve personal goals. You can add more later.\"</li> <li><code>TextInput</code> field: \"Add a new goal...\" (e.g., \"Improve sleep,\" \"Reduce stress,\" \"Learn a new skill\").</li> <li>Button: \"Add Goal\" (adds text from input to a list).</li> <li><code>FlatList</code> or <code>ScrollView</code> displaying added goals:<ul> <li>Each goal item: Text of the goal, small \"x\" button to remove.</li> </ul> </li> <li>Primary CTA button: \"Next\".</li> <li>Secondary button: \"Skip for now\".</li> </ul> </li> <li>Interactions:<ul> <li>Typing in <code>TextInput</code> updates its value.</li> <li>Tapping \"Add Goal\" adds the goal to the list.</li> <li>Tapping \"x\" removes a goal.</li> <li>\"Next\" navigates to Tone Style Preference.</li> <li>\"Skip for now\" navigates to Tone Style Preference.</li> </ul> </li> <li>States: Empty input, populated list.</li> </ul> </li> <li> <p>Tone Style Preference Screen (<code>OnboardingScreen.tsx</code> - Step 5):</p> <ul> <li>Visuals: A screen for users to select Saira's preferred response tone.</li> <li>Elements:<ul> <li>Headline: \"How should Saira speak to you?\"</li> <li>Description: \"Choose a general tone style for Saira's responses.\"</li> <li>Radio buttons or segmented control for tone options: \"Empathetic,\" \"Direct,\" \"Neutral,\" \"Playful.\"</li> <li>Short example text for each tone option.</li> <li>Primary CTA button: \"Finish Setup\".</li> </ul> </li> <li>Interactions: Tapping a radio button selects the tone. \"Finish Setup\" button navigates to the Main Application.</li> <li>States: Selected/unselected tone options.</li> </ul> </li> <li> <p>Onboarding Complete Screen (<code>OnboardingScreen.tsx</code> - Final Step):</p> <ul> <li>Visuals: A celebratory screen confirming setup completion.</li> <li>Elements:<ul> <li>Large success icon/animation.</li> <li>Headline: \"You're All Set!\"</li> <li>Body text: \"Saira is ready to begin your journey together.\"</li> <li>Primary CTA button: \"Start Talking to Saira\".</li> </ul> </li> <li>Interactions: Tapping \"Start Talking to Saira\" dismisses the onboarding flow and navigates to the <code>ConversationScreen</code>.</li> <li>States: Success animation.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>OnboardingScreen.tsx</code> will be a single React Native component that manages the state for which step of the onboarding the user is on. It will conditionally render sub-components for each step (e.g., <code>WelcomeStep</code>, <code>ConsentStep</code>, <code>PersonalityStep</code>, <code>GoalStep</code>, <code>ToneStep</code>, <code>CompletionStep</code>).</li> <li>Reusable Component Library: Utilize <code>Button</code>, <code>TextInput</code>, <code>Checkbox</code>, <code>RadioGroup</code>, <code>Card</code> components from a shared library.</li> <li>State Management:<ul> <li><code>useState</code> hook within <code>OnboardingScreen.tsx</code> to manage the <code>currentStep</code> (integer).</li> <li><code>useState</code> for form inputs (e.g., <code>userName</code>, <code>selectedPersonalityId</code>, <code>goalsList</code>, <code>selectedTone</code>).</li> <li><code>useState</code> for consent checkbox states.</li> <li><code>useEffect</code> to enable/disable \"Continue\" buttons based on validation.</li> <li>Global state (e.g., <code>Zustand</code> store) to persist onboarding data temporarily before final submission.</li> </ul> </li> <li>Routing:<ul> <li><code>AppNavigator.tsx</code> will check an <code>onboarding_completed</code> flag (from <code>UserManager</code>) on app launch. If <code>false</code>, it navigates to <code>OnboardingScreen</code>.</li> <li>Internal navigation within <code>OnboardingScreen</code> will update <code>currentStep</code> state.</li> <li>Upon \"Finish Setup,\" <code>AppNavigator</code> will replace the onboarding stack with the <code>MainTabView</code> stack.</li> </ul> </li> <li>Responsive Design: Use <code>Flexbox</code> for layout. Ensure content scrolls on smaller window sizes if necessary. Text sizes should scale appropriately.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Create User: The entire onboarding flow is a \"Create\" operation for the <code>User</code> entity. Data is collected across multiple screens and submitted once on \"Finish Setup.\"<ul> <li>Validation: Frontend validation for required fields (e.g., name not empty, consent checked).</li> <li>Required Fields: <code>name</code>, <code>gdpr_ccpa_consent</code>, <code>emotion_recording_consent</code>, <code>memory_recording_consent</code> (from <code>Users</code> table schema).</li> <li>Flow:<ol> <li>User inputs data in each step.</li> <li>On \"Finish Setup,\" <code>OnboardingScreen</code> collects all <code>useState</code> data.</li> <li>Calls <code>UserManager.createUser(collectedData)</code>.</li> <li><code>UserManager</code> sends this data via <code>IPCService</code> to the Node.js <code>data_manager.ts</code> to insert into the <code>Users</code> table.</li> <li><code>UserManager</code> updates the <code>onboarding_completed</code> flag in local storage.</li> </ol> </li> </ul> </li> <li>Read Personality Profiles: <code>PersonalitySelectionScreen</code> performs a \"Read\" operation to fetch available <code>PersonalityProfiles</code> from <code>PersonalityManager</code>.<ul> <li>Flow: <code>PersonalitySelectionScreen</code> <code>useEffect</code> calls <code>PersonalityManager.getPersonalityProfiles()</code> -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>PersonalityProfiles</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Visual Progress: A simple progress indicator (e.g., \"Step X of Y\" or a progress bar) at the top of each onboarding screen.</li> <li>Clear Instructions: Concise headings and body text to guide the user.</li> <li>Feedback: Immediate visual feedback on button presses (e.g., loading spinner on \"Finish Setup\").</li> <li>Error Handling: If <code>UserManager.createUser</code> fails (e.g., database error), display a user-friendly error message (e.g., \"Setup failed. Please try again.\") and allow retries.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Consent Display: Clearly present privacy policy and consent checkboxes. Ensure they are prominent and require explicit interaction.</li> <li>Input Masking: If any sensitive input (e.g., future password for sync) is added, ensure it's masked.</li> <li>No Sensitive Data in UI State: Avoid storing sensitive user data directly in easily inspectable UI state. Pass it to backend services immediately.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test individual <code>OnboardingScreen</code> sub-components for rendering, state updates, and button interactions.</li> <li>Integration Tests: Test the full multi-step flow, ensuring data is correctly collected and passed between steps. Mock <code>UserManager</code> calls.</li> <li>End-to-End Tests: Simulate a full user onboarding from app launch to <code>MainTabView</code> display, verifying all UI elements and interactions.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Frontend Validation Errors: Display inline error messages for invalid inputs (e.g., \"Name cannot be empty\").</li> <li>Backend Errors: If <code>createUser</code> API call fails, display a generic error message (e.g., \"Failed to save preferences. Please try again.\") in a toast or alert. Log detailed error to <code>Logger.ts</code>.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-main-application-navigation_1","title":"Feature: Main Application Navigation","text":"<p>Feature Goal (UI-specific) To provide a clear, intuitive, and persistent navigation structure that allows users to easily switch between the primary sections of the Saira application.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>AppStateManager</code> (Frontend Service): May update global state based on active tab (e.g., to pause/resume background processes).</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Tab Bar Display:</p> <ul> <li>Visuals: A persistent tab bar at the bottom (or left sidebar for macOS desktop feel) of the main application window.</li> <li>Elements:<ul> <li>Icons and text labels for each primary section: \"Conversation,\" \"Journal,\" \"Deep Talk,\" \"Profile.\"</li> <li>Visual indication of the currently active tab.</li> </ul> </li> <li>Interactions: Tapping a tab icon/label switches to the corresponding screen.</li> <li>States: Active/inactive tab states.</li> </ul> </li> <li> <p>Screen Switching:</p> <ul> <li>Visuals: Smooth transitions between screens when switching tabs.</li> <li>Behavior: Each tab maintains its own navigation stack (e.g., navigating deep within \"Journal\" and then switching to \"Conversation\" and back, returns to the deep state in \"Journal\").</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>App.tsx</code> (root) renders <code>AppNavigator.tsx</code>. <code>AppNavigator.tsx</code> uses <code>React Navigation</code>'s <code>createBottomTabNavigator</code> (for mobile) or <code>createMaterialTopTabNavigator</code> (can be styled for desktop sidebar) to define the main tab structure. Each tab will render its own root screen component (e.g., <code>ConversationScreen</code>, <code>JournalingScreen</code>).</li> <li>Reusable Component Library: Standard React Native <code>Tab.Navigator</code> and <code>Tab.Screen</code> components. Custom icons for tabs.</li> <li>State Management: <code>React Navigation</code> handles internal state for active tab and navigation stacks.</li> <li>Routing: <code>React Navigation</code>'s declarative API defines the tab routes.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>This feature is purely navigational and does not directly involve CRUD operations on data entities. It facilitates access to other features that perform CRUD.</li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Immediate Feedback: Tapping a tab should instantly switch the view.</li> <li>Consistency: Tab bar remains visible and in the same position across all main screens.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>No direct security implications for the navigation component itself. Access to sensitive tabs (e.g., Profile Settings) is implicitly protected by the overall app's local authentication.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>AppNavigator</code> component for correct tab rendering and initial route.</li> <li>Integration Tests: Verify that tapping tabs correctly switches screens and that navigation stacks are maintained.</li> <li>End-to-End Tests: Simulate user navigating between all main tabs.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>No specific error handling for navigation itself, beyond standard React Native error boundaries.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-conversation-screen_1","title":"Feature: Conversation Screen","text":"<p>Feature Goal (UI-specific) To provide a dynamic and intuitive chat interface for real-time voice and text interaction with Saira, displaying conversational history, real-time transcription, and AI responses.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>IPCService</code> (Frontend Service): Sends audio chunks, text input, and receives transcription updates, emotion data, LLM text, and TTS audio.</li> <li><code>MessageManager</code> (Frontend Service): Adds new messages (user and AI) to the local database.</li> <li><code>ConversationManager</code> (Frontend Service): Manages the current conversation session (e.g., starting a new one, updating title).</li> <li><code>MemoryManager</code> (Frontend Service): Triggers memory formation based on conversation turns.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Chat History Display:</p> <ul> <li>Visuals: A scrollable list of chat bubbles, visually distinguishing between user messages and AI responses.</li> <li>Elements:<ul> <li><code>FlatList</code> or <code>ScrollView</code> to display <code>MessageBubble</code> components.</li> <li>Each <code>MessageBubble</code> displays text content.</li> <li>User messages aligned right, AI messages aligned left.</li> <li>Timestamps for messages (optional, on hover or grouped).</li> </ul> </li> <li>Interactions: Auto-scroll to bottom on new message. User can manually scroll up to view history.</li> <li>States: Empty state (e.g., \"Say 'Hey Saira' to begin!\"), loading state for initial history.</li> </ul> </li> <li> <p>Voice Input Indicator:</p> <ul> <li>Visuals: A prominent, dynamic indicator (e.g., a pulsing microphone icon, a waveform visualization) that changes based on the AI's listening/processing state.</li> <li>Elements:<ul> <li>Microphone icon.</li> <li>Optional: Simple waveform animation or pulsating circle.</li> <li>Text status: \"Listening...\", \"Thinking...\", \"Speaking...\", \"Idle.\"</li> </ul> </li> <li>Interactions: Tapping the microphone icon can manually toggle listening (if not using wake word).</li> <li>States: Idle, Listening, Processing (AI thinking), Speaking (AI responding).</li> </ul> </li> <li> <p>Real-time Transcription Display:</p> <ul> <li>Visuals: Text area above the input bar that updates as the user speaks.</li> <li>Elements:<ul> <li><code>Text</code> component displaying the current partial or final transcription.</li> <li>Text color/style might change to indicate finality.</li> </ul> </li> <li>Interactions: Updates dynamically.</li> <li>States: Empty, partial transcription, final transcription.</li> </ul> </li> <li> <p>Input Bar:</p> <ul> <li>Visuals: A persistent input area at the bottom of the screen.</li> <li>Elements:<ul> <li><code>TextInput</code> for typing messages.</li> <li>Microphone button (to initiate voice input if wake word is off, or to manually stop/start recording).</li> <li>Send button (for text input).</li> </ul> </li> <li>Interactions:<ul> <li>Typing in <code>TextInput</code> updates its value.</li> <li>Tapping microphone button toggles voice input.</li> <li>Tapping send button sends text message.</li> </ul> </li> <li>States: Empty, text entered, sending.</li> </ul> </li> <li> <p>AI Response Playback:</p> <ul> <li>Behavior: Audio plays automatically as AI response text appears.</li> <li>Visuals: No explicit UI for playback, but the <code>VoiceInputIndicator</code> should show \"Speaking...\" state.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>ConversationScreen.tsx</code> will be the main screen. It will contain <code>MessageList</code> (a <code>FlatList</code> of <code>MessageBubble</code> components), <code>VoiceInputIndicator</code>, and <code>InputBar</code>.</li> <li>Reusable Component Library: <code>MessageBubble</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code> components.</li> <li>State Management:<ul> <li><code>useState</code> for <code>currentTranscription</code>, <code>aiResponseText</code>, <code>inputMessageText</code>.</li> <li><code>useState</code> for <code>conversationState</code> (e.g., 'idle', 'listening', 'processing', 'speaking').</li> <li><code>useState</code> for <code>messages</code> array (local display cache).</li> <li><code>useEffect</code> to manage WebSocket connection to <code>ai_inference_server.ts</code>.</li> <li><code>useEffect</code> to auto-scroll <code>FlatList</code> when new messages arrive.</li> <li>Global state (e.g., <code>Zustand</code> store) for <code>currentConversationId</code>, <code>activePersonalityId</code>.</li> </ul> </li> <li>Routing: <code>ConversationScreen</code> is the default tab in <code>MainTabView</code>.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Read Messages: <code>ConversationScreen</code> performs a \"Read\" operation to fetch initial conversation history.<ul> <li>Flow: On component mount, <code>ConversationScreen</code> calls <code>MessageManager.getMessages(currentConversationId, pagination)</code> -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>Messages</code> table.</li> </ul> </li> <li>Create Message (User):<ul> <li>Flow (Voice): <code>audio_processor.ts</code> streams audio to <code>ai_inference_server.ts</code> -&gt; <code>ai_inference_server.ts</code> sends <code>transcription_update</code> via WebSocket -&gt; <code>ConversationScreen</code> updates <code>currentTranscription</code>. When <code>isFinal</code> is true, <code>MessageManager.addMessage</code> is called.</li> <li>Flow (Text): User types and presses send -&gt; <code>MessageManager.addMessage</code> is called.</li> <li><code>MessageManager</code> sends message data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>Messages</code> table.</li> </ul> </li> <li>Create Message (AI):<ul> <li>Flow: <code>ai_inference_server.ts</code> sends <code>ai_response_text</code> via WebSocket -&gt; <code>ConversationScreen</code> updates UI. Once full response received, <code>MessageManager.addMessage</code> is called to store AI's message.</li> <li><code>MessageManager</code> sends message data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>Messages</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Real-time Feedback: Visual and textual indicators for AI's state (listening, thinking, speaking).</li> <li>Smooth Scrolling: Ensure <code>FlatList</code> performs well with many messages.</li> <li>Typing vs. Voice: Seamless switch between typing and voice input modes.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Microphone Permission: React Native will trigger the macOS system prompt for microphone access. The UI should guide the user if permission is denied.</li> <li>No Raw Audio Display: Raw audio data is processed by the backend and never displayed in the UI.</li> <li>Consent Adherence: If emotion/memory recording consent is denied, the UI should reflect this (e.g., disable related indicators or features).</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>MessageBubble</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code> components for rendering and basic interactions.</li> <li>Integration Tests: Test WebSocket connection, real-time transcription updates, and message display. Mock backend responses.</li> <li>End-to-End Tests: Simulate full voice conversations, verifying UI updates, message persistence, and AI response playback.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Connection Errors: If WebSocket connection to backend fails, display a prominent error message (e.g., \"Saira's brain is offline. Please restart the app.\") and disable voice input.</li> <li>Transcription Errors: If ASR fails, display \"Could not understand. Please try again.\"</li> <li>LLM Errors: If LLM fails to generate a response, display \"Saira is having trouble thinking. Please try again.\"</li> <li>Audio Playback Errors: Log errors if TTS audio fails to play.</li> <li>Logging: Detailed logs of audio stream events, AI processing steps, and message storage.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-daily-check-ins-journaling-screen_1","title":"Feature: Daily Check-ins &amp; Journaling Screen","text":"<p>Feature Goal (UI-specific) To provide a dedicated space for users to reflect on their emotional state, log daily moods, and create personal journal entries, with visual summaries of emotional patterns over time.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>JournalManager</code> (Frontend Service): Creates, reads, updates, and deletes journal entries and mood summaries.</li> <li><code>MessageManager</code> (Frontend Service): May retrieve emotion data from past messages for mood summaries.</li> <li><code>MemoryManager</code> (Frontend Service): Journal entries can trigger memory formation.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Daily Mood Summary (<code>JournalingScreen.tsx</code> - Main View):</p> <ul> <li>Visuals: A calendar-like view or a series of daily cards displaying aggregated mood data.</li> <li>Elements:<ul> <li>Date navigation (e.g., left/right arrows to change day/week/month).</li> <li>For each day:<ul> <li>Date display.</li> <li>Aggregated mood representation (e.g., a dominant emoji, a color gradient, or a simple text summary like \"Mostly Neutral\").</li> <li>Count of journal entries for that day.</li> <li>Link/button to view/add journal entry for that day.</li> </ul> </li> </ul> </li> <li>Interactions: Tapping a day navigates to its journal entry list/detail.</li> <li>States: Loading state, empty state for days with no data.</li> </ul> </li> <li> <p>Emotional Check-in Interface (<code>JournalingScreen.tsx</code> - Button/Modal):</p> <ul> <li>Visuals: A prominent button on the main journaling screen, or a modal that appears when triggered.</li> <li>Elements:<ul> <li>Button: \"How are you feeling today?\"</li> <li>Modal/Screen:<ul> <li>Headline: \"Log Your Mood.\"</li> <li>Mood picker: A visual slider for Arousal (Calm to Excited) and Valence (Negative to Positive), or a selection of emotion emojis/categories (e.g., Happy, Sad, Angry, Neutral).</li> <li>Optional <code>TextInput</code>: \"Add a note about your mood...\"</li> <li>Button: \"Save Mood\".</li> </ul> </li> </ul> </li> <li>Interactions: Tapping the button opens the modal. Interacting with sliders/pickers updates mood selection. Tapping \"Save Mood\" submits the data.</li> <li>States: Default, selected mood, saving.</li> </ul> </li> <li> <p>Journal Entry List (<code>JournalingScreen.tsx</code> - Sub-section/Screen):</p> <ul> <li>Visuals: A scrollable list of journal entries, ordered by date.</li> <li>Elements:<ul> <li><code>FlatList</code> displaying <code>JournalEntryCard</code> components.</li> <li>Each <code>JournalEntryCard</code> displays: Date, a snippet of the entry content, and a small mood indicator (if available).</li> <li>Button: \"New Journal Entry\" (floating action button or prominent header button).</li> </ul> </li> <li>Interactions: Tapping a <code>JournalEntryCard</code> navigates to <code>JournalEntryDetailView</code>. Tapping \"New Journal Entry\" navigates to <code>JournalEntryCreationView</code>.</li> <li>States: Loading, empty list.</li> </ul> </li> <li> <p>Journal Entry Creation/Detail (<code>JournalEntryCreationView.tsx</code>/<code>JournalEntryDetailView.tsx</code>):</p> <ul> <li>Visuals: A dedicated screen for writing/editing a journal entry.</li> <li>Elements:<ul> <li>Date/Time display (auto-filled, editable).</li> <li>Large <code>TextInput</code> for journal content.</li> <li>Optional: Mood picker (similar to check-in) if not already set.</li> <li>Button: \"Save Entry\".</li> <li>For detail view: \"Edit\" button, \"Delete\" button.</li> </ul> </li> <li>Interactions: Typing updates content. Tapping \"Save Entry\" persists. \"Edit\" makes content editable. \"Delete\" prompts confirmation then deletes.</li> <li>States: Editing, viewing, saving, deleting.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>JournalingScreen.tsx</code> will be the main tab screen. It will contain <code>MoodSummaryCalendar</code> and <code>JournalEntryList</code> components. <code>JournalEntryCreationView.tsx</code> and <code>JournalEntryDetailView.tsx</code> will be separate screens navigated to from <code>JournalingScreen</code>.</li> <li>Reusable Component Library: <code>MoodSummaryCard</code>, <code>JournalEntryCard</code>, <code>DatePicker</code>, <code>Slider</code>, <code>EmojiPicker</code> (custom).</li> <li>State Management:<ul> <li><code>useState</code> for <code>selectedDate</code>, <code>journalEntries</code> array, <code>moodSummaries</code> array.</li> <li><code>useState</code> for <code>currentJournalContent</code>, <code>selectedMood</code>.</li> <li><code>useEffect</code> to fetch data from <code>JournalManager</code> based on <code>selectedDate</code> or filters.</li> <li>Global state for <code>userId</code>.</li> </ul> </li> <li>Routing: <code>JournalingScreen</code> is a tab in <code>MainTabView</code>. <code>JournalEntryCreationView</code> and <code>JournalEntryDetailView</code> are pushed onto the <code>JournalingScreen</code>'s navigation stack.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Create Journal Entry:<ul> <li>Flow: User types in <code>JournalEntryCreationView</code> -&gt; taps \"Save Entry\" -&gt; <code>JournalManager.createJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to insert into <code>JournalEntries</code> table.</li> <li>Also triggers <code>JournalManager.generateMoodSummary</code> for the relevant date.</li> </ul> </li> <li>Read Journal Entries:<ul> <li>Flow: <code>JournalingScreen</code> or <code>JournalEntryList</code> mounts -&gt; <code>JournalManager.getJournalEntries</code> is called (with date range/pagination) -&gt; <code>IPCService</code> to <code>data_manager.ts</code> to query <code>JournalEntries</code>.</li> <li><code>JournalManager.getMoodSummaries</code> is called for the calendar view.</li> </ul> </li> <li>Update Journal Entry:<ul> <li>Flow: User edits in <code>JournalEntryDetailView</code> -&gt; taps \"Save\" -&gt; <code>JournalManager.updateJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to update <code>JournalEntries</code> table.</li> </ul> </li> <li>Delete Journal Entry:<ul> <li>Flow: User taps \"Delete\" in <code>JournalEntryDetailView</code> -&gt; confirmation dialog -&gt; <code>JournalManager.deleteJournalEntry</code> is called.</li> <li><code>JournalManager</code> sends data via <code>IPCService</code> to <code>data_manager.ts</code> to delete from <code>JournalEntries</code> table.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Intuitive Logging: Easy access to mood check-in and new journal entry creation.</li> <li>Visual Summaries: Mood calendar provides a quick overview of emotional trends.</li> <li>Seamless Editing: Smooth transition from viewing to editing journal entries.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Data Privacy: Remind users about the privacy of their journal entries.</li> <li>Consent Adherence: Ensure emotion data is only recorded/displayed if consent was given.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test <code>MoodSummaryCard</code>, <code>JournalEntryCard</code>, <code>MoodPicker</code> components.</li> <li>Integration Tests: Test the full flow of creating, viewing, editing, and deleting journal entries. Verify mood summary generation.</li> <li>End-to-End Tests: Simulate a user logging moods and creating multiple journal entries over several days.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Save Errors: Display \"Failed to save entry. Please try again.\"</li> <li>Load Errors: Display \"Could not load journal entries.\"</li> <li>Logging: Log all CRUD operations on journal entries and mood summaries.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-deep-talk-screen_1","title":"Feature: Deep Talk Screen","text":"<p>Feature Goal (UI-specific) To provide a specialized conversational mode that encourages deeper exploration of complex personal topics, leveraging Saira's long-term memory (RAG) to provide more insightful and context-rich responses.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>IPCService</code> (Frontend Service): Sends audio/text input, receives AI responses, specifically optimized for RAG queries.</li> <li><code>MemoryManager</code> (Frontend Service): Triggers semantic search for relevant memories based on conversation context.</li> <li><code>ConversationManager</code> (Frontend Service): Manages the conversation session, potentially tagging it as a \"Deep Talk\" session.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Specialized Conversation Interface:</p> <ul> <li>Visuals: Similar to the <code>ConversationScreen</code>, but with subtle visual cues indicating \"Deep Talk Mode\" (e.g., a different background color, a unique icon in the header).</li> <li>Elements:<ul> <li>Chat history display (same as <code>ConversationScreen</code>).</li> <li>Voice input indicator (same as <code>ConversationScreen</code>).</li> <li>Real-time transcription display (same as <code>ConversationScreen</code>).</li> <li>Input bar (same as <code>ConversationScreen</code>).</li> <li>Optional: \"Suggested Topics\" or \"Memory Prompts\" area (e.g., a carousel of buttons with memory snippets or questions like \"Tell me more about X event\").</li> </ul> </li> <li>Interactions: Standard voice/text interaction. Tapping suggested topics injects them as a prompt.</li> <li>States: Active Deep Talk session.</li> </ul> </li> <li> <p>Memory Integration Indicators (Subtle):</p> <ul> <li>Visuals: When Saira's response is heavily influenced by a retrieved memory, a subtle visual indicator might appear (e.g., a small \"memory\" icon next to the AI's message, or a tooltip on hover revealing the source memory snippet). This is for transparency and to highlight Saira's RAG capability.</li> <li>Elements: Small icon, tooltip/popover.</li> <li>Interactions: Hovering/tapping the icon reveals the source memory.</li> <li>States: Memory-augmented response, non-augmented response.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>DeepTalkScreen.tsx</code> will largely reuse components from <code>ConversationScreen.tsx</code> (e.g., <code>MessageList</code>, <code>VoiceInputIndicator</code>, <code>InputBar</code>). It will wrap these in a container that applies Deep Talk specific styling.</li> <li>Reusable Component Library: Existing chat components. New <code>MemoryHint</code> component.</li> <li>State Management:<ul> <li>Inherits <code>conversationState</code> and <code>messages</code> state from a shared context or uses its own.</li> <li><code>useState</code> for <code>suggestedTopics</code> (populated by AI or based on recent memories).</li> <li>Global state for <code>userId</code>, <code>currentConversationId</code>.</li> </ul> </li> <li>Routing: <code>DeepTalkScreen</code> is a tab in <code>MainTabView</code>.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>Read Memories (for RAG): This is the core \"Read\" operation.<ul> <li>Flow: When a user speaks/types in Deep Talk mode, the <code>ConversationScreen</code>'s ViewModel (or a dedicated <code>DeepTalkViewModel</code>) will:<ol> <li>Send the user's query to <code>ai_inference_server.ts</code>'s <code>/ai/embeddings/generate</code> to get an embedding.</li> <li>Send the embedding to <code>ai_inference_server.ts</code>'s <code>/data/search_vectors</code> to retrieve relevant <code>Memories</code> from <code>sqlite-vec</code>.</li> <li>These retrieved memories are then included in the LLM's prompt.</li> </ol> </li> </ul> </li> <li>Create/Read Messages: Same as <code>ConversationScreen</code>.</li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Focused Interaction: The UI should encourage deeper, more reflective conversations.</li> <li>Transparency: Subtle indicators help the user understand when Saira is drawing from its memory.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Memory Visibility: Ensure that only memories marked <code>is_active</code> and allowed by <code>memory_visibility_rules</code> are retrieved and used for RAG. The UI should reflect these settings.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Integration Tests: Test the RAG pipeline: simulate a query, verify that relevant memories are retrieved from the mocked backend, and that the LLM response incorporates them.</li> <li>End-to-End Tests: Simulate a Deep Talk session where the user asks about a past event, and Saira responds with details from a previously ingested memory.</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>RAG Failure: If memory retrieval or embedding generation fails, the LLM should still attempt to respond, but the UI might show a subtle warning that \"Saira couldn't access all its memories.\"</li> <li>Performance: Monitor RAG query latency. If too slow, provide feedback (e.g., \"Saira is thinking deeply...\").</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-profile-settings-screen_1","title":"Feature: Profile Settings Screen","text":"<p>Feature Goal (UI-specific) To provide a centralized hub for users to customize Saira's behavior, manage their data, configure privacy settings, and update AI models.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>UserManager</code> (Frontend Service): Reads/updates user preferences, privacy settings, and triggers data export/deletion.</li> <li><code>PersonalityManager</code> (Frontend Service):): Reads/updates personality profiles.</li> <li><code>ModelManager</code> (Frontend Service): Reads model versions, triggers model downloads.</li> <li><code>SyncService</code> (Frontend Service): Configures sync, initiates sync, handles recovery keys.</li> <li><code>NudgeManager</code> (Frontend Service): Reads/updates nudge preferences.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Main Settings Navigation (<code>ProfileSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: A list of categories (e.g., \"General,\" \"Personality,\" \"Privacy &amp; Data,\" \"AI Models,\" \"Notifications,\" \"Sync\").</li> <li>Elements:<ul> <li><code>FlatList</code> of <code>SettingsCategoryItem</code> components.</li> <li>Each item: Icon, Category Name, disclosure indicator (\"&gt;\").</li> </ul> </li> <li>Interactions: Tapping a category navigates to its dedicated sub-screen.</li> <li>States: Loading, default.</li> </ul> </li> <li> <p>General Settings (<code>GeneralSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Basic user profile information.</li> <li>Elements:<ul> <li>User's Name (<code>TextInput</code>).</li> <li>Profile Picture (display, button to change).</li> <li>Preferred Tone Style (radio buttons/dropdown).</li> </ul> </li> <li>Interactions: Editing fields updates user data.</li> <li>States: Editing, saving.</li> </ul> </li> <li> <p>Personality Settings (<code>PersonalitySettingsScreen.tsx</code>):</p> <ul> <li>Visuals: List of available personality profiles, with the active one highlighted.</li> <li>Elements:<ul> <li>Headline: \"Saira's Personality.\"</li> <li><code>FlatList</code> of <code>PersonalityProfileCard</code> components:<ul> <li>Each card: Name, description, \"Activate\" button (if not active).</li> <li>Active card has a distinct visual style.</li> </ul> </li> <li>Button: \"Create Custom Personality\" (navigates to <code>CustomPersonalityCreationScreen</code>).</li> </ul> </li> <li>Interactions: Tapping \"Activate\" changes Saira's active personality.</li> <li>States: Loading, active/inactive profiles.</li> </ul> </li> <li> <p>Privacy &amp; Data Settings (<code>PrivacyDashboardScreen.tsx</code>):</p> <ul> <li>Visuals: A dashboard for managing data consents and privacy actions.</li> <li>Elements:<ul> <li>Toggle: \"Allow emotion recording from voice.\"</li> <li>Toggle: \"Allow memory recording from conversations.\"</li> <li>Button: \"View Audit Logs\" (opens <code>AuditLogsScreen</code>).</li> <li>Button: \"Export All My Data\" (triggers data export).</li> <li>Button: \"Delete All My Data\" (triggers data deletion, with confirmation).</li> <li>Link to Privacy Policy.</li> </ul> </li> <li>Interactions: Toggling changes consent. Buttons trigger respective actions.</li> <li>States: Toggles on/off, processing states for export/delete.</li> </ul> </li> <li> <p>AI Models Settings (<code>AIModelManagementScreen.tsx</code>):</p> <ul> <li>Visuals: List of currently loaded AI models and their versions.</li> <li>Elements:<ul> <li>For each model (LLM, ASR, TTS, SER, Embedding):<ul> <li>Model Name (e.g., \"Mistral 7B,\" \"Whisper.cpp\").</li> <li>Current Version.</li> <li>Status (e.g., \"Loaded,\" \"Updating,\" \"Available Update\").</li> <li>Button: \"Check for Updates\" (for all models).</li> <li>Button: \"Download Update\" (if available for a specific model).</li> <li>Button: \"Rollback to Previous Version\" (if applicable).</li> </ul> </li> </ul> </li> <li>Interactions: Tapping \"Download Update\" initiates download.</li> <li>States: Loading, up-to-date, update available, downloading, installing, error.</li> </ul> </li> <li> <p>Notifications Settings (<code>NotificationSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Controls for proactive nudges.</li> <li>Elements:<ul> <li>Toggle: \"Enable Proactive Nudges.\"</li> <li>List of Nudge Types (e.g., \"Reminders,\" \"Positive Reinforcement,\" \"Wellness Suggestions\").</li> <li>For each type: Toggle to enable/disable, frequency picker (Daily, Weekly, Event-based), time picker.</li> </ul> </li> <li>Interactions: Toggling enables/disables nudges. Adjusting frequency/time updates preferences.</li> <li>States: Enabled/disabled, configured.</li> </ul> </li> <li> <p>Sync Settings (<code>SyncSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Controls for optional cloud synchronization.</li> <li>Elements:<ul> <li>Toggle: \"Enable Cloud Sync.\"</li> <li>Input fields for cloud service credentials (if using a generic E2EE service like EteSync).</li> <li>Dropdown: \"Sync Frequency\" (Manual, Daily, Weekly, On App Close).</li> <li>Display: \"Last Synced At:\"</li> <li>Display: \"Current Status:\"</li> <li>Button: \"Sync Now\".</li> <li>Button: \"Generate Recovery Key\" (for E2EE data).</li> <li>Button: \"Resolve Conflicts\" (if conflicts detected).</li> </ul> </li> <li>Interactions: Toggling enables/disables sync. Buttons trigger sync, key generation, or conflict resolution.</li> <li>States: Enabled/disabled, syncing, synced, conflicts, error.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li> <p>Frontend Architecture:</p> <ul> <li>Component Hierarchy: <code>ProfileSettingsScreen.tsx</code> will be the root of the settings tab, using <code>React Navigation</code>'s stack navigator to push sub-screens (e.g., <code>GeneralSettingsScreen</code>, <code>PrivacyDashboardScreen</code>).</li> <li>Reusable Component Library: <code>ToggleSwitch</code>, <code>TextInput</code>, <code>Button</code>, <code>Picker</code>, <code>List</code> components.</li> <li>State Management:<ul> <li><code>useState</code> for local form inputs within each settings sub-screen.</li> <li><code>useEffect</code> to load initial settings data from relevant managers (<code>UserManager</code>, <code>PersonalityManager</code>, etc.).</li> <li><code>useCallback</code> for debouncing input changes before saving to backend.</li> <li>Global state for <code>userId</code>, <code>currentPersonalityId</code>, <code>syncStatus</code>.</li> </ul> </li> <li>Routing: <code>ProfileSettingsScreen</code> is a tab in <code>MainTabView</code>. Sub-screens are pushed onto its own navigation stack.</li> </ul> </li> <li> <p>CRUD Operations (UI Perspective):</p> <ul> <li>User Preferences (Update <code>Users</code> table):<ul> <li>Flow: User changes name, tone, sync settings, nudge preferences -&gt; <code>UserManager.updateUser</code> or <code>NudgeManager.updateNudgePreference</code> is called.</li> <li><code>IPCService</code> sends updates to <code>data_manager.ts</code> to update <code>Users</code> or <code>NudgePreferences</code> table.</li> </ul> </li> <li>Personality Profiles (Read/Update <code>PersonalityProfiles</code> table):<ul> <li>Flow: <code>PersonalitySettingsScreen</code> loads <code>PersonalityManager.getPersonalityProfiles()</code>. User taps \"Activate\" -&gt; <code>PersonalityManager.activatePersonalityProfile</code> updates <code>Users.current_personality_profile_id</code>.</li> </ul> </li> <li>Privacy Consents (Update <code>Users</code> table):<ul> <li>Flow: Toggling consent switches calls <code>UserManager.updatePrivacySettings</code>.</li> </ul> </li> <li>Data Export/Deletion (Trigger Backend Process):<ul> <li>Flow: User clicks \"Export\" -&gt; <code>UserManager.exportData()</code> calls <code>IPCService</code> to trigger a Python script (<code>data_exporter.py</code>) in the backend.</li> <li>User clicks \"Delete\" -&gt; confirmation -&gt; <code>UserManager.deleteUser()</code> calls <code>IPCService</code> to <code>data_manager.ts</code> to delete user data.</li> </ul> </li> <li>AI Model Management (Read/Update Model Files):<ul> <li>Flow: <code>AIModelManagementScreen</code> loads <code>ModelManager.getLoadedModels()</code>. User clicks \"Download Update\" -&gt; <code>ModelManager.downloadModelUpdate()</code> calls <code>IPCService</code> to <code>model_manager.ts</code> (Node.js) which uses <code>model_downloader.py</code> (Python) to download the model.</li> <li><code>model_manager.ts</code> then loads the new model into the <code>ai_inference_addon</code>.</li> </ul> </li> </ul> </li> <li> <p>User Experience Flow (UI Perspective):</p> <ul> <li>Clear Categorization: Settings are logically grouped to prevent overwhelming the user.</li> <li>Instant Feedback: Toggles and input fields update immediately.</li> <li>Confirmation: Critical actions (e.g., data deletion) require explicit confirmation.</li> <li>Progress Indicators: Show progress for long-running operations like model downloads or data exports.</li> </ul> </li> <li> <p>Security Considerations (UI Perspective):</p> <ul> <li>Sensitive Data Input: If cloud sync requires credentials, ensure input fields are secure (e.g., password masking).</li> <li>Consent Visibility: Clearly display current consent status for sensitive data processing.</li> <li>Confirmation for Destructive Actions: Double-confirm data deletion.</li> <li>Recovery Key Display: When generating a recovery key, display it prominently with warnings about secure storage.</li> </ul> </li> <li> <p>Testing Strategy (UI Perspective):</p> <ul> <li>Unit Tests: Test individual settings components (e.g., <code>ToggleSwitch</code>, <code>Picker</code>).</li> <li>Integration Tests: Test data loading and saving for each settings section. Simulate model downloads and sync processes.</li> <li>End-to-End Tests: Simulate a user changing various settings, verifying persistence and impact on app behavior (e.g., personality change affects LLM response).</li> </ul> </li> <li> <p>Error Handling &amp; Logging (UI Perspective):</p> <ul> <li>Save Errors: Display \"Failed to save settings. Please try again.\" for backend save failures.</li> <li>Download Errors: Display \"Failed to download model update.\" for model download failures.</li> <li>Sync Errors: Display \"Sync failed. Check network or credentials.\" for sync issues.</li> <li>Deletion Errors: Display \"Failed to delete data.\"</li> <li>Logging: Log all settings changes, sync events, model updates, and data export/deletion attempts.</li> </ul> </li> </ol>"},{"location":"ui_implemenation/#feature-proactive-nudges-notifications_1","title":"Feature: Proactive Nudges &amp; Notifications","text":"<p>Feature Goal (UI-specific) To proactively engage the user with timely reminders, positive reinforcement, and wellness suggestions, delivered through macOS system notifications and an in-app history.</p> <p>Any API Relationships (UI-specific)</p> <ul> <li><code>NotificationService</code> (Frontend Service): Registers for and displays macOS system notifications.</li> <li><code>NudgeManager</code> (Frontend Service): Manages nudge preferences and records nudge history.</li> <li><code>ai_inference_server.ts</code> (Node.js Backend): The LLM in the backend will generate nudge content based on user data and preferences.</li> <li><code>data_manager.ts</code> (Node.js Backend): Stores nudge preferences and history.</li> </ul> <p>Detailed Feature Requirements (UI-specific)</p> <ol> <li> <p>Notification Display (macOS System Level):</p> <ul> <li>Visuals: Standard macOS notification banners/alerts.</li> <li>Elements:<ul> <li>App icon.</li> <li>Title (e.g., \"Saira Check-in,\" \"Saira Motivation\").</li> <li>Body text (the nudge message, e.g., \"How are you feeling today?\").</li> <li>Optional: Action buttons (e.g., \"Log Mood,\" \"Dismiss\").</li> </ul> </li> <li>Interactions: User can click on the notification to open the app to a relevant screen (e.g., Journaling screen for mood check-in).</li> <li>States: Delivered, interacted.</li> </ul> </li> <li> <p>In-App Nudge History (<code>NotificationSettingsScreen.tsx</code> - Sub-section):</p> <ul> <li>Visuals: A scrollable list of past nudges.</li> <li>Elements:<ul> <li><code>FlatList</code> displaying <code>NudgeHistoryItem</code> components.</li> <li>Each item: Date/Time, Nudge Type, Message Content.</li> <li>Optional: Icon indicating if user interacted with it.</li> </ul> </li> <li>Interactions: User can review past nudges.</li> <li>States: Loading, empty list.</li> </ul> </li> <li> <p>Nudge Configuration (Part of <code>ProfileSettingsScreen.tsx</code> -&gt; <code>NotificationSettingsScreen.tsx</code>):</p> <ul> <li>Visuals: Toggles and selectors for customizing nudge behavior.</li> <li>Elements:<ul> <li>Toggle: \"Enable Proactive Nudges.\"</li> <li>Section for \"Nudge Types\":<ul> <li>For each type (e.g., \"Reminders,\" \"Positive Reinforcement,\" \"Wellness Suggestions\"):<ul> <li>Toggle to enable/disable.</li> <li>Dropdown/Picker for \"Frequency\" (Daily, Weekly, Event-based).</li> <li>Time picker for daily/weekly nudges.</li> </ul> </li> </ul> </li> <li>Button: \"Customize Nudge Content\" (future feature, opens a modal for prompt templates).</li> </ul> </li> <li>Interactions: Toggling enables/disables. Selecting frequency/time updates preferences.</li> <li>States: Enabled/disabled, configured.</li> </ul> </li> </ol> <p>Detailed Implementation Guide (UI-specific)</p> <ol> <li>System Architecture Overview:<ul> <li>High-level Architecture: Nudge generation logic resides in the Node.js</li> </ul> </li> </ol>"},{"location":"api/","title":"API Documentation","text":"<p>This directory contains all documentation related to the APIs exposed by Saira AI Companion.</p>"},{"location":"api/#how-to-document-apis","title":"How to Document APIs","text":"<ul> <li>OpenAPI/Swagger:   Use <code>openapi.yaml</code> for structured API specifications.   You can edit this file directly or use tools like Swagger Editor or Stoplight.</li> <li>Markdown:   For quick docs or additional context, use the <code>endpoint-template.md</code> provided here.   Copy and fill this template for each endpoint as needed.</li> </ul>"},{"location":"api/#structure","title":"Structure","text":"<ul> <li><code>openapi.yaml</code> \u2014 Main OpenAPI/Swagger spec (add endpoints as you design them)</li> <li><code>endpoint-template.md</code> \u2014 Copy this file to document individual endpoints in Markdown</li> </ul>"},{"location":"api/#references","title":"References","text":"<ul> <li>OpenAPI Specification</li> <li>Swagger Editor</li> <li>Markdown Guide</li> </ul> <p>Last updated: 2025-06-09</p>"},{"location":"api/eendpoint-template/","title":"API Endpoint Documentation","text":"<p>Use this template to describe a specific API endpoint. Copy and fill out per endpoint.</p>"},{"location":"api/eendpoint-template/#endpoint","title":"Endpoint","text":"<p><code>[HTTP METHOD] /path/to/endpoint</code></p>"},{"location":"api/eendpoint-template/#description","title":"Description","text":"<p>What does this endpoint do?</p>"},{"location":"api/eendpoint-template/#request","title":"Request","text":""},{"location":"api/eendpoint-template/#parameters","title":"Parameters","text":"Name In Type Required Description example body string true Example parameter"},{"location":"api/eendpoint-template/#request-body-if-applicable","title":"Request Body (if applicable)","text":"<pre><code>{\n  \"example\": \"value\"\n}\n</code></pre>"},{"location":"api/eendpoint-template/#response","title":"Response","text":""},{"location":"api/eendpoint-template/#success-200","title":"Success (200)","text":"<pre><code>{\n  \"result\": \"success\"\n}\n</code></pre>"},{"location":"api/eendpoint-template/#error-responses","title":"Error Responses","text":"Status Description 400 Bad request 401 Unauthorized ... ..."},{"location":"api/eendpoint-template/#notes","title":"Notes","text":"<ul> <li>Authentication required? [Yes/No]</li> <li>Permissions/roles needed: [role1, role2]</li> </ul> <p>Last updated: 2025-06-09</p>"}]}