const assert = require('assert');const path = require('path');const addon = require(path.join(__dirname, '../native/build/Release/ai_inference_addon.node'));// Mock node-llama-cpp getLlama function and Llama instanceconst mockLlama = {    loadModel: (modelPath) => {        console.log(`Mock Llama: Loading model from ${modelPath}`);        return Promise.resolve();    },    session: {        prompt: (promptText, options) => {            console.log(`Mock Llama: Generating for prompt: "${promptText}" with options:`, options);            return Promise.resolve(`Mock generated text for: ${promptText}`);        }    },    embed: (text) => {        console.log(`Mock Llama: Embedding text: "${text}"`);        // Return a mock Float32Array        return Promise.resolve(new Float32Array([0.1, 0.2, 0.3, 0.4]));    }};function getMockLlama() {    return mockLlama;}async function runTests() {    console.log('Running tests for llmGenerate and createEmbedding...');    // Initialize the addon with the mock getLlama function    addon.initLlama(getMockLlama);    // Test 1: llmGenerate without model loaded (should fail)    try {        console.log('Attempting llmGenerate without model loaded (should fail)...');        await addon.llmGenerate('Hello');        assert.fail('Test 1 failed: llmGenerate succeeded without model loaded.');    } catch (error) {        console.log('Caught expected error:', error.message);        assert.ok(error.message.includes('Llama model is not loaded'), 'Test 1 failed: Error message did not indicate model not loaded.');    }    // Load the mock Llama model    await addon.loadModel('llama', '/path/to/mock/llama.gguf');    // Test 2: llmGenerate with model loaded    try {        console.log('Attempting llmGenerate with model loaded...');        const generatedText = await addon.llmGenerate('What is the capital of France?', { maxTokens: 10 });        console.log('Generated Text:', generatedText);        assert.ok(generatedText.includes('Mock generated text'), 'Test 2 failed: Generated text is incorrect.');    } catch (error) {        console.error('Error during llmGenerate:', error.message);        assert.fail('Test 2 failed: llmGenerate failed unexpectedly.');    }    // Test 3: createEmbedding without model loaded (should fail)    try {        console.log('Attempting createEmbedding without model loaded (should fail)...');        // Unload the model first for this test        addon.unloadModel('llama');        await addon.createEmbedding('test');        assert.fail('Test 3 failed: createEmbedding succeeded without model loaded.');    } catch (error) {        console.log('Caught expected error:', error.message);        assert.ok(error.message.includes('Llama model is not loaded'), 'Test 3 failed: Error message did not indicate model not loaded.');    }    // Reload the mock Llama model for subsequent tests    await addon.loadModel('llama', '/path/to/mock/llama.gguf');    // Test 4: createEmbedding with model loaded    try {        console.log('Attempting createEmbedding with model loaded...');        const embedding = await addon.createEmbedding('Hello world');        console.log('Embedding:', embedding);        assert.ok(embedding instanceof Float32Array, 'Test 4 failed: Embedding is not a Float32Array.');        assert.strictEqual(embedding.length, 4, 'Test 4 failed: Embedding length is incorrect.');    } catch (error) {        console.error('Error during createEmbedding:', error.message);        assert.fail('Test 4 failed: createEmbedding failed unexpectedly.');    }    // Unload the model after all tests    addon.unloadModel('llama');    console.log('\nAll llmGenerate and createEmbedding tests passed!');}runTests().catch(err => {    console.error('An unexpected error occurred during tests:', err);    process.exit(1);});